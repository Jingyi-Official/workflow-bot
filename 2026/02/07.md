# Daily Paper Digest Â· 2026-02-07
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/pdf/2602.04317v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Monocular human reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenges of maintaining model stability and geometric plausibility in human reconstruction from monocular images.

### 3. Core Idea
- The core idea is to optimize the reconstruction of human avatars by jointly refining pose parameters and camera trajectories to ensure geometric precision and temporal coherence.

### 4. Method
- **Pipeline**: The method involves a joint optimization framework that integrates pose refinement and camera trajectory adjustments.
- **Architecture / Loss / Training**: The loss function combines LBS constraints, SMPL geometric constraints, and dynamic attribute regularization.
- **Complexity / Resources**: The method requires significant computational resources for training and evaluation.

</details>

### [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/pdf/2602.04913v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Data processing for emotion-rich QA generation

### 2. Motivation & Gaps
- The paper presents a pipeline for constructing a high-quality QA dataset from VoxCeleb clips, focusing on emotion-rich content.

### 3. Core Idea
- The pipeline utilizes advanced models for transcription, emotion estimation, and QA generation to create a dataset that emphasizes emotional content.

### 4. Method
- **Pipeline**: The pipeline includes transcription, emotion estimation, cleaning, QA generation, and audio synthesis.
- **Architecture / Loss / Training**: The model is optimized using AdamW with specific loss functions for parameter regression, spatial geometric consistency, and temporal dynamics consistency.
- **Complexity / Resources**: Experiments are conducted on a node with 8 NVIDIA A100 GPUs, utilizing PyTorch for implementation.

</details>

### [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/pdf/2602.03439v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Ontology and knowledge graph construction using large language models

### 2. Motivation & Gaps
- The paper addresses the need for efficient methods to construct ontologies and knowledge graphs, leveraging the capabilities of large language models (LLMs).

### 3. Core Idea
- Utilizing LLMs to automate the construction of ontologies and knowledge graphs, improving efficiency and accuracy.

### 4. Method
- **Pipeline**: The proposed method involves preprocessing text data, applying LLMs for information extraction, and structuring the output into ontologies.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method requires significant computational resources for training and inference with LLMs.

</details>

## video understanding

### [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/pdf/2602.06043v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Continual Learning

### 2. Motivation & Gaps
- This work represents one of the earliest attempts to enable parameter-efficient continual learning, leveraging either low-rank adapters or data.

### 3. Core Idea
- The core idea is to enable parameter-efficient continual learning through the use of low-rank adapters or data, while addressing the limitations of current models.

### 4. Method
- **Pipeline**: Continual model merging and learning at scale using low-rank adapters.
- **Architecture / Loss / Training**: Utilizes LoRA and Share with specific hyperparameters for different tasks.
- **Complexity / Resources**: The method reduces computational resources required for training, thereby lowering energy consumption and carbon footprints.

</details>

### [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/pdf/2602.06041v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Camera pose prediction from perspective descriptions

### 2. Motivation & Gaps
- The paper addresses the challenge of predicting camera pose based on textual descriptions of scenes, which is crucial for spatial reasoning tasks.

### 3. Core Idea
- The core idea is to fine-tune a language model with LoRA while jointly training pose-related modules to improve camera pose prediction accuracy.

### 4. Method
- **Pipeline**: Fine-tuning the language backbone with LoRA while training pose-related modules.
- **Architecture / Loss / Training**: The overall objective is a weighted sum of language modeling loss and pose regression loss.
- **Complexity / Resources**: Utilizes 4 GPUs with specific batch sizes and training steps.

</details>

### [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/pdf/2602.06040v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning with adaptive mode selection

### 2. Motivation & Gaps
- Existing multimodal models follow fixed reasoning patterns, limiting their adaptability to different tasks.

### 3. Core Idea
- SwimBird adopts a hybrid autoregressive paradigm that allows for adaptive switching between text-only, vision-only, and interleaved reasoning modes.

### 4. Method
- **Pipeline**: The model uses a systematic curation and mode-labeling strategy for effective multi-mode training.
- **Architecture / Loss / Training**: A cosine learning rate scheduler is applied with an initial learning rate of 1e-5, and the model is fine-tuned on A100-80G GPUs.
- **Complexity / Resources**: Training is performed with a global batch size of 128, keeping the vision encoder and multimodal projector frozen.

</details>

## model collapse

### [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/pdf/2602.06039v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-Agent Reasoning

### 2. Motivation & Gaps
- The paper addresses the inefficiencies in multi-agent systems that rely on fixed topologies and communication patterns, which can lead to redundant computations and increased resource consumption.

### 3. Core Idea
- DyTopo introduces a dynamic topology routing mechanism that adapts communication patterns based on semantic matching, improving efficiency and accuracy in multi-agent reasoning tasks.

### 4. Method
- **Pipeline**: The method involves initializing agents, performing single-pass inference, inducing topology based on semantic relevance, and updating memory through deterministic message ordering.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: DyTopo achieves high accuracy with significantly lower token consumption and latency compared to fixed-horizon baselines.

</details>
