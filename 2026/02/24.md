# Daily Paper Digest Â· 2026-02-24
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations](https://arxiv.org/pdf/2602.19874v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dynamic Pose Estimation

### 2. Motivation & Gaps
- The paper addresses the challenge of accurately estimating the poses of a four-legged avatar using a reduced set of joints, focusing on improving the realism of the poses and the efficiency of the model.

### 3. Core Idea
- The core idea is to optimize a mesh model for dynamic pose estimation by adapting individual parameters and using a combination of loss functions to ensure realistic poses and smooth transitions over time.

### 4. Method
- **Pipeline**: The method involves initializing model parameters, optimizing them using Adam, and adapting the mesh for individual subjects based on multi-view data.
- **Architecture / Loss / Training**: The architecture employs multiple loss functions including keypoint reprojection loss, silhouette reprojection loss, pose and bone constraints, and smoothness loss to guide the optimization process.
- **Complexity / Resources**: Action tracking, label generation, and surface fitting was executed on a single NVIDIA Geforce RTX 3090 GPU. A single optimization step in surface fitting takes 0.019 s, resulting in an effective per-frame runtime of 6.65 s for our default 350-epoch setting.

</details>

### [PoseCraft: Tokenized 3D Body Landmark and Camera Conditioning for Photorealistic Human Image Synthesis](https://arxiv.org/pdf/2602.19350v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D human pose estimation and image synthesis

### 2. Motivation & Gaps
- PoseCraft aims to improve the fidelity and realism of synthesized images by utilizing discrete 3D tokens as conditioning signals.

### 3. Core Idea
- The core idea of PoseCraft is to use discrete 3D tokens for conditioning a diffusion model, allowing for better control over image synthesis and improved fidelity.

### 4. Method
- **Pipeline**: PoseCraft integrates a diffusion model with a RigCraft triangulation module to convert 2D detections into stable 3D landmarks.
- **Architecture / Loss / Training**: The architecture employs a 3D Control Tokenizer to condition the latent diffusion model, optimizing for PSNR, SSIM, LPIPS, and FID metrics.
- **Complexity / Resources**: The method is designed to be lightweight, avoiding costly template fitting while achieving competitive results.

</details>

### [MagHeart: Exploring Playful Avatar Co-Creation and Shared Heartbeats for Icebreaking in Hybrid Meetings](https://arxiv.org/pdf/2602.18676v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Exploring the impact of playful co-creation and physiological cues on hybrid meeting dynamics

### 2. Motivation & Gaps
- The study investigates how MagHeart's combination of playful co-creation and embodied physiological cues can address engagement barriers in hybrid meetings.

### 3. Core Idea
- MagHeart utilizes playful avatar co-creation and heartbeat interactions to enhance social presence and reduce engagement barriers in hybrid meetings.

### 4. Method
- **Pipeline**: Participants engage in avatar co-creation followed by heartbeat interaction during meetings.
- **Architecture / Loss / Training**: The physical device consists of a cubic MDF base housing an electromagnet and control electronics, with a LEGO figurine mounted on a PETG-printed base.
- **Complexity / Resources**: The system is powered by a 12 V supply and uses an ESP32 microcontroller for operation.

</details>

## video understanding

### [Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device](https://arxiv.org/pdf/2602.20161v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Unified multimodal understanding and generation

### 2. Motivation & Gaps
- The paper addresses the need for efficient multimodal models that can operate on mobile devices, ensuring user privacy and offline functionality.

### 3. Core Idea
- Mobile-O demonstrates competitive visual understanding despite its mobile-optimized architecture.

### 4. Method
- **Pipeline**: The method involves a three-stage training setup: cross-modal alignment, targeted fine-tuning, and unified multimodal post-training.
- **Architecture / Loss / Training**: Utilizes a combination of learnable fusion, compression, and channel attention in the MCP, with mixed-precision training for efficiency.
- **Complexity / Resources**: A 2B-parameter model in FP16 requires approximately 4.0 GB just for the weights alone, excluding memory for activations, attention caches, and runtime overhead.

</details>

### [A Very Big Video Reasoning Suite](https://arxiv.org/pdf/2602.20159v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-step planning and reasoning in video tasks

### 2. Motivation & Gaps
- The paper addresses the need for comprehensive evaluation metrics and benchmarks for video reasoning tasks, highlighting the limitations of existing models in handling complex reasoning scenarios.

### 3. Core Idea
- To provide a robust suite for evaluating video reasoning capabilities across various tasks and models.

### 4. Method
- **Pipeline**: The evaluation involves computing performance metrics across different models using statistical methods like Pearson correlation and boxplots for score distributions.
- **Architecture / Loss / Training**: The benchmark includes diverse architectures such as transformer-based and latent diffusion models, focusing on their reasoning capabilities.
- **Complexity / Resources**: The benchmark supports batch execution and caching mechanisms, allowing for efficient resource utilization during evaluations.

</details>

### [Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning](https://arxiv.org/pdf/2602.20157v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dynamic Scene Reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of reconstructing dynamic scenes from video data, particularly focusing on the lack of annotated geometry and camera poses in existing datasets.

### 3. Core Idea
- The proposed method leverages flow supervision as an intermediate supervisory signal to enhance visual geometry learning, rather than focusing on pixel-level matching accuracy.

### 4. Method
- **Pipeline**: The method involves generating pseudo labels for dynamic datasets and using a stride-based sampling protocol for evaluation.
- **Architecture / Loss / Training**: The model combines geometry and camera representations to predict correspondences.
- **Complexity / Resources**: Trained on 4 H100 GPUs for 160k iterations using Adam optimizer with a learning rate of 2Ã—10^-5.

</details>

## model collapse

### [tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction](https://arxiv.org/pdf/2602.20160v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenges of autoregressive 3D reconstruction, particularly focusing on the limitations of memory capacity in fast weights and the impact of scene complexity on performance.

### 3. Core Idea
- Our method surpasses previous feedforward methods and can further surpass optimization-based methods with a few steps post-optimization.

### 4. Method
- **Pipeline**: The method involves a training-free strategy that uses historical states for selective updates and applies elastic regularization based on parameter importance.
- **Architecture / Loss / Training**: The model uses a curriculum training strategy progressing from low to high resolution, with specific training stages and loss functions.
- **Complexity / Resources**: Our model can be linearly accelerated with multiple GPUs, here we report time on a single Nvidia A100 80GB GPU.

</details>
