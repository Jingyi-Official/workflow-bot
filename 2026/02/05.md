# Daily Paper Digest Â· 2026-02-05
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/pdf/2602.04317v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Monocular human reconstruction

### 2. Motivation & Gaps
- The paper addresses the need for improved stability and geometric plausibility in human reconstruction from monocular images.

### 3. Core Idea
- The proposed method optimizes human pose and attributes jointly to enhance reconstruction quality and spatial alignment.

### 4. Method
- **Pipeline**: The method involves a joint optimization framework that refines human poses and camera trajectories iteratively.
- **Architecture / Loss / Training**: Utilizes a combination of LBS constraints, SMPL model constraints, and dynamic attribute regularization with specific loss weights.
- **Complexity / Resources**: The method requires moderate computational resources for training and evaluation.

</details>

### [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/pdf/2602.03439v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-Agent Conversation for LLM Applications

### 2. Motivation & Gaps
- The paper addresses the need for advanced applications of large language models (LLMs) through multi-agent interactions.

### 3. Core Idea
- The core idea is to leverage multi-agent systems to enhance the capabilities and applications of large language models.

### 4. Method
- **Pipeline**: The method involves a multi-agent framework where different agents collaborate to perform tasks using LLMs.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method requires significant computational resources for training and inference with LLMs.

</details>

### [OFERA: Blendshape-driven 3D Gaussian Control for Occluded Facial Expression to Realistic Avatars in VR](https://arxiv.org/pdf/2602.01748v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-time reconstruction of occluded facial expressions as photorealistic 3D Gaussian avatars

### 2. Motivation & Gaps
- The importance of system-level integration for practical avatar deployment in VR, highlighting the need for coordination across sensing, representation, and rendering stages.

### 3. Core Idea
- OFERA integrates headset-available sensing, calibrated parameter mapping, and lightweight real-time communication to enable realistic avatar control in VR.

### 4. Method
- **Pipeline**: An end-to-end pipeline that takes blendshape data from a VR headset and renders a Gaussian avatar with realistic expressions in real-time.
- **Architecture / Loss / Training**: The architecture includes BDA, EPM, and MiA modules, with a focus on real-time performance and reducing distribution mismatch.
- **Complexity / Resources**: The system is constrained by the blendshape space and requires careful calibration across different VR headsets.

</details>

## video understanding

### [Reinforced Attention Learning](https://arxiv.org/pdf/2602.04884v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal alignment and visual understanding

### 2. Motivation & Gaps
- The paper addresses the limitations of outcome-based reinforcement learning methods that neglect the underlying cross-modal reasoning process.

### 3. Core Idea
- Reinforced Attention Learning (RAL) shifts optimization from text token distribution to internal attention distributions, treating attention as a policy to enhance visual grounding and perceptual focus.

### 4. Method
- **Pipeline**: The method involves a two-stage process: supervised fine-tuning (SFT) followed by reinforcement learning (RL) to optimize attention distributions.
- **Architecture / Loss / Training**: The architecture employs a format reward that penalizes outputs deviating from a specified structure, enhancing the model's focus on relevant information.
- **Complexity / Resources**: The method maintains all hyper-parameters and data volumes from main experiments while modifying data formats and reward functions.

</details>

### [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/pdf/2602.04877v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Point Tracking

### 2. Motivation & Gaps
- The paper addresses the challenges in point tracking by proposing a method that avoids traditional correlation-based approaches.

### 3. Core Idea
- Our method avoids cost volumes entirely and operates directly on feature maps using a warping-based head, enabling high-resolution tracking without sacrificing practicality.

### 4. Method
- **Pipeline**: The method involves extracting features from video frames, warping these features according to current tracks, and iteratively updating the hidden state and tracks.
- **Architecture / Loss / Training**: The architecture utilizes a standard backbone for feature extraction and does not construct a correlation or cost volume.
- **Complexity / Resources**: Our method can run comfortably on stride 1/2 features, which only takes a few gigabytes of memory to store.

</details>

### [Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning](https://arxiv.org/pdf/2602.04872v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- multi-modal in-context learning

### 2. Motivation & Gaps
- The study addresses the limitations of single-layer LSA models in multi-modal ICL tasks, highlighting the need for a more robust architecture.

### 3. Core Idea
- Introduction of a novel transformer-like architecture that incorporates depth and context-aware mechanisms to achieve Bayes-optimal performance in multi-modal ICL tasks.

### 4. Method
- **Pipeline**: Gradient descent optimization on non-asymptotic objectives.
- **Architecture / Loss / Training**: CA architecture designed to learn Bayes-optimal predictors.
- **Complexity / Resources**: The model includes learnable parameters and skip-connections, diverging from conventional architectures.

</details>

## model collapse

### [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/pdf/2602.04883v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Protein Generation and Designability Assessment

### 2. Motivation & Gaps
- The study focuses on generating long proteins and assessing their designability using models like PAR and Proteina.

### 3. Core Idea
- The paper proposes a method for generating long proteins while evaluating their designability and diversity through various metrics.

### 4. Method
- **Pipeline**: The method involves generating protein structures using the PAR model and evaluating them against designability metrics.
- **Architecture / Loss / Training**: Utilizes transformer-based architectures for both AR and decoder modules.
- **Complexity / Resources**: The study mentions the use of 400M and 60M parameter models, indicating a focus on balancing model size and performance.

</details>

### [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/pdf/2602.04881v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Continual Learning in IoT

### 2. Motivation & Gaps
- Contrastive continual learning (CCL) offers a compelling path for IoT systems that must adapt over time while remaining robust, secure, privacy-preserving, and resource-efficient.

### 3. Core Idea
- Unifying objectives such as contrastive learning, replay, and distillation to align with IoT device architectures.

### 4. Method
- **Pipeline**: Contrastive learning framework integrated with IoT architectures.
- **Architecture / Loss / Training**: Utilizes contrastive objectives to enhance representation learning.
- **Complexity / Resources**: Requires careful design of privacy and communication mechanisms.

</details>
