# Daily Paper Digest Â· 2026-02-16
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [OMEGA-Avatar: One-shot Modeling of 360Â° Gaussian Avatars](https://arxiv.org/pdf/2602.11693v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Expression Reenactment

### 2. Motivation & Gaps
- The goal of Expression Reenactment is to transfer the pose and expression from a driving image to the source image while preserving the source identity and maintaining high fidelity.

### 3. Core Idea
- To create a complete canonical 3D Gaussian avatar by combining Gaussian primitives from vertex and UV branches and animating it using parameters extracted from a driving image.

### 4. Method
- **Pipeline**: The framework utilizes a pre-trained diffusion model to generate multi-view images for training, ensuring geometric and appearance constraints.
- **Architecture / Loss / Training**: The total objective function includes L_reconstruction, L_perceptual, L_mv, and L_local regularization terms.
- **Complexity / Resources**: The model is trained on two NVIDIA H100 GPUs for 250,000 iterations with a batch size of 6.

</details>

### [ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles](https://arxiv.org/pdf/2602.11575v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Surface reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of training visual navigation policies in dynamic environments by generating photorealistic navigation datasets.

### 3. Core Idea
- ReaDy-Go combines a GS scene, a human animation module, an expert planner, and a human planner to generate datasets that improve navigation performance in both simulation and real-world environments.

### 4. Method
- **Pipeline**: Combines scene generation, human animation, and planning modules to create photorealistic datasets.
- **Architecture / Loss / Training**: The navigation policy consists of ten convolutional layers with residual connections and three MLP layers, trained with the Adam optimizer.
- **Complexity / Resources**: Requires approximately 80kâ€“120k data samples for training, with a learning rate of 10^-4.

</details>

### [3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars](https://arxiv.org/pdf/2602.10516v2)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Emotion-driven speech animation

### 2. Motivation & Gaps
- The paper addresses the challenge of modeling fine-grained emotional dynamics in speech, which is often subtle and context-dependent.

### 3. Core Idea
- Introducing audio-rich representations that capture frame-wise amplitude and emotion cues for better emotional expression in speech.

### 4. Method
- **Pipeline**: The framework incorporates a center motion trajectory for expressive flexibility and robust dynamic control.
- **Architecture / Loss / Training**: The architecture employs a dual-branch design for emotion disentanglement and utilizes a loss function that emphasizes both lip synchronization and emotional expression.
- **Complexity / Resources**: The model requires substantial computational resources for training due to the complexity of 3D facial animation and emotion modeling.

</details>

## video understanding

### [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/pdf/2602.13197v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Policy learning for robotic manipulation

### 2. Motivation & Gaps
- The paper addresses the challenges in robotic manipulation by improving trajectory modeling for policy learning.

### 3. Core Idea
- The core idea is to enhance policy learning by utilizing any-point trajectory modeling to improve the efficiency and effectiveness of robotic manipulation tasks.

### 4. Method
- **Pipeline**: The method involves a simulation-based approach where trajectories are filtered and evaluated based on their compatibility with the task.
- **Architecture / Loss / Training**: The architecture employs a loss function that penalizes collisions and encourages successful trajectory execution.
- **Complexity / Resources**: The method requires significant computational resources for simulation and data processing, including the use of RGB-D data.

</details>

### [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/pdf/2602.13195v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Referring expression segmentation

### 2. Motivation & Gaps
- The paper addresses the challenge of segmenting regions in images based on conversational prompts, aiming to improve performance in zero-shot settings.

### 3. Core Idea
- The core idea is to leverage conversational prompts to guide the segmentation process, enhancing the model's ability to understand and segment based on human-like queries.

### 4. Method
- **Pipeline**: The method involves a multi-stage pipeline that includes scene understanding, mask generation, and prompt refinement.
- **Architecture / Loss / Training**: The architecture employs a loss function that emphasizes mask quality and alignment with conversational prompts.
- **Complexity / Resources**: The model is designed to be efficient, utilizing a 3B backbone while achieving competitive results.

</details>

### [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/pdf/2602.13191v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- next-frame retrieval

### 2. Motivation & Gaps
- The method leverages codec primitives to improve performance in next-frame retrieval tasks.

### 3. Core Idea
- The Î”-Encoder preserves semantically meaningful motion and appearance cues critical for retrieval.

### 4. Method
- **Pipeline**: Current frame along with motion vectors and residuals processed by the Î”-encoder and pre-training transformers.
- **Architecture / Loss / Training**: The architecture includes a lightweight Î”-Encoder with two auxiliary transformer modules for pre-training, and employs a two-stage training pipeline.
- **Complexity / Resources**: The model is lightweight with less than 15M parameters and was trained using 16Ã—A100 GPUs.

</details>

## model collapse

### [$\texttt{GPUmonty}$: A GPU-accelerated relativistic Monte Carlo radiative transfer code](https://arxiv.org/pdf/2602.13198v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Computing electromagnetic spectra emitted by hot gas in accretion flows around black holes

### 2. Motivation & Gaps
- The paper addresses the need for efficient computation of electromagnetic spectra in astrophysical contexts, particularly around black holes.

### 3. Core Idea
- GPUmonty is a CUDA port of grmonty that utilizes NVIDIA GPUs for parallel processing in five optimized kernels.

### 4. Method
- **Pipeline**: The method involves superphoton generation, sampling, tracking, scattering, and recording.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The implementation achieves a 12Ã— speedup on a single GPU compared to the CPU-based grmonty.

</details>

### [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/pdf/2602.13194v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Estimating entropy rates of text sequences through semantic chunking

### 2. Motivation & Gaps
- This work connects redundancy in natural language to predictability through a minimal theory linking multiscale chunking to language predictability.

### 3. Core Idea
- The paper discusses the derivation of the entropy rate for weak ordered partition random tree models, particularly focusing on the case of K=2 and its implications in large K limits.

### 4. Method
- **Pipeline**: The method involves deriving entropy rates using recursive relations and Mellin transforms.
- **Architecture / Loss / Training**: One-parameter random K-ary tree ensemble that predicts log-normal distribution of chunk sizes.
- **Complexity / Resources**: Empirically selected values of K reflect human working-memory limits.

</details>
