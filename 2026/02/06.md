# Daily Paper Digest Â· 2026-02-06
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/pdf/2602.04317v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Monocular human reconstruction

### 2. Motivation & Gaps
- The paper addresses the need for improved stability and geometric plausibility in human reconstruction from monocular images.

### 3. Core Idea
- The proposed method optimizes human pose and attributes jointly to enhance reconstruction quality and stability.

### 4. Method
- **Pipeline**: The method involves a joint optimization framework that refines human poses and camera trajectories iteratively.
- **Architecture / Loss / Training**: The loss function combines LBS constraints, SMPL geometric constraints, and dynamic attribute regularization.
- **Complexity / Resources**: The method requires moderate computational resources, with specific weights assigned to different loss components.

</details>

### [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/pdf/2602.04913v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Animation Generation

### 2. Motivation & Gaps
- Current audio-driven approaches often produce limited expressiveness and fail to capture the nuances of emotional context in facial animations.

### 3. Core Idea
- A2-LLM integrates language processing, audio generation, and facial animation into a single multimodal large model, enabling context-aware and emotionally congruent facial expressions.

### 4. Method
- **Pipeline**: End-to-end framework that unifies multiple modalities.
- **Architecture / Loss / Training**: Utilizes a hierarchical motion modeling strategy to ensure stability and expressiveness.
- **Complexity / Resources**: Achieves interaction latency of approximately 500ms.

</details>

### [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/pdf/2602.03439v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-Agent Conversation for LLM Applications

### 2. Motivation & Gaps
- The paper addresses the need for advanced applications of large language models (LLMs) through multi-agent interactions.

### 3. Core Idea
- The core idea is to leverage multi-agent systems to enhance the capabilities and applications of large language models.

### 4. Method
- **Pipeline**: The method involves a multi-agent framework where agents interact and collaborate to perform tasks using LLMs.
- **Architecture / Loss / Training**: The architecture employs a transformer-based model with fine-tuning on domain-specific datasets to optimize performance.
- **Complexity / Resources**: The method requires significant computational resources for training LLMs, including GPUs and large datasets.

</details>

## video understanding

### [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/pdf/2602.06043v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Continual Learning

### 2. Motivation & Gaps
- This work represents one of the earliest attempts to enable parameter-efficient continual learning, leveraging either low-rank adapters or data.

### 3. Core Idea
- The core idea is to enable parameter-efficient continual learning through the use of low-rank adapters or data.

### 4. Method
- **Pipeline**: Continual model merging and learning at scale using low-rank adapters.
- **Architecture / Loss / Training**: Utilizes a backbone model like RoBERTa and Flux for different tasks with specific hyperparameters.
- **Complexity / Resources**: The method reduces computational resources required for training, thereby lowering energy consumption and carbon footprints.

</details>

### [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/pdf/2602.06041v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Camera pose prediction from perspective descriptions

### 2. Motivation & Gaps
- The paper addresses the challenge of predicting camera pose based on textual descriptions of scenes, which is crucial for spatial reasoning tasks.

### 3. Core Idea
- The core idea is to fine-tune a language model with LoRA while jointly training pose-related modules to improve camera pose prediction accuracy.

### 4. Method
- **Pipeline**: The method involves a weighted sum of language modeling loss and pose regression loss during training.
- **Architecture / Loss / Training**: LoRA is used to fine-tune the language backbone while training pose-related modules with specific loss weights.
- **Complexity / Resources**: Utilizes 4 GPUs with a per-device batch size of 2 and an effective batch size of 8.

</details>

### [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/pdf/2602.06040v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning with adaptive mode selection

### 2. Motivation & Gaps
- Existing multimodal models follow fixed reasoning patterns, limiting their adaptability to different tasks.

### 3. Core Idea
- SwimBird adopts a hybrid autoregressive paradigm that allows for adaptive switching between text-only, vision-only, and interleaved reasoning modes.

### 4. Method
- **Pipeline**: SwimBird utilizes a systematic curation and mode-labeling strategy for effective multi-mode training.
- **Architecture / Loss / Training**: A cosine learning rate scheduler is applied with an initial learning rate of 1e-5, and only LLM parameters are updated during training.
- **Complexity / Resources**: Training is performed on A100-80G GPUs with a global batch size of 128.

</details>

## model collapse

### [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/pdf/2602.06039v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-Agent Reasoning

### 2. Motivation & Gaps
- The paper addresses the inefficiencies in multi-agent systems that rely on fixed topologies and communication patterns, which can lead to redundant computations and increased resource consumption.

### 3. Core Idea
- DyTopo introduces a dynamic topology routing mechanism that adapts communication patterns based on semantic matching, improving efficiency and accuracy in multi-agent reasoning tasks.

### 4. Method
- **Pipeline**: The method involves initializing agents, performing single-pass inference, inducing topology based on semantic relevance, and updating memory through deterministic message ordering.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: DyTopo achieves high accuracy with significantly lower token consumption and latency compared to fixed-horizon baselines.

</details>
