# Daily Paper Digest Â· 2026-02-01
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Remember Me, Not Save Me: A Collective Memory System for Evolving Virtual Identities in Augmented Reality](https://arxiv.org/pdf/2601.20437v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Understanding collective dialogue and digital identity

### 2. Motivation & Gaps
- The paper explores how collective dialogue transforms into coherent digital identity through technical innovations.

### 3. Core Idea
- The system achieved stable ISTP personality emergence from recorded interactions, highlighting the balance between remembering and forgetting in digital memory.

### 4. Method
- **Pipeline**: The system utilizes augmented reality to capture user interactions and synthesize collective memories.
- **Architecture / Loss / Training**: The model employs dual memory paths with conflict detection but not resolution.
- **Complexity / Resources**: The system requires significant computational resources for real-time interaction analysis and memory synthesis.

</details>

### [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/pdf/2601.18633v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Talking head synthesis

### 2. Motivation & Gaps
- The paper addresses the challenges in creating realistic talking head avatars using advanced techniques.

### 3. Core Idea
- The core idea is to utilize Gaussian splatting techniques to enhance the realism and efficiency of talking head synthesis.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates Gaussian splatting with neural rendering techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and computational efficiency during training.
- **Complexity / Resources**: The method is designed to be resource-efficient, allowing for real-time applications.

</details>

### [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/pdf/2601.18851v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Head avatar generation and reenactment

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity head avatar generation from selfie videos, focusing on detail reconstruction.

### 3. Core Idea
- A head avatar generation framework utilizing a detailed reconstruction model based on 3DMMs and a StyleGAN-based generator.

### 4. Method
- **Pipeline**: The method involves a detailed reconstruction model with several losses for head texture recovery and avatar image generation.
- **Architecture / Loss / Training**: Utilizes multiple losses on different dimensions and fields during training.
- **Complexity / Resources**: Utilizes multiple StyleGAN-based networks and requires a 3-minute selfie video for training.

</details>

## video understanding

### [UEval: A Benchmark for Unified Multimodal Generation](https://arxiv.org/pdf/2601.22155v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- open-ended tasks in UEval

### 2. Motivation & Gaps
- The paper discusses the advancements in unified multimodal models that integrate various modalities for understanding and generation tasks.

### 3. Core Idea
- Produce step-by-step visual guides for various tasks.

### 4. Method
- **Pipeline**: The method involves a unified framework that integrates various modalities and utilizes advanced training techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances the contributions from different modalities during training.
- **Complexity / Resources**: The model requires significant computational resources for training and inference due to its complexity.

</details>

### [Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions](https://arxiv.org/pdf/2601.22150v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyzing illusion-specific biases in models

### 2. Motivation & Gaps
- The study investigates how different models respond to visual illusions when only the illusion-inducing elements are presented.

### 3. Core Idea
- The models exhibit varying degrees of template bias when responding to visual illusions, with some showing extreme reliance on memorized patterns.

### 4. Method
- **Pipeline**: The study employs a perturbation pipeline to isolate the contribution of visual factors in illusions.
- **Architecture / Loss / Training**: Perception-first architectures that compare visual evidence before drawing conclusions.
- **Complexity / Resources**: The study evaluates 15 recent models from four families, including OpenAI, Anthropic, Google, and Qwen.

</details>

### [JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion](https://arxiv.org/pdf/2601.22143v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation and Dubbing

### 2. Motivation & Gaps
- The paper addresses the challenges in generating accurate video content that aligns with audio inputs, particularly in multilingual contexts.

### 3. Core Idea
- The model employs a dual learning rate strategy and masked loss training to enhance audio-visual alignment and focus on speaker-specific features.

### 4. Method
- **Pipeline**: The model is trained on preprocessed video-audio pairs with a focus on speaker-specific features and scene context.
- **Architecture / Loss / Training**: Masked loss training with a 10:1 ratio between foreground and background regions.
- **Complexity / Resources**: Trained for 2,000 steps with batch size 1, gradient checkpointing, and mixed-precision on video-audio pairs at 960x544 resolution.

</details>

## model collapse

### [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/pdf/2601.22159v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cybersecurity Benchmarking and Evaluation

### 2. Motivation & Gaps
- The paper addresses the need for specialized language models in cybersecurity, highlighting the limitations of general-purpose models in this domain.

### 3. Core Idea
- RedSage is designed to improve performance in cybersecurity tasks by leveraging specialized training and evaluation protocols.

### 4. Method
- **Pipeline**: The RedSage pipeline includes continued pretraining, supervised fine-tuning, and deployment of a decision-making optimizer.
- **Architecture / Loss / Training**: The model architecture is optimized for both accuracy and efficiency, with a focus on reducing hallucinations in outputs.
- **Complexity / Resources**: The total computational cost for the RedSage-8B pipeline is approximately 4,096 GPU-hours.

</details>

### [One-step Latent-free Image Generation with Pixel Mean Flows](https://arxiv.org/pdf/2601.22158v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in image generation by proposing a novel method that leverages pixel mean flows.

### 3. Core Idea
- The core idea is to utilize pixel mean flows for latent-free image generation, enhancing the efficiency and quality of generated images.

### 4. Method
- **Pipeline**: The method involves a training pipeline that incorporates pixel mean flows and various configurations for optimal performance.
- **Architecture / Loss / Training**: The architecture employs perceptual loss based on LPIPS and ConvNeXt-V2, with a focus on maintaining high fidelity in generated images.
- **Complexity / Resources**: The implementation is based on JAX and TPUs, requiring significant computational resources for training.

</details>

### [Discovering Hidden Gems in Model Repositories](https://arxiv.org/pdf/2601.22157v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Model selection in text generation

### 2. Motivation & Gaps
- The research focuses on weight space learning to derive insights from model populations and improve existing models without needing in-distribution data.

### 3. Core Idea
- Existing model populations contain valuable models that can outperform commonly used models without applying population-level methods.

### 4. Method
- **Pipeline**: The method involves selecting models from popular model trees, evaluating them under consistent conditions, and using a custom scheduler for efficient model discovery.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The evaluation involves over 2,000 models and a custom query selection process to ensure unbiased representation.

</details>
