# Daily Paper Digest Â· 2026-02-08
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/pdf/2602.04317v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Monocular human reconstruction

### 2. Motivation & Gaps
- The paper addresses the need for improved stability and geometric plausibility in human reconstruction from monocular images.

### 3. Core Idea
- The proposed method optimizes human pose and attributes jointly to enhance reconstruction quality and stability.

### 4. Method
- **Pipeline**: The method involves a joint optimization framework that refines human poses and camera trajectories simultaneously.
- **Architecture / Loss / Training**: The loss function combines LBS constraints, SMPL geometric constraints, and dynamic attribute regularization.
- **Complexity / Resources**: The method requires moderate computational resources, with specific weights assigned to different loss components.

</details>

### [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/pdf/2602.04913v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Animation Generation

### 2. Motivation & Gaps
- Current audio-driven approaches often produce limited expressiveness and fail to capture the nuances of emotional context in facial animations.

### 3. Core Idea
- A2-LLM integrates language processing, audio generation, and facial animation into a single multimodal large model, enabling context-aware and emotionally congruent facial expressions.

### 4. Method
- **Pipeline**: End-to-end framework that unifies multiple modalities.
- **Architecture / Loss / Training**: Joint training strategy to ensure precise phase alignment and synchronization.
- **Complexity / Resources**: Requires significant computational resources for training and inference.

</details>

### [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/pdf/2602.03439v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Ontology and knowledge graph construction using large language models

### 2. Motivation & Gaps
- The paper addresses the need for efficient methods to construct ontologies and knowledge graphs, leveraging the capabilities of large language models (LLMs).

### 3. Core Idea
- Utilizing large language models to enhance the construction of ontologies and knowledge graphs by automating the extraction and structuring of information.

### 4. Method
- **Pipeline**: The proposed method involves a pipeline that integrates LLMs for data extraction, processing, and ontology construction.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method requires significant computational resources for training and inference with large language models.

</details>

## video understanding

### [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/pdf/2602.06043v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Continual Learning

### 2. Motivation & Gaps
- This work represents one of the earliest attempts to enable parameter-efficient continual learning, leveraging either low-rank adapters or data.

### 3. Core Idea
- The core idea is to enable parameter-efficient continual learning through the use of low-rank adapters or data, while addressing the limitations of existing models.

### 4. Method
- **Pipeline**: Continual model merging and learning at scale using low-rank adapters.
- **Architecture / Loss / Training**: Utilizes a backbone model like RoBERTa and Flux for various tasks.
- **Complexity / Resources**: The method reduces computational resources required for training, thereby lowering energy consumption and carbon footprints.

</details>

### [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/pdf/2602.06041v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Camera pose prediction from perspective descriptions

### 2. Motivation & Gaps
- The paper addresses the challenge of predicting camera pose based on textual descriptions of scenes, which is crucial for spatial reasoning tasks.

### 3. Core Idea
- The core idea is to fine-tune a language model with LoRA while jointly training pose-related modules to improve camera pose prediction accuracy.

### 4. Method
- **Pipeline**: The method involves a weighted sum of language modeling loss and pose regression loss during training.
- **Architecture / Loss / Training**: The architecture includes a language backbone fine-tuned with LoRA and pose-related modules.
- **Complexity / Resources**: Utilizes 4 GPUs with specific training hyperparameters for effective training.

</details>

### [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/pdf/2602.06040v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning with adaptive mode selection

### 2. Motivation & Gaps
- Existing multimodal models follow fixed reasoning patterns, limiting their adaptability to different tasks.

### 3. Core Idea
- SwimBird is a reasoning-switchable MLLM that can adaptively switch among text-only, vision-only, and interleaved vision-text reasoning modes.

### 4. Method
- **Pipeline**: Hybrid autoregressive paradigm with dynamic latent visual token allocation.
- **Architecture / Loss / Training**: Utilizes a systematic curation and mode-labeling strategy for effective multi-mode training.
- **Complexity / Resources**: Extensive experiments conducted to validate performance across various benchmarks.

</details>

## model collapse

### [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/pdf/2602.06039v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-Agent Reasoning

### 2. Motivation & Gaps
- The paper addresses the inefficiencies in multi-agent systems that rely on fixed topologies and communication patterns, which can lead to redundant computations and increased resource consumption.

### 3. Core Idea
- DyTopo introduces a dynamic topology routing mechanism that adapts the communication structure among agents based on semantic matching, improving efficiency and accuracy in multi-agent reasoning tasks.

### 4. Method
- **Pipeline**: The method involves initializing agents, performing single-pass inference, inducing topology based on semantic relevance, and updating memory and routing based on task requirements.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: DyTopo achieves high accuracy with significantly lower token consumption and latency compared to fixed-horizon baselines.

</details>
