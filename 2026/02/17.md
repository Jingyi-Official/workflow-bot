# Daily Paper Digest Â· 2026-02-17
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [A System of Care, Not Control: Co-Designing Online Safety and Wellbeing Solutions with Guardians ad Litem for Youth in Child Welfare](https://arxiv.org/pdf/2602.13989v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Computational Narrative Analysis

### 2. Motivation & Gaps
- The paper addresses the complexities of risk in algorithmic systems, particularly in the context of child welfare.

### 3. Core Idea
- The paper proposes a new framework for understanding risk in algorithmic systems through narrative analysis of case notes.

### 4. Method
- **Pipeline**: The method involves collecting and analyzing case notes from child welfare systems to identify patterns and insights.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The analysis requires access to comprehensive case note data and computational tools for narrative analysis.

</details>

### [OMEGA-Avatar: One-shot Modeling of 360Â° Gaussian Avatars](https://arxiv.org/pdf/2602.11693v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Expression Reenactment

### 2. Motivation & Gaps
- The goal of Expression Reenactment is to transfer the pose and expression from a driving image to the source image while preserving the source identity and maintaining high fidelity.

### 3. Core Idea
- To create a complete canonical 3D Gaussian avatar by combining Gaussian primitives from vertex and UV branches, and animating it using parameters extracted from a driving image.

### 4. Method
- **Pipeline**: The pipeline involves employing a face tracker to extract expression and pose parameters, which are then injected into the canonical avatar to animate it.
- **Architecture / Loss / Training**: The total objective function includes L_reconstruction, L_perceptual, L_mv, and L_local, with hyperparameters to balance contributions.
- **Complexity / Resources**: The model is trained on two NVIDIA H100 GPUs for 250,000 iterations with a batch size of 6.

</details>

### [ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles](https://arxiv.org/pdf/2602.11575v2)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Training visual navigation policies using real-to-sim dynamic simulation

### 2. Motivation & Gaps
- The paper addresses the need for effective training of visual navigation policies in dynamic environments using photorealistic simulations.

### 3. Core Idea
- ReaDy-Go integrates a GS scene, a human animation module, and planners to generate photorealistic navigation datasets for dynamic scenarios.

### 4. Method
- **Pipeline**: The pipeline consists of a GS scene, human animation module, expert planner, and human planner to create dynamic navigation scenarios.
- **Architecture / Loss / Training**: The policy consists of ten convolutional layers with residual connections and three MLP layers, trained with the Adam optimizer.
- **Complexity / Resources**: The model is efficient, requiring only 2M parameters, significantly less than general models.

</details>

## video understanding

### [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/pdf/2602.15031v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Content generation and propagation in augmented reality

### 2. Motivation & Gaps
- The need for low latency content generation in augmented reality applications to enhance user experience.

### 3. Core Idea
- EditCtrl uses optical flow to propagate content from previous frames to future frames, ensuring low latency and visually appealing results.

### 4. Method
- **Pipeline**: Optical flow propagation from last known frame to next future frame.
- **Architecture / Loss / Training**: The architecture employs a combination of autoregressive models and diffusion techniques, focusing on minimizing loss during training.
- **Complexity / Resources**: The method is designed to be efficient, allowing for real-time editing without significant computational overhead.

</details>

### [Scaling Beyond Masked Diffusion Language Models](https://arxiv.org/pdf/2602.15014v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluating the performance of diffusion language models on various benchmarks

### 2. Motivation & Gaps
- This paper addresses the limitations of existing diffusion language models and explores their scaling laws and performance across different tasks.

### 3. Core Idea
- The study investigates the scaling laws of Uniform-state diffusion models and their performance compared to autoregressive models, emphasizing the importance of sampling efficiency and downstream performance.

### 4. Method
- **Pipeline**: Training diffusion models with a focus on likelihood-based benchmarks and mathematical reasoning tasks.
- **Architecture / Loss / Training**: Using AdamW optimizer with a cosine learning rate schedule and fine-tuning on augmented datasets.
- **Complexity / Resources**: Training models on large datasets with significant computational resources, including 1.4T tokens processed.

</details>

### [TouchFusion: Multimodal Wristband Sensing for Ubiquitous Touch Interactions](https://arxiv.org/pdf/2602.15011v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal sensing for touch interactions

### 2. Motivation & Gaps
- The paper addresses the need for effective touch interaction sensing using a multimodal wristband.

### 3. Core Idea
- The proposed system utilizes a combination of CNN and LSTM architectures to analyze sensor data for touch interactions.

### 4. Method
- **Pipeline**: The system processes raw sensor signals through a series of convolutional and LSTM layers.
- **Architecture / Loss / Training**: Models are trained without featurization on raw signals, with specific architectures for different touch states.
- **Complexity / Resources**: The models have varying complexities based on the architecture, with some requiring significant computational resources.

</details>

## model collapse

### [Image Generation with a Sphere Encoder](https://arxiv.org/pdf/2602.15030v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in image generation using diffusion models and explores the use of spherical latent spaces for improved stability in variational autoencoders.

### 3. Core Idea
- The core idea is to utilize spherical latent spaces to enhance the stability and performance of variational autoencoders in image generation tasks.

### 4. Method
- **Pipeline**: The proposed method involves a pipeline that integrates spherical latent spaces into the diffusion model framework.
- **Architecture / Loss / Training**: The architecture employs a loss function that encourages consistency in the generated outputs while training on spherical representations.
- **Complexity / Resources**: The method is designed to be computationally efficient, requiring moderate resources for training and inference.

</details>

### [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/pdf/2602.15029v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Theoretical analysis of word embeddings and their geometric properties

### 2. Motivation & Gaps
- The paper explores how symmetries in language statistics influence the geometry of model representations, particularly in the context of language models and their ability to capture semantic relationships.

### 3. Core Idea
- The paper discusses the spectral decomposition of operators related to language statistics and their implications for model representations.

### 4. Method
- **Pipeline**: The method involves diagonalizing linear differential operators and analyzing eigenfunctions and eigenvalues.
- **Architecture / Loss / Training**: Utilizes a modified co-occurrence matrix and kernel functions to derive embeddings.
- **Complexity / Resources**: Involves global spectral transformations and requires knowledge of co-occurrence probabilities.

</details>
