# Daily Paper Digest Â· 2026-01-03
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/pdf/2512.23576v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the need for real-time interactive long video generation, which is a significant challenge in the field of video synthesis.

### 3. Core Idea
- The core idea is to develop a method for generating long videos interactively in real-time, leveraging autoregressive models to ensure coherence and responsiveness.

### 4. Method
- **Pipeline**: The distillation pipeline consists of two sequential stages: ODE initialization and on-policy distribution matching distillation.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances quality and coherence across generated frames.
- **Complexity / Resources**: Batch size 64, Learning rate 4eâˆ’5, Optimizer AdamW, Î²1 = 0.9, Î²2 = 0.999, Ïµ= 1eâˆ’8, weight decay=0

</details>

### [SoulX-LiveTalk: Real-Time Infinite Streaming of Audio-Driven Avatars via Self-Correcting Bidirectional Distillation](https://arxiv.org/pdf/2512.23379v2)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Audio-driven avatar video generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating avatar videos that can adapt to infinite-length audio inputs, enhancing the realism and interactivity of virtual avatars.

### 3. Core Idea
- The proposed method utilizes a novel approach to generate avatar videos that can seamlessly adapt to audio inputs of any length, ensuring continuous and coherent animation.

### 4. Method
- **Pipeline**: The method involves a multi-stage pipeline that processes audio inputs and generates corresponding avatar animations in real-time.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and coherence in the generated videos, trained on a diverse dataset of audio-visual pairs.
- **Complexity / Resources**: The model is designed to be efficient, requiring moderate computational resources while maintaining high-quality output.

</details>

### [StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars](https://arxiv.org/pdf/2512.22065v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Talking Avatar Generation

### 2. Motivation & Gaps
- This work focuses on talking avatar generation for constructive, human-centered applications.

### 3. Core Idea
- Real-time generation of talking avatars using advanced generative models.

### 4. Method
- **Pipeline**: The method utilizes a diffusion model architecture to generate portrait videos based on audio input.
- **Architecture / Loss / Training**: Incorporates adversarial distillation and consistency-aware discriminators to enhance video quality and temporal consistency.
- **Complexity / Resources**: Utilizes two H800 GPUs for coding processes.

</details>

## video understanding

### [SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time](https://arxiv.org/pdf/2512.25075v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation with Controlled Camera Trajectories

### 2. Motivation & Gaps
- The paper addresses the challenge of generating long-range videos with precise control over camera trajectories and temporal manipulation.

### 3. Core Idea
- The proposed CamÃ—Time dataset enables enhanced temporal control in video generation, allowing for diverse camera movements and temporal dynamics.

### 4. Method
- **Pipeline**: The model generates video segments conditioned on both the source video and previously generated segments, enabling continuous video generation.
- **Architecture / Loss / Training**: The architecture includes a camera embedder, animation-time embedder, self-attention layers, and a projector, trained to maintain contextual coherence.
- **Complexity / Resources**: The model requires significant computational resources for training and generating high-fidelity videos.

</details>

### [Randomization Times under Quantum Chaotic Hamiltonian Evolution](https://arxiv.org/pdf/2512.25074v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the randomization time of quantum states under chaotic dynamics

### 2. Motivation & Gaps
- The study aims to define and identify the intrinsic randomization time generated by quantum dynamics, focusing on the transition from unentangled to Haar-random states.

### 3. Core Idea
- The study reveals that non-random Hamiltonians can randomize states faster than their charge-conserving counterparts, with significant implications for quantum dynamics.

### 4. Method
- **Pipeline**: Define a temporal ensemble of states evolved from unentangled initial states and analyze their observables over time.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Utilizes the Mixed Field Ising Model (MFIM) for quantum chaotic dynamics.

</details>

### [Classification of Interacting Topological Crystalline Superconductors in Three Dimensions and Beyond](https://arxiv.org/pdf/2512.25069v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Classifying three-dimensional interacting fermionic SPTs and topological crystalline superconductors

### 2. Motivation & Gaps
- The paper addresses the classification of fully gapped topological superconductors protected by crystalline symmetries, filling a gap between abstract classifications and practical definitions.

### 3. Core Idea
- Developing a framework that combines domain-wall decoration and fermionic crystalline equivalence principles to classify topological crystalline superconductors.

### 4. Method
- **Pipeline**: Utilizes a domain-wall decoration framework and fermionic crystalline equivalence principle to classify superconductors.
- **Architecture / Loss / Training**: Obtain formulas for all three layers of decoration (n2, n3, Î½4) that are amenable to automatic computation.
- **Complexity / Resources**: Utilizes algorithms for group theory calculations and classification of topological phases.

</details>

## model collapse

### [Edit3r: Instant 3D Scene Editing from Sparse Unposed Images](https://arxiv.org/pdf/2512.25071v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D scene reconstruction and editing

### 2. Motivation & Gaps
- Existing methods struggle with robustness and consistency in 3D scene editing, particularly under varying input conditions.

### 3. Core Idea
- Edit3r unifies reconstruction and instruction-driven editing of 3D scenes from unposed, instruction-edited images using a single-pass prediction of edited 3D Gaussian splats.

### 4. Method
- **Pipeline**: Decouples 2D editing from feed-forward reconstruction, allowing different editors at test time.
- **Architecture / Loss / Training**: Incorporates a SAM2-based recoloring strategy and an asymmetric input scheme for improved supervision and coherence.
- **Complexity / Resources**: Lightweight 3D regularization on Gaussian centers to maintain multi-view coherence.

</details>
