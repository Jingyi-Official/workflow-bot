# Daily Paper Digest Â· 2026-01-29
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Remember Me, Not Save Me: A Collective Memory System for Evolving Virtual Identities in Augmented Reality](https://arxiv.org/pdf/2601.20437v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Understanding collective dialogue and digital identity emergence

### 2. Motivation & Gaps
- The paper explores how collective dialogue transforms into coherent digital identity through technical innovations.

### 3. Core Idea
- The system achieved stable ISTP personality emergence from recorded interactions, highlighting the importance of trust and digital memory.

### 4. Method
- **Pipeline**: Deployment of the DCM model at the Jinan International Biennale, utilizing AR interfaces for participant engagement.
- **Architecture / Loss / Training**: The model employs dual memory paths with conflict detection but not resolution.
- **Complexity / Resources**: The system uses a mathematical weighting formula for balancing frequency, emotion, and resonance in collective memory formation.

</details>

### [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/pdf/2601.18633v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Talking head synthesis

### 2. Motivation & Gaps
- The paper addresses the challenges in creating realistic talking head avatars using advanced techniques.

### 3. Core Idea
- The core idea is to utilize Gaussian splatting techniques to enhance the realism and efficiency of talking head synthesis.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates Gaussian splatting with neural rendering techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and computational efficiency during training.
- **Complexity / Resources**: The method is designed to be resource-efficient, allowing for real-time applications.

</details>

### [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/pdf/2601.18851v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Head avatar generation and reenactment

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity head avatar generation from selfie videos, focusing on detailed texture reconstruction.

### 3. Core Idea
- A head avatar generation framework utilizing a detailed reconstruction model based on 3DMMs and a StyleGAN-based generator.

### 4. Method
- **Pipeline**: The method involves a detailed reconstruction model with multiple losses for head texture recovery and avatar image generation.
- **Architecture / Loss / Training**: Utilizes several losses on different dimensions and fields during training.
- **Complexity / Resources**: Utilizes multiple StyleGAN-based networks and requires a 3-minute selfie video for training.

</details>

## video understanding

### [Probabilistic Interpolation of Sagittarius A*'s Multi-Wavelength Light Curves Using Diffusion Models](https://arxiv.org/pdf/2601.20863v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluate model performance on observational data of Sgr A*

### 2. Motivation & Gaps
- The paper discusses the need for diverse training data in modeling the variability of accreting black holes (BHs) and the limitations of using a single simulation dataset.

### 3. Core Idea
- The paper proposes moving towards hybrid modeling frameworks that integrate empirical structures, domain-specific priors, and physical realism to improve generalization from simulations to real observational data.

### 4. Method
- **Pipeline**: Models are conditioned on all available observations to reconstruct the full multi-wavelength light curve.
- **Architecture / Loss / Training**: Models include MOGP, TripletFormer, and CSPD, trained on synthetic data without exposure to observational data.
- **Complexity / Resources**: Each model is trained for a maximum of 12 hours using one NVIDIA V100 GPU with 16GB of memory and access to 32GB of RAM.

</details>

### [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/pdf/2601.20857v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Gaussian Splatting

### 2. Motivation & Gaps
- The paper addresses the challenges in 3D Gaussian Splatting and proposes a method to enhance its performance using diffusion models without the need for fine-tuning.

### 3. Core Idea
- The core idea is to utilize diffusion models to boost the performance of 3D Gaussian Splatting without requiring fine-tuning, thereby simplifying the process and improving results.

### 4. Method
- **Pipeline**: The method involves a structured sampling strategy during 3D refinement and the application of certainty as guidance during denoising.
- **Architecture / Loss / Training**: The loss term is defined using Fisher information, which is derived from the uncertainty attribute of 3DGS.
- **Complexity / Resources**: The method requires computational resources for 3D refinement and the generation of extrapolated views.

</details>

### [Extreme winds on the emerging dayside of an ultrahot Jupiter](https://arxiv.org/pdf/2601.20849v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Atmospheric characterization of exoplanets

### 2. Motivation & Gaps
- The study aims to unveil trends in atmospheric dynamics of ultra-hot Jupiters (UHJs) and constrain the mechanisms driving these trends.

### 3. Core Idea
- Utilizing high-resolution phase-resolved emission spectroscopy to characterize the atmosphere of KELT-9 b, revealing significant atmospheric winds and thermal structures.

### 4. Method
- **Pipeline**: Use the ESO sky tool molecfit to correct for telluric features
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: High spectral resolution (R âˆ¼98,000) and 3D radiative transfer simulations.

</details>

## model collapse

### [Bottom-heavy initial mass functions reveal hidden mass in early galaxies](https://arxiv.org/pdf/2601.20864v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the properties of ultra-massive quiescent galaxies at high redshifts

### 2. Motivation & Gaps
- The study aims to understand the formation and evolution of ultra-massive quiescent galaxies in the early universe.

### 3. Core Idea
- The paper presents findings from the JWST EXCELS survey, focusing on the characteristics and formation of ultra-massive quiescent galaxies at redshifts between 3 and 5.

### 4. Method
- **Pipeline**: JWST Science Calibration Pipeline v1.17.1 and version 1322 of the Calibration Reference Data System (CRDS)
- **Architecture / Loss / Training**: Simultaneous fitting of the entire rest-frame wavelength range using the absorption line fitter (alf), which accounts for variability in abundance patterns and IMF shape.
- **Complexity / Resources**: Utilizes advanced observational techniques and data from the JWST.

</details>

### [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/pdf/2601.20861v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Fine-tuning LLMs using Evolutionary Strategies

### 2. Motivation & Gaps
- The paper investigates the performance of Evolutionary Strategies (ES) for fine-tuning large language models (LLMs) and compares it with Gradient-based Reinforcement Policy Optimization (GRPO).

### 3. Core Idea
- The study compares Evolution Strategies (ES) and Generalized Policy Optimization (GRPO) in terms of model training and performance, particularly focusing on KL-divergence and its impact on task performance.

### 4. Method
- **Pipeline**: The training pipeline involves sampling candidate outputs, evaluating them for rewards, and updating the policy using a policy-gradient objective with KL regularization.
- **Architecture / Loss / Training**: Implemented on the VERL library using the HybridFlow engine, with specific hyperparameters for training.
- **Complexity / Resources**: Training was conducted on NVIDIA RTX A6000 GPUs using Fully Sharded Data Parallel (FSDP) protocol.

</details>
