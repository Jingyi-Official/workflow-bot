# Daily Paper Digest Â· 2026-01-21
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/pdf/2601.13837v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D head avatar generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating 3D head avatars from limited input data, focusing on improving the consistency and quality of the generated avatars.

### 3. Core Idea
- The model predicts offsets of per-Gaussian position and color from an MLP to model facial dynamics.

### 4. Method
- **Pipeline**: The method involves a multi-step pipeline that includes feature extraction, Gaussian attribute regression, and 3D geometry consistency checks.
- **Architecture / Loss / Training**: The architecture incorporates a VAE decoder with cross-view attention and uses VGGT geometry loss for training.
- **Complexity / Resources**: Higher computation cost and longer runtime with more input views.

</details>

### [HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction](https://arxiv.org/pdf/2601.13801v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Human-Drone Interaction

### 2. Motivation & Gaps
- The integration of human-like avatars in drones raises ethical concerns regarding privacy, consent, and potential biases in demographic estimations.

### 3. Core Idea
- HoverAI combines visual projection, conversational AI, and demographic-adaptive avatar generation into a mobile platform for enhanced human-drone interaction.

### 4. Method
- **Pipeline**: Integration of MEMS laser projection with a semi-rigid screen, multimodal perception through vision and speech, and closed-loop interaction via LLM-based dialogue and face analysis.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The system utilizes a 2D MEMS scanning mirror, a Speedybee F405V4 flight controller, and a close-talking microphone.

</details>

### [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/pdf/2601.13148v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D virtual human avatar creation

### 2. Motivation & Gaps
- The paper addresses limitations in state-of-the-art methods for creating interactive, photorealistic 3D virtual human avatars.

### 3. Core Idea
- The proposed system integrates dynamic full-body 3D reconstruction, audio-driven facial animation, and LLM-powered conversational interaction to create lifelike avatars.

### 4. Method
- **Pipeline**: The method involves training models on audio, expression parameters, and head-pose sequences, followed by testing on unseen data.
- **Architecture / Loss / Training**: Utilizes PSNR, SSIM, LPIPS, and Sync-C for evaluation.
- **Complexity / Resources**: Achieves real-time performance with an average frame rate of 250 FPS on specified hardware.

</details>

## video understanding

### [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/pdf/2601.14255v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dense prediction

### 2. Motivation & Gaps
- The paper addresses the need for high-quality dense prediction in visual tasks using diffusion-based models.

### 3. Core Idea
- Utilizing diffusion-based techniques to enhance the quality and accuracy of dense predictions in visual tasks.

### 4. Method
- **Pipeline**: The method involves a diffusion-based approach that iteratively refines predictions for dense visual tasks.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for dense prediction tasks, optimizing for both accuracy and computational efficiency.
- **Complexity / Resources**: The model is designed to balance complexity and resource requirements, making it feasible for real-time applications.

</details>

### [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/pdf/2601.14253v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Motion Reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of generating 4D content from 3D motion data, focusing on the need for effective motion trajectory prediction.

### 3. Core Idea
- The method extends existing static 3D assets to dynamic 4D content, enabling flexible application scenarios.

### 4. Method
- **Pipeline**: The pipeline consists of a shape encoder for 3D features and a motion latent block for temporal dynamics, processed through a series of attention layers.
- **Architecture / Loss / Training**: The architecture employs a QK-Norm cross-attention mechanism and a two-layer MLP for decoding motion trajectories, trained with MSE loss.
- **Complexity / Resources**: The method utilizes AdamW optimizer with a cosine annealing learning rate schedule, gradient checkpointing, and mixed precision training to manage computational resources.

</details>

### [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/pdf/2601.14250v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Transfer Combination

### 2. Motivation & Gaps
- The approach enables seamless combination of multiple video transfer operations.

### 3. Core Idea
- Concatenating reference video tokens and MLLM tokens allows for the combination of various video transfer tasks.

### 4. Method
- **Pipeline**: The method involves a novel pipeline that integrates advanced generative models for animation.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions to ensure identity preservation and animation quality.
- **Complexity / Resources**: The method requires significant computational resources for training and inference.

</details>

## model collapse

### [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/pdf/2601.14256v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Compression and Representation Learning

### 2. Motivation & Gaps
- The paper addresses the challenges in video compression and representation learning using implicit neural representations (INRs).

### 3. Core Idea
- The core idea is to utilize implicit neural representations to achieve efficient and unified encoding of video data.

### 4. Method
- **Pipeline**: The method involves training a multi-layer perceptron with positional encoding to represent video frames as implicit functions.
- **Architecture / Loss / Training**: Trained with LPIPS and SSIM losses in addition to pixel-wise and DINOv3 MSE losses.
- **Complexity / Resources**: The training process is resource-intensive, requiring significant computational power and time.

</details>
