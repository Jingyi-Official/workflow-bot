# Daily Paper Digest Â· 2026-01-30
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Remember Me, Not Save Me: A Collective Memory System for Evolving Virtual Identities in Augmented Reality](https://arxiv.org/pdf/2601.20437v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Understanding collective dialogue and digital identity

### 2. Motivation & Gaps
- The paper explores how collective dialogue transforms into coherent digital identity through technical innovations.

### 3. Core Idea
- The system achieved stable ISTP personality emergence from recorded interactions, highlighting trust and digital memory.

### 4. Method
- **Pipeline**: The system analyzes synthetic photos and text dialogues to ground AI identity in urban experiences.
- **Architecture / Loss / Training**: The model employs dual memory paths with conflict detection but not resolution.
- **Complexity / Resources**: The system requires a multimodal LLM and AR interfaces for deployment.

</details>

### [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/pdf/2601.18633v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Talking head synthesis

### 2. Motivation & Gaps
- The paper addresses the challenges in creating realistic talking head avatars using advanced techniques.

### 3. Core Idea
- The core idea is to utilize Gaussian splatting techniques to enhance the realism and efficiency of talking head synthesis.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates Gaussian splatting with neural rendering techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and computational efficiency during training.
- **Complexity / Resources**: The method is designed to be resource-efficient, allowing for real-time applications.

</details>

### [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/pdf/2601.18851v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Head avatar generation and reenactment

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity head avatar generation from selfie videos, focusing on detailed texture reconstruction.

### 3. Core Idea
- A head avatar generation framework that utilizes a detailed reconstruction model with multiple losses for texture recovery and avatar image generation.

### 4. Method
- **Pipeline**: The method involves a detailed reconstruction model integrated with a StyleGAN-based generator.
- **Architecture / Loss / Training**: The framework employs several losses on different dimensions for effective training.
- **Complexity / Resources**: Requires a 3-minute selfie video for training and utilizes two face datasets from MEAD and IMA vatar.

</details>

## video understanding

### [UEval: A Benchmark for Unified Multimodal Generation](https://arxiv.org/pdf/2601.22155v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- open-ended tasks in UEval

### 2. Motivation & Gaps
- The need for models to produce step-by-step visual guides for various tasks.

### 3. Core Idea
- To generate step-by-step visual guides for various tasks using different models.

### 4. Method
- **Pipeline**: The method involves a unified framework that integrates various modalities and utilizes advanced training techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances the contributions from different modalities during training.
- **Complexity / Resources**: The model requires significant computational resources for training and inference due to its complexity.

</details>

### [Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions](https://arxiv.org/pdf/2601.22150v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyzing illusion-specific biases in models

### 2. Motivation & Gaps
- The study investigates how different models respond to visual illusions when only the illusion-inducing elements are presented.

### 3. Core Idea
- The research reveals that certain models exhibit extreme template bias when responding to visual illusions, particularly in the absence of visual evidence.

### 4. Method
- **Pipeline**: The study employs a perturbation pipeline to isolate the contribution of visual factors in illusions.
- **Architecture / Loss / Training**: Perception-first architectures that explicitly compare visual evidence before drawing conclusions.
- **Complexity / Resources**: The study evaluates 15 recent models from four families, including OpenAI, Anthropic, Google, and Qwen.

</details>

### [JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion](https://arxiv.org/pdf/2601.22143v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation and Dubbing

### 2. Motivation & Gaps
- The paper addresses the challenges in generating accurate video content, particularly focusing on multilingual prompts and the synchronization of audio and visual elements.

### 3. Core Idea
- The paper proposes a novel framework for video generation that utilizes structured multilingual prompts and advanced training techniques to enhance the quality and accuracy of generated videos.

### 4. Method
- **Pipeline**: The framework processes generated language-switching videos by encoding visual and audio components separately, applying noise selectively, and denoising them jointly using a diffusion model.
- **Architecture / Loss / Training**: The model employs a masked loss training strategy focusing on speaker-specific features while preserving scene context, with a dual learning rate strategy for video and audio modules.
- **Complexity / Resources**: The training utilizes rank-128 LoRA modules and a comprehensive evaluation framework with standard and challenging benchmarks.

</details>

## model collapse

### [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/pdf/2601.22159v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cybersecurity Benchmarking and Evaluation

### 2. Motivation & Gaps
- The paper addresses the need for specialized language models in cybersecurity, highlighting the limitations of general-purpose models in this domain.

### 3. Core Idea
- RedSage is designed to improve performance in cybersecurity tasks by leveraging specialized training and evaluation protocols.

### 4. Method
- **Pipeline**: The RedSage pipeline includes continued pretraining, supervised fine-tuning, and deployment of a decision-making optimizer.
- **Architecture / Loss / Training**: The architecture employs a transformer-based model with specific loss functions tailored for cybersecurity tasks.
- **Complexity / Resources**: The total pipeline requires approximately 134 hours and 4,096 GPU-hours for training and evaluation.

</details>

### [One-step Latent-free Image Generation with Pixel Mean Flows](https://arxiv.org/pdf/2601.22158v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in image generation by proposing a novel method that leverages pixel mean flows.

### 3. Core Idea
- The core idea is to utilize pixel mean flows for latent-free image generation, enhancing the efficiency and quality of generated images.

### 4. Method
- **Pipeline**: The method involves a training pipeline that incorporates various configurations and hyper-parameters to optimize image generation.
- **Architecture / Loss / Training**: The architecture employs perceptual loss based on LPIPS and ConvNeXt-V2, with a focus on maintaining high fidelity in generated images.
- **Complexity / Resources**: The implementation is based on JAX and TPUs, ensuring efficient resource utilization.

</details>

### [Discovering Hidden Gems in Model Repositories](https://arxiv.org/pdf/2601.22157v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Model Retrieval and Evaluation

### 2. Motivation & Gaps
- The study aims to improve model retrieval efficiency and accuracy by identifying hidden gem models that outperform popular base versions with fewer query budgets.

### 3. Core Idea
- The proposed method utilizes correlated sampling to enhance model retrieval performance while reducing the query budget required.

### 4. Method
- **Pipeline**: Correlated sampling for model retrieval across varying query budgets.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method is designed to operate efficiently with half the query budget compared to baseline methods.

</details>
