# Daily Paper Digest Â· 2026-01-04
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/pdf/2512.23576v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the need for real-time interactive long video generation, which is a significant challenge in the field of video synthesis.

### 3. Core Idea
- The core idea is to develop a method for generating long videos interactively in real-time, leveraging autoregressive models to enhance the quality and coherence of the output.

### 4. Method
- **Pipeline**: The distillation pipeline consists of two sequential stages: ODE initialization and on-policy distribution matching distillation.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances quality and coherence across generated frames.
- **Complexity / Resources**: The method is designed to be computationally efficient, allowing for real-time processing on standard hardware.

</details>

### [SoulX-LiveTalk: Real-Time Infinite Streaming of Audio-Driven Avatars via Self-Correcting Bidirectional Distillation](https://arxiv.org/pdf/2512.23379v2)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Audio-driven avatar video generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating avatar videos that can adapt to infinite lengths based on audio input.

### 3. Core Idea
- The proposed method utilizes a novel approach to generate avatar videos that can seamlessly adapt to varying audio inputs over an infinite duration.

### 4. Method
- **Pipeline**: The method involves a pipeline that processes audio input and generates corresponding avatar video frames in real-time.
- **Architecture / Loss / Training**: The architecture employs a loss function that optimizes for both visual fidelity and synchronization with audio.
- **Complexity / Resources**: The model is designed to be resource-efficient, allowing for real-time generation on standard hardware.

</details>

### [StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars](https://arxiv.org/pdf/2512.22065v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- talking avatar generation

### 2. Motivation & Gaps
- This work focuses on talking avatar generation for constructive, human-centered applications.

### 3. Core Idea
- The system supports real-time generation of talking avatars while ensuring ethical considerations are met.

### 4. Method
- **Pipeline**: The method involves a two-stage process: audio processing and video generation using diffusion models.
- **Architecture / Loss / Training**: The architecture employs adversarial training with a consistency-aware discriminator to enhance video quality and temporal stability.
- **Complexity / Resources**: The model is evaluated using two H800 GPUs.

</details>

## video understanding

### [SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time](https://arxiv.org/pdf/2512.25075v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation with Temporal Control

### 2. Motivation & Gaps
- The paper addresses the challenge of generating coherent videos with diverse temporal controls and camera motions.

### 3. Core Idea
- The proposed method, SpaceTimePilot, enables disentangled control over camera motion and temporal dynamics through a novel dataset and embedding strategies.

### 4. Method
- **Pipeline**: The method involves rendering multi-view video sequences and applying temporal warping for training.
- **Architecture / Loss / Training**: Utilizes 1D-Conv embedding for time embedding and incorporates the CamÃ—Time dataset for improved results.
- **Complexity / Resources**: Requires significant computational resources for rendering and training on large datasets.

</details>

### [Randomization Times under Quantum Chaotic Hamiltonian Evolution](https://arxiv.org/pdf/2512.25074v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the randomization time of quantum states under chaotic dynamics

### 2. Motivation & Gaps
- The study aims to define and identify the intrinsic randomization time generated by quantum dynamics, focusing on the transition from unentangled to Haar-random states.

### 3. Core Idea
- The study reveals that non-random Hamiltonians can randomize states faster than their charge-conserving counterparts, with significant implications for quantum dynamics.

### 4. Method
- **Pipeline**: Define a temporal ensemble of states evolved from unentangled initial states and analyze their observables over time.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Utilizes a one-dimensional Mixed Field Ising Model with periodic boundary conditions.

</details>

### [Classification of Interacting Topological Crystalline Superconductors in Three Dimensions and Beyond](https://arxiv.org/pdf/2512.25069v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- To classify 2+1D interacting fermionic symmetry protected topological phases (FSPT) using cohomological methods.

### 2. Motivation & Gaps
- The paper addresses the classification of 2+1D FSPT protected by internal discrete symmetries, focusing on the role of cochains in defining the phases.

### 3. Core Idea
- The classification is characterized by a triplet (n1, n2, Î½3) representing different fermionic decorations and their interactions under symmetry transformations.

### 4. Method
- **Pipeline**: Utilizes cohomological techniques to derive conditions for the classification of FSPT phases.
- **Architecture / Loss / Training**: Obtain formulas for all three layers of decoration (n2, n3, Î½4) that are amenable to automatic computation.
- **Complexity / Resources**: The method involves complex algebraic structures and requires significant computational resources for calculations.

</details>

## model collapse

### [Edit3r: Instant 3D Scene Editing from Sparse Unposed Images](https://arxiv.org/pdf/2512.25071v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D scene reconstruction and editing from unposed images

### 2. Motivation & Gaps
- Existing methods struggle with robustness and consistency in 3D scene editing, particularly under varying input conditions.

### 3. Core Idea
- Edit3r unifies reconstruction and instruction-driven editing of 3D scenes by predicting edited 3D Gaussian splats in a single pass, enabling fast and coherent rendering.

### 4. Method
- **Pipeline**: Decouples 2D editing from feed-forward reconstruction, allowing for different editors at test time.
- **Architecture / Loss / Training**: Utilizes a SAM2-based recoloring strategy and an asymmetric input scheme to ensure cross-view consistency.
- **Complexity / Resources**: Lightweight 3D regularization on Gaussian centers to maintain multi-view coherence.

</details>
