# Daily Paper Digest Â· 2026-01-27
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/pdf/2601.18633v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Talking head synthesis

### 2. Motivation & Gaps
- The paper addresses the challenges in creating realistic talking head avatars using advanced techniques.

### 3. Core Idea
- The core idea is to utilize Gaussian splatting techniques to enhance the realism and efficiency of talking head synthesis.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates Gaussian splatting with neural rendering techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and computational efficiency during training.
- **Complexity / Resources**: The method is designed to be resource-efficient, allowing for real-time applications.

</details>

### [Co-Designing Digital Humans for Online Learning: A Framework for Human-AI Pedagogical Integration](https://arxiv.org/pdf/2601.17434v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Developing a framework for the design and integration of digital teachers in online education

### 2. Motivation & Gaps
- The framework aims to guide the creation of more effective digital teachers and explore new design opportunities.

### 3. Core Idea
- The framework provides a structured approach for designers to develop educational platforms by identifying critical factors in digital teaching.

### 4. Method
- **Pipeline**: An iterative, three-stage process involving conceptualization, application of the framework, and systematic inspection and modification of designs.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The framework incorporates insights from over 87 papers and empirical data from surveys and expert workshops.

</details>

### [SkyReels-V3 Technique Report](https://arxiv.org/pdf/2601.17323v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- SkyReels-V3 represents a significant step toward scalable, controllable, and general-purpose video generation systems.

### 3. Core Idea
- The paper introduces a new model for video generation that emphasizes scalability and control, aiming to improve upon existing generative models.

### 4. Method
- **Pipeline**: The framework integrates reference-based video synthesis, video extension, and audio-driven talking avatar generation.
- **Architecture / Loss / Training**: Utilizes hybrid hierarchical data training and unified multi-segment positional encoding for accurate motion modeling.
- **Complexity / Resources**: Achieves high-definition outputs with 720p video generation and supports various aspect ratios.

</details>

## video understanding

### [MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data](https://arxiv.org/pdf/2601.18792v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Sentiment analysis using MEG data

### 2. Motivation & Gaps
- The study aims to improve sentiment analysis by utilizing magnetoencephalography (MEG) data, but identifies several limitations in current methodologies and datasets.

### 3. Core Idea
- To enhance sentiment prediction capabilities by addressing class imbalance, improving interpretability, and exploring personalized modeling approaches.

### 4. Method
- **Pipeline**: Data is split into training, validation, and test sets. Models are trained using MLP and LSTM architectures to predict sentiment based on brain activity.
- **Architecture / Loss / Training**: MLP with 2 layers and 128 hidden units; LSTM with 2 layers and 128 hidden units, both using a learning rate of 0.0001 and a batch size of 32.
- **Complexity / Resources**: Training involves 10 random seeds for 200 epochs, with MEG data sampled at 250 Hz across 269 sensor channels.

</details>

### [Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods](https://arxiv.org/pdf/2601.18782v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Low-bit quantization of bandlimited graph signals

### 2. Motivation & Gaps
- The paper presents iterative noise-shaping methods for low-bit quantization of bandlimited graph signals.

### 3. Core Idea
- The paper introduces permutation-based methods with structured initialization and random sampling methods that achieve low reconstruction error for graph signals.

### 4. Method
- **Pipeline**: The method involves iterative noise-shaping techniques for quantization.
- **Architecture / Loss / Training**: The architecture focuses on minimizing quantization error while maintaining visual fidelity.
- **Complexity / Resources**: The method is computationally efficient, leveraging graph structures to reduce complexity.

</details>

### [Holography with an Inner Boundary: A Smooth Horizon as a Sum over Horizonless States](https://arxiv.org/pdf/2601.18775v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the nature of black hole horizons and their emergent properties in the context of AdS/CFT.

### 2. Motivation & Gaps
- This paper discusses the relationship between the continuum density of states and the spectrum of microstates in the context of black holes and holographic CFT.

### 3. Core Idea
- The paper discusses the Chern-Simons path integral on a three-manifold with boundary conditions, focusing on the transition amplitude and the role of boundary dynamics in the context of gauge theories.

### 4. Method
- **Pipeline**: The analysis involves a saddle-point approximation of partition functions in the high-temperature limit.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The study relies on modular invariance and basic information about the distribution of states in the holographic CFT.

</details>

## model collapse

### [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/pdf/2601.18796v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Generating plain language summaries and analyzing abstract pairs

### 2. Motivation & Gaps
- The study aims to generate plain language summaries of biomedical abstracts to improve accessibility for a general audience.

### 3. Core Idea
- Utilizing gpt-4o-mini to generate summaries and analyze commonalities and differences between biomedical abstracts.

### 4. Method
- **Pipeline**: The method involves generating summaries using gpt-4o-mini, clustering abstracts with BERTopic, and training models with specific hyperparameters.
- **Architecture / Loss / Training**: The training uses SFTTrainer with specific configurations for optimizing the model parameters.
- **Complexity / Resources**: Training is conducted on one Nvidia H100 GPU, with specific training times for one-phase and two-phase procedures.

</details>

### [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/pdf/2601.18795v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Reinforcement Learning from Human Feedback

### 2. Motivation & Gaps
- The paper revisits reinforcement learning techniques to optimize learning from human feedback in large language models (LLMs).

### 3. Core Idea
- The paper explores the effectiveness of off-policy traces and prefix training in reinforcement learning, particularly focusing on the performance of models like Qwen and Llama.

### 4. Method
- **Pipeline**: The method involves using off-policy traces for training with a focus on reward normalization and policy shaping.
- **Architecture / Loss / Training**: The architecture employs a constant learning rate and specific batch sizes, with adjustments for KL and entropy regularization.
- **Complexity / Resources**: FLOPs are computed based on the number of tokens processed and the model's trainable parameters.

</details>
