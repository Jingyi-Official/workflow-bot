# Daily Paper Digest · 2025-09-24
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](http://arxiv.org/pdf/2509.19297v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction from multi-view images

### 2. Motivation & Gaps
- Existing methods for 3D Gaussian Splatting are limited by a rigid coupling of Gaussian density to input image resolution and high sensitivity to multi-view alignment errors.

- **Related work challenges:**
  - Neural Radiance Fields (NeRF): Computationally intensive and slow to run at inference time.
  - 3D Gaussian Splatting (3DGS): Heavily dependent on the number of input views and struggles with occlusions or low texture.
  - NeRF: Long training times and dependency on accurate geometric proxies.
  - 3DGS: Inability to handle multiple input views effectively due to redundancy and inconsistency.
  - EV olSplat: Requires explicit 3D point clouds and is not generalized for broader scenarios.
  - pixelSplat: Predicts Gaussian parameters on a per-pixel basis, which may lead to inconsistencies in 3D space.
  - MVSplat: Similar to pixelSplat, it struggles with geometric consistency when unprojecting to 3D.
  - Gaussian Graph Network (GGN): Refines pixel-aligned approaches but does not fully utilize voxel structures for better redundancy reduction.
  - DepthSplat: Limited by pixel distribution, resulting in poor handling of complex geometries.
  - MVSplat: High sensitivity to variations in data complexity and distribution.
  - pixelSplat: Inability to adapt Gaussian density to scene complexity.
  - Nerf: Representing scenes as neural radiance fields for view synthesis: Rigid coupling of Gaussian density to input image resolution.
  - 3D Gaussian Splatting for real-time radiance field rendering: High sensitivity to multi-view alignment errors.
  - N/A: N/A

### 3. Core Idea
- The V olSplat framework shifts the reconstruction process from 2D pixels to a 3D voxel-aligned space, allowing for adaptive control over Gaussian density and resolving alignment ambiguities.

### 4. Method
- **Pipeline**: Constructing 3D voxel features and predicting Gaussians directly from this unified representation.
- **Architecture / Loss / Training**: Incorporates a 3D U-Net architecture to refine and enhance raw feature volume.
- **Complexity / Resources**: Trained on 4×A100 GPUs with a total batch size of 4, using a learning rate of 2×10^-4 for most layers and 2×10^-6 for the Depth Anything V2 backbone.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on RealEstate10K and ScanNet datasets using PSNR, SSIM, and LPIPS metrics.
- **Baselines**: 3DGS, DepthSplat, EV olSplat, FreeSplat, FreeSplat++, GGN, Gaussian Graph Network (GGN), MVSplat, N/A, NeRF, No refinement module, Pixel-aligned feed-forward 3DGS methods, Standard 3D CNN, TranSplat, pixelSplat
- **Main Results**: Removing the refinement stage leads to a significant drop in performance, while substituting with a standard 3D CNN yields better results than no refinement but still falls short of the full model’s performance.
- **Ablations**: Conducted an ablation study with two variants: removing the refinement module and replacing the 3D U-Net with a standard 3D CNN.
- **Limitations / Stress Tests**: The model's performance is less susceptible to domain shifts compared to pixel-aligned models, but it still requires careful tuning of voxel size.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art performance on public benchmarks., Reduces alignment errors and improves multi-view consistency., Enables adaptive control over Gaussian density based on scene complexity.
- **Cons**: Still relies on the quality of input images., May require significant computational resources for high-resolution outputs., Complexity in implementation compared to simpler pixel-aligned methods.
- **Future Work**: Further research into voxel alignment techniques., Exploration of integration with other 3D signals like depth maps., Potential applications in real-time systems and large-scale datasets.

</details>

### [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](http://arxiv.org/pdf/2509.19296v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- 3D Generation from Video

### 2. Motivation & Gaps
- The paper aims to develop a diverse 3D generator capable of synthesizing simulation environments for autonomous agents.

- **Related work challenges:**
  - Recent advances in neural 3D reconstruction: Reliance on accurate camera poses and high-quality images significantly limits scalability.
  - Video diffusion models: Generate only 2D frames, lacking explicit 3D representations.
  - Feed-forward reconstruction models: Scarcity of diverse, large-scale 3D training data leads to poor out-of-domain generalization.
  - Watson et al., 2023: Focus on object-centric scenes without background.
  - Sargent et al., 2024: Need for expensive optimization stages for explicit 3D representation.
  - Bai et al., 2025a: Output not grounded in 3D.
  - GS-LRM (Zhang et al., 2024e): Operates on a limited number of images (2-4) at lower resolutions (512x512).
  - AnySplat (Jiang et al., 2025a): Trained with only 24 images at 448x448 resolution, limiting its scalability.
  - ZeroNVS: Limited performance on dynamic scenes.
  - ViewCrafter: Inability to handle temporal changes effectively.
  - Bolt3D: Lack of robustness in diverse scenarios.
  - RealEstate10K (Zhou et al., 2018): Limited generalizability to out-of-distribution scenes.
  - DL3DV (Ling et al., 2024b): Commonly used but leads to limited generalizability.
  - N/A: N/A
  - Camera pose estimation emerging in video diffusion transformer: Accurate camera pose estimation is crucial for generating realistic video sequences.
  - Vipe: Video pose engine for 3D geometric perception: Existing methods struggle with 3D geometric perception in dynamic environments.
  - Dreamphysics: Learning physical properties of dynamic 3D Gaussians with video diffusion priors: Incorporating physical properties into video generation remains a significant challenge.
  - N/A: N/A
  - Generative gaussian splatting: Generating 3d scenes with video diffusion priors: Limited efficiency in generating complex 3D scenes.
  - Vid-camedit: Video camera trajectory editing with generative rendering from estimated geometry: Challenges in real-time editing of video camera trajectories.
  - MVDream: Multi-view diffusion for 3D generation: Scalability issues in multi-view generation.
  - Act-r: Adaptive camera trajectories for single view 3d reconstruction: Limited adaptability of camera trajectories in existing methods.
  - Cpa: Camera-pose-awareness diffusion transformer for video generation: Inadequate integration of camera pose awareness in video generation.
  - Prolific-Dreamer: High-fidelity and diverse text-to-3D generation with variational score distillation: Challenges in achieving high fidelity and diversity in generated 3D content.
  - GEN3C (Ren et al., 2025): Background leakage and incomplete foreground completion due to occluded object parts.
  - BTimer (Liang et al., 2025b): BTimer is purely regression-based and runs out of memory when using high-resolution frames.
  - CAT3D (Gao et al., 2024b): Previous works primarily focused on single object categories and relied on optimization for 3D reconstruction.
  - Bolt3D (Szymanowicz et al., 2025b): Bolt3D fine-tunes CAT3D but does not leverage the advantages of video diffusion models.
  - Bolt3D (Szymanowicz et al., 2025b): Fine-tunes CAT3D to output pointmaps using a geometry autoencoder.
  - Feed-forward 3D models (Hong et al., 2024): Limited to producing static 3D scenes.
  - 4D generation (Singer et al., 2023): Most existing approaches remain object-centric, neglecting background elements.

### 3. Core Idea
- Our work builds upon a pre-trained camera-controlled video diffusion model and directly decodes the multi-view video latents into 3D Gaussians.

### 4. Method
- **Pipeline**: Generate 3D scenes from text, export as .ply files, convert to .usdz format for use in simulation environments.
- **Architecture / Loss / Training**: Utilizes Plücker embeddings for camera encoding and depth supervision to prevent flat geometries.
- **Complexity / Resources**: Requires significant computational resources for training and inference, particularly with high-resolution video data.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on static and dynamic Lyradatasets using PSNR, SSIM, and LPIPS metrics.
- **Baselines**: 3DGS, AnySplat, BTimer (GEN3C), Bolt3D, Existing video generation models, Feed-forward reconstruction models, GEN3C, GS-LRM, Generative gaussian splatting, MVDream, MotionCtrl (Wang et al., 2024e), N/A, NeRF, Neural 3D reconstruction methods, ReCamMaster (Bai et al., 2025a), State-of-the-art motion transfer techniques, SynCamMaster (Bai et al., 2025b), Traditional 3D reconstruction methods, Vid-camedit, Video diffusion models, ViewCrafter, Wonderland, ZeroNVS, real data only, self-distillation + real data
- **Main Results**: Our method outperforms BTimer (GEN3C) in both static and dynamic evaluations, showing higher quality and fewer artifacts.
- **Ablations**: Depth loss ablation shows that using depth supervision improves visual quality without flat geometries.
- **Limitations / Stress Tests**: BTimer (GEN3C) runs out of memory with high-resolution frames, necessitating subsampling for evaluation.

### 6. Takeaways
- **Pros**: Enables generation of large-scale synthetic environments from video diffusion models., Allows efficient processing of multiple views in latent space., Guarantees geometric consistency for downstream tasks.
- **Cons**: Dependence on the quality of the video diffusion model., Limited by the diversity of the training data used for the video model.
- **Future Work**: Explore further improvements in dynamic scene generation., Investigate the application of the framework in other domains., Enhance the model's ability to handle more complex environments.

</details>

### [STFT-AECNN: An Attention-Enhanced CNN for Efficient Φ-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing](http://arxiv.org/pdf/2509.19281v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Event classification in resource-constrained IoT environments

### 2. Motivation & Gaps
- The paper addresses the challenge of achieving both state-of-the-art accuracy and high computational efficiency for Φ-OTDR event classification in IoT environments.

- **Related work challenges:**
  - Conventional machine learning approaches: Dependence on handcrafted feature extraction limits robustness and scalability.
  - Deep learning methods: High computational complexity and large parameter counts limit real-time deployment in IoT edge environments.
  - Sequential Models for Φ-OTDR Time-Series: High model complexity and large parameter counts require vast labeled datasets, leading to overfitting and poor generalization.
  - Image-based Models for Φ-OTDR Signals: Transforming raw signals into images can result in information loss and may not capture critical high-frequency details.
  - N/A: Existing methods may not effectively capture the spatial and frequency characteristics of Φ-OTDR signals.
  - Previous studies on event classification in Φ-OTDR systems: Limited accuracy and inability to distinguish subtle variations in events.
  - Traditional SVM and 2D-CNN models: These models rely on handcrafted features and may not effectively capture complex patterns in noisy environments.
  - SVM: Low accuracy for high-reliability requirements in IoT-enabled systems.
  - 2D-CNN: Impractical for deployment due to lower accuracy despite competitive efficiency metrics.
  - Transformer architectures (ST-T, ViT-VSEC): Significant computational overhead and model complexity.
  - Transformer-based architectures: High-latency bottleneck that renders them impractical at the edge.
  - 2D-CNN: Lags substantially in convergence speed and final performance.
  - Vibration events recognition of optical fiber based on multi-scale 1-D CNN: N/A
  - Practical multi-class event classification approach for distributed vibration sensing using Deep Dual Path Network: N/A
  - MI-SI based distributed optical fiber sensor for no-blind zone location and pattern recognition: N/A
  - Distributed optical fiber sensing intrusion pattern recognition based on GAF and CNN: N/A
  - Optical fiber distributed vibration sensing using grayscale image and multi-class deep learning framework for multi-event recognition: N/A
  - An event recognition method for fiber distributed acoustic sensing systems based on the combination of MFCC and CNN: N/A
  - An event recognition method based on MFCC, superposition algorithm and deep learning for buried distributed optical fiber sensors: N/A
  - An event recognition method for Φ-OTDR sensing system based on deep learning: N/A
  - Adaptive shrinkage denoising and sequential state extraction model for vibration event recognition: N/A
  - A deep learning model enabled multi-event recognition for distributed optical fiber sensing: N/A
  - Inter-sequence-attention Transformer network for distributed fiber-optic sensing signal recognition: N/A
  - St-t: a spatio-temporal Transformer for Φ-OTDR multi-location time series classification: N/A
  - High-accuracy classification method of vibration sensing events in Φ-OTDR system based on Vision Transformer: N/A
  - A novel DAS signal recognition method based on spatiotemporal information extraction with 1DCNNs-BiLSTM network: N/A
  - Disturbance pattern recognition based on an ALSTM in a long-distance Φ-OTDR sensing system: N/A
  - An open dataset of Φ-OTDR events with two classification models as baselines: N/A
  - Pattern recognition of Φ-OTDR signals based on Markov transition field: N/A
  - Fused feature extract method for Φ-OTDR event recognition based on VGGish transfer learning: N/A

### 3. Core Idea
- Introduction of a novel STFT-based data representation that transforms raw multi-channel signals into stacked spectrograms, enabling efficient 2D-CNN processing.

### 4. Method
- **Pipeline**: Utilizes STFT-based data representation and incorporates a custom SEAM attention module.
- **Architecture / Loss / Training**: Joint loss function to enhance discriminative feature learning.
- **Complexity / Resources**: Requires less than 20% of the FLOPs of ST-T and 10% of ViT-VSEC, with a model size of only 0.43M parameters.

### 5. Experiments
- **Datasets & Metrics**: Extensive experiments demonstrating performance against foundational architectures and state-of-the-art methods.
- **Baselines**: 1D-CNNs, 2D-CNN, 2D-CNNs, Conventional machine learning classifiers, LSTMs, N/A, ST-T, SVM, State-of-the-art CNN models, Traditional machine learning classifiers, Transformers, ViT-VSEC
- **Main Results**: Surpasses foundational architectures and rivals more complex methods in accuracy while maintaining a minimal computational footprint.
- **Ablations**: Ablation studies demonstrate the importance of the SEAM attention module and Triplet Loss in enhancing model performance.
- **Limitations / Stress Tests**: The model's performance is consistent across all event types, avoiding class-specific trade-offs.

### 6. Takeaways
- **Pros**: High accuracy of 99.94% on the BJTUΦ-OTDR dataset., Efficient processing suitable for resource-constrained IoT environments., Robust event recognition capabilities.
- **Cons**: High false alarm rates in practical applications., Dependence on the quality of input data., Potential limitations in generalizing to other sensing modalities.
- **Future Work**: Exploration of further optimizations for real-time processing., Investigation of generalization to other types of event recognition., Integration with other IoT sensing technologies.

</details>

## Gaussian Splatting

### [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](http://arxiv.org/pdf/2509.19300v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Optimization in Latent Diffusion Models

### 2. Motivation & Gaps
- The paper addresses the optimization dilemma in latent diffusion models, focusing on the challenges of training diffusion transformers.

- **Related work challenges:**
  - Diffusion and flow-based methods: Require the model to learn both mass transport and conditional injection simultaneously.
  - Flow matching: Forces the network to handle long-range transport and semantic injection at once, slowing convergence and impairing sample quality.
  - Classic VAE-based latent diffusion model: Does not incorporate explicit semantic awareness.
  - Recent works by Yu et al. and Leng et al.: Augment VAE training with semantic alignment but may lead to trivial solutions.
  - REPA-E [Leng et al., 2025]: End-to-end VAE-diffusion tuning favored a simpler latent space with reduced variance.
  - Generative modeling advancements: N/A
  - Flow matching techniques: N/A
  - Rectified flow matching: N/A
  - REPA-E: End-to-end training of diffusion models and VAE encoders/decoders leading to trivial solutions.
  - Yu et al. (2025): Aligning representations within deep nets without addressing the transformation of source and target distributions.
  - Yao et al. (2025): Aligning latent spaces without considering the implications for flow matching.
  - Representation alignment for generation: Training diffusion transformers is easier than you think.: Difficulty in aligning representations during the training of diffusion models.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- CAR-Flow's design choice supports per-condition alignment to reduce transport and distinguishes it from unconditional learnable sources.

### 4. Method
- **Pipeline**: The method involves training lightweight condition networks that learn additive shifts for class embeddings, utilizing a specific architecture and training regimen.
- **Architecture / Loss / Training**: The architecture includes linear layers for mapping class embeddings to shifts, with training conducted using AdamW optimizer and specific learning rates for different networks.
- **Complexity / Resources**: The experiments were conducted on an Apple M1 Pro laptop and a TPU slice, with a total of 1,993 trainable parameters in the model.

### 5. Experiments
- **Datasets & Metrics**: CIFAR-10, FID
- **Baselines**: Baseline rectified flow model, Classic VAE-based models, Joint CAR-Flow, Previous state-of-the-art methods, REPA-E, Recent semantic-aware models, SiT-XL/2, SiT/XL-2 baseline, Source-only CAR-Flow, Standard diffusion models, Target-only CAR-Flow, Traditional flow matching models
- **Main Results**: All CAR-Flow variants outperformed the baseline on CIFAR-10.
- **Ablations**: Qualitative ablation of CAR-Flow variants shows joint model produces the most realistic images.
- **Limitations / Stress Tests**: The model's performance may degrade under certain conditions, such as extreme shifts in the source or target distributions.

### 6. Takeaways
- **Pros**: Relieves the velocity network of unnecessary transport., Improves performance on both synthetic and natural image data., Adds negligible computational overhead.
- **Cons**: Potential for trivial solutions leading to mode collapse., Increased complexity in model training and implementation., Dependence on the quality of the condition-dependent mappings.
- **Future Work**: Explore further applications of CAR-Flow in other generative models., Investigate the impact of different conditioning strategies., Develop more robust theoretical frameworks for understanding flow matching.

</details>

## avatar

### [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](http://arxiv.org/pdf/2509.19259v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Scene navigation and goal discovery through vision

### 2. Motivation & Gaps
- Current avatar motion generation methods lack human-like sensors, which are crucial for realistic motion.

- **Related work challenges:**
  - Existing human motion generation systems: They typically use abstract representations for perception, lacking human-like vision.
  - Datasets with isolated human motion: They do not provide context of a scene or lack scale.
  - Reinforcement Learning methods: They face challenges in mapping visual inputs to actions while generating natural human motion.
  - Existing methods using precomputed waypoints for obstacle avoidance: These methods lack a realistic connection to the avatar's body and often result in lifeless motion.
  - Reinforcement learning approaches for motion generation: High-dimensional action spaces complicate reward function construction, leading to unnatural poses.
  - Text-to-motion generation methods: These methods provide little autonomy to the agent, limiting the realism of generated motions.
  - Text-to-motion approaches: Lack of semantic control and user input requirement
  - EgoGen: Generates avatar motion without providing a path, relying on a lidar-like sensor and exact goal location.
  - EgoGen: Limited to known goals and does not utilize egocentric vision effectively.
  - 3D-MEM: Lacks integration of visual input for memory-based navigation.
  - Vision-language models: Not fully explored for enhancing avatar navigation capabilities.
  - Resolving 3D human pose ambiguities with 3D scene constraints: N/A
  - Stochastic scene-aware motion prediction: N/A
  - Autonomous Character-Scene Interaction Synthesis from Text Instruction: N/A
  - Scaling Up Dynamic Human-Scene Interaction Modeling: N/A
  - EgoGen: An Egocentric Synthetic Data Generator: N/A
  - AMASS: Archive of motion capture as surface shapes: N/A
  - Expressive body capture: 3D hands, face, and body from a single image: N/A
  - Adversarial motion priors for stylized physics-based character control: N/A
  - Generating diverse human motions from textual descriptions: N/A
  - BABEL: Bodies, action and behavior with english labels: N/A
  - Neural state machine for character-scene interactions: N/A
  - The replica dataset: A digital replica of indoor spaces: N/A
  - GRAB: A dataset of whole-body human grasping of objects: N/A
  - Unified physics-based character control through masked motion inpainting: N/A
  - Human motion diffusion model: N/A
  - Closing the loop between simulation and diffusion for multi-task character control: N/A
  - Putting human motion generation in context: N/A
  - Adversarial learning for modeling human motion: N/A
  - Language-conditioned human motion generation in 3d scenes: N/A
  - 3d scene memory for embodied exploration and reasoning: N/A
  - Unified physics-based motion control via scalable discrete representations: N/A
  - Human-aware 3D scene generation: N/A
  - Scenic: Scene-aware semantic navigation with instruction-guided control: N/A
  - The wanderings of odysseus in 3D scenes: N/A
  - A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control: N/A
  - Synthesizing diverse human motions in 3d indoor scenes: N/A

### 3. Core Idea
- CLOPS integrates egocentric vision into avatar motion generation to improve navigation and realism.

### 4. Method
- **Pipeline**: Decouples motion skill learning from visual sensing, using reinforcement learning for visual input mapping.
- **Architecture / Loss / Training**: Utilizes a Q-learning policy for motion control based on visual inputs.
- **Complexity / Resources**: Requires significant computational resources for training and data processing.

### 5. Experiments
- **Datasets & Metrics**: Trained on multiple scenes (S1 to S5) with success and collision rates measured.
- **Baselines**: CLOPS (only Vision), CLOPS+ (known Goal), Data-driven methods, EgoGen, End-to-end RL methods, Existing human motion generation systems, Existing motion generation methods using waypoints, Existing text-to-motion approaches, N/A, Other autonomous navigation methods, Reinforcement learning methods with continuous action spaces, Text-to-motion generation approaches
- **Main Results**: CLOPS outperforms EgoGen in success rate and collision avoidance while relying solely on vision.
- **Ablations**: Experimented with sensor placement and its impact on avatar motion.
- **Limitations / Stress Tests**: CLOPS struggles with navigation in cluttered scenes due to lack of control over the avatar's body.

### 6. Takeaways
- **Pros**: CLOPS generates natural human motion using egocentric vision., The method is data-efficient and generalizes to new scenes., It allows avatars to navigate and avoid obstacles effectively.
- **Cons**: The approach may struggle with scenes where the domain gap is significant., Training complexity increases due to the need to decouple motion generation and high-level control., Existing datasets may not fully capture the necessary context for training.
- **Future Work**: Explore additional sensory inputs beyond vision for avatar navigation., Investigate the application of CLOPS in more complex environments., Develop methods to automatically generate scenes for training.

</details>

### [Audio-Driven Universal Gaussian Head Avatars](http://arxiv.org/pdf/2509.18924v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- 3D avatar generation from audio and image inputs

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic 3D avatars that can express emotions based on audio input.

- **Related work challenges:**
  - VASA-1: Primarily operates in 2D, lacking the underlying 3D structure necessary for free-viewpoint rendering.
  - 3D Morphable Models (3DMMs): Do not model dynamic textures and view-dependent appearance directly from audio signals.
  - Neural Radiance Fields (NeRFs): Require costly per-subject optimization or training, hindering the creation of universal models.
  - 3D Morphable Models (3DMMs): Limited expressive capacity due to low-dimensional PCA parameters.
  - Diffusion models: Focus on predicting parameters for established representations rather than directly synthesizing nuanced appearance changes.
  - Neural Radiance Fields (NeRF): Often person-specific and require per-subject optimization.
  - GaussianAvatars: Requires extensive per-subject data and training.
  - URAvatar: Involves extensive fine-tuning or acquisition of dynamically tracked non-rigid facial geometry.
  - Authentic Volumetric Avatars: Adapting to new, unseen identities can present challenges.
  - Kerbl et al. [2023b]: Rendering differentiable images from Gaussian primitives.
  - Stan et al. [2023b]: Learning expression-specific changes in a variational autoencoder.
  - Martinez et al. [2024]: Utilizing multi-view images for dynamic geometry tracking.
  - FaceTalk: Primarily predicts latent codes for geometry-only parametric models.
  - DDPM: Requires effective conditioning on multiple inputs for accurate expression generation.
  - Ng et al. 2024: Originally applied to predict person-specific codes, not person-agnostic expression features.
  - Faceformer [Fan et al. 2022a]: Focuses on generating 3D mesh deformations without generalizability from speech input.
  - CodeTalker [Xing et al. 2023]: Requires personalization and does not achieve photoreal renderings.
  - FaceDiffuser [Stan et al. 2023a]: Similar limitations in generating realistic animations from audio.
  - GaussianSpeech: Audio-Driven Gaussian Avatars: Limited ability to synthesize nuanced expressions and maintain high fidelity in diverse conditions.
  - FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models: Challenges in achieving accurate lip synchronization and dynamic facial expressions.
  - Voice2face: Audio-driven facial and tongue rig animations with cvaes: Struggles with rendering fine details and complex independent motions.
  - Out of time: automated lip sync in the wild: Limited accuracy in real-world scenarios.
  - EMOCA: Emotion Driven Monocular Face Capture and Animation: Difficulty in capturing emotional nuances in lip movements.
  - AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis: High computational resources required for real-time applications.
  - NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis: Limited generalization of avatars across different environments.
  - Audio- and Gaze-Driven Facial Animation of Codec Avatars: Challenges in achieving realistic facial animations driven by audio and gaze.
  - GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians: Difficulty in creating photorealistic avatars that maintain performance across various applications.
  - LivePortrait: Generalizing expression representation across diverse identities.
  - CodeTalker: Achieving accurate geometric motion in facial dynamics.
  - FaceFormer: Maintaining identity preservation with fewer input views.
  - Live3DPortrait: Directly regressing the latent expression code compatible with the model.
  - N/A: N/A

### 3. Core Idea
- The proposed method utilizes a combination of audio and image inputs to generate 3D Gaussian parameters for avatars, enabling realistic expressions and appearances.

### 4. Method
- **Pipeline**: The method involves a Monocular Expression Encoder, an Audio-to-Expression Diffusion Model, and multiple decoders for generating Gaussian parameters.
- **Architecture / Loss / Training**: The training of our UHAP model and the personalization fine-tuning stage involve several loss terms weighted by hyperparameters.
- **Complexity / Resources**: The Universal Head Avatar Prior (UHAP) is trained for a total of 300k iterations on 4 NVIDIA A40 GPUs (with a batch size of 1 per GPU). The Monocular Expression Encoder (𝐸𝑖𝑚𝑎𝑔𝑒) is subsequently trained for 100k iterations. The audio-to-expression diffusion model (G𝜃) is trained for 200k iterations on a single NVIDIA A40 GPU.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize a dataset of images and audio clips to evaluate the performance of the avatar generation.
- **Baselines**: 3D Morphable Models, 3D Morphable Models (3DMMs), Audio-Driven Facial Animation, Authentic Volumetric Avatars, CodeTalker, CodeTalker [Xing et al. 2023], CodeTalker+GA, FaceDiffuser, FaceDiffuser [Stan et al. 2023a], FaceDiffuser+GA, FaceFormer, Faceformer [Fan et al. 2022a], Faceformer+GA, GaussianAvatars, GaussianSpeech, Ground Truth, Live3DPortrait, Monocular Encoder, N/A, NeRF, Neural Radiance Fields, Neural Radiance Fields (NeRF), Other existing avatar generation models, Pretrained Encoder, Previous audio-driven avatar models, Recent neural network approaches, Standard 3D avatar synthesis methods, Traditional lip sync methods, URAvatar, VASA-1
- **Main Results**: The proposed model outperforms existing methods in generating realistic avatars with accurate expressions.
- **Ablations**: Ablation studies demonstrate the importance of each component in the architecture for achieving high-quality results.
- **Limitations / Stress Tests**: The model may struggle with extreme expressions or low-quality input data.

### 6. Takeaways
- **Pros**: Generates highly realistic avatars with precise lip synchronization., Captures nuanced expressive details such as eyebrow movement and gaze shifts., First generalizable audio-driven avatar model accounting for detailed appearance modeling.
- **Cons**: Requires significant computation time for training., Still faces challenges in rendering dynamic textures.
- **Future Work**: Explore further optimizations for real-time applications., Investigate broader identity generalization techniques., Enhance the model's ability to handle diverse audio inputs.

</details>

### ["I don't like my avatar": Investigating Human Digital Doubles](http://arxiv.org/pdf/2509.17748v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Understanding the impact of realism in virtual characters on user perception

### 2. Motivation & Gaps
- The study aims to explore how different levels of realism in speech and animation affect the perceived personality traits of virtual characters.

- **Related work challenges:**
  - Previous avatar perception studies: Often used generic digital avatars that do not resemble existing humans.
  - Studies on personalized digital avatars: Usually fall short in high-fidelity due to access to high-end systems.
  - Familiarity aspect in avatar perception: Remains underexplored despite its potential impact on user perception.
  - Kang et al. [22]: Limited exploration of how viewers perceive the identity of communication partners’ avatars during interactions.
  - Gonzalez-Franco et al. [15]: Did not investigate the fidelity of avatar representations in the context of identity perception.
  - Matthew et al. [13]: Focused on matching avatars with real-life photos but did not explore the impact of familiarity on identification.
  - Garau [14]: Mismatches between appearance and behavior reduce presence.
  - Pakanen et al. [45]: Realistic avatars can enhance both self and social presence, but fidelity was limited.
  - Fraser et al. [12]: Did not study the familiarity dimension in link with cartoonish and realistic styles.
  - Previous studies on avatar perception: Limited understanding of how familiarity affects social presence.
  - Previous studies on avatar representation: Limited understanding of how familiarity and style impact user interaction.
  - N/A: N/A
  - Amadou et al. [1]: Previous findings on social presence and avatar realism.
  - Higgins et al. [16]: Inconsistencies in the relationship between avatar realism and user affinity.
  - Previous studies on avatar affinity: Contradictory results regarding the attractiveness of realistic avatars.
  - Pakanen et al. [45]: People preferred to see the other user as photorealistic avatars.
  - [4]: Reported strongest neural responses for one’s own face, suggesting personal familiarity amplifies identity-specific processing.
  - Previous studies on avatar realism: Lack of comprehensive understanding of how avatar realism affects user experience.
  - Angela Tinwell et al. (2013): Perception of psychopathy and the Uncanny Valley in virtual characters.
  - Stephen Wonchul Song and Mincheol Shin (2024): Uncanny Valley Effects on Chatbot Trust and Adoption Intention.
  - Rachel McDonnell et al. (2012): Investigating the Effect of Render Style on the Perception of Animated Virtual Humans.
  - N/A: N/A

### 3. Core Idea
- The research investigates how variations in speech and animation realism can alter the perceived personality of virtual characters, potentially affecting user interactions.

### 4. Method
- **Pipeline**: The study employs a series of experiments where participants interact with virtual characters exhibiting different levels of speech and animation realism.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Consumer grade hardware and software were used, but high-end devices are needed for truly realistic avatars.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize a custom dataset of virtual characters with varying realism levels, measuring user perception through surveys.
- **Baselines**: Generic digital avatars, MetaHuman avatars, N/A, Personalized digital avatars, Photorealistic avatars, Previous avatar realism studies, Previous studies on avatar perception, Previous studies on realistic virtual humans [1, 12, 16, 72], Previous studies on self-identification and avatar perception, ReadyPlayerMe avatars, Realistic MetaHuman avatar, Realistic avatars, Stylized RPM cartoon avatar, Stylized avatars, Traditional animated characters
- **Main Results**: Results indicate that higher realism in speech and animation significantly enhances the perceived personality traits of virtual characters.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The study acknowledges limitations in the diversity of character designs and the potential bias in participant responses.

### 6. Takeaways
- **Pros**: Higher realism enhances self/other identification., Increased perceived realism and social presence with realistic avatars., Greater tolerance towards avatars of acquaintances or unknown individuals.
- **Cons**: Lower identification and affinity with familiar avatars., Participants dislike their own realistic avatars., Potential negative impact on self-perception.
- **Future Work**: Further exploration of familiarity in avatar perception., Investigate psychological implications of avatars in various contexts., Optimize use of human avatars in diverse applications.

</details>

## video understanding

### [Audio-Based Pedestrian Detection in the Presence of Vehicular Noise](http://arxiv.org/pdf/2509.19295v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Pedestrian detection in varying acoustic environments

### 2. Motivation & Gaps
- This research investigates the impact of the acoustic environment on pedestrian detection using a novel dataset with vehicular noise.

- **Related work challenges:**
  - Rasouli et al.: Model performance deteriorates in varied real-world conditions.
  - Hasan et al.: Traditional models generalize poorly due to narrow training datasets.
  - Seshadri et al.: Challenges in signal separation, data imbalance, and generalizability across urban soundscapes.
  - SONYC (Sounds of New York City): Limited scope in classifying general urban sounds.
  - Han et al. and Seshadri et al.: Generalizability of models across diverse urban environments remains underexplored.
  - ASPED dataset analysis: Limited generalization across different recording setups.
  - FSD50K sound classification: Misclassification of non-human sounds as pedestrian-related.
  - Cross-dataset evaluation: Performance drop when models are tested on datasets different from their training set.
  - Pedestrian detection based on deep learning model: Limited domain generalization capability when trained on different environments.
  - ASPED: An audio dataset for detecting pedestrians: Models show varying sensitivities based on training data’s acoustic characteristics.
  - State-of-the-art approaches to bicycle and pedestrian counters: False positives across various non-human sound categories.

### 3. Core Idea
- The study highlights the critical role of the acoustic environment in training robust pedestrian detection systems and suggests future work on domain adaptation techniques.

### 4. Method
- **Pipeline**: Evaluation of models trained on different acoustic environments and their performance on pedestrian detection.
- **Architecture / Loss / Training**: Utilizes VGGish backbone for feature extraction and employs weighted batch sampling and variable weighted loss to address class imbalance.
- **Complexity / Resources**: The model is trained on a dataset with 1,321 hours of audio and requires significant computational resources for processing.

### 5. Experiments
- **Datasets & Metrics**: Evaluation on ASPED and FSD50K datasets, focusing on pedestrian and non-human sound categories.
- **Baselines**: ASPED v.a, ASPED v.b, Infrared sensors, Seshadri et al. model, Traditional video-based pedestrian detection systems, v.a-trained model, v.b-trained model
- **Main Results**: Models trained on v.a data misclassify certain musical instruments as 'pedestrian' more than v.b-trained models.
- **Ablations**: Investigated the impact of vehicle presence in training data on model performance.
- **Limitations / Stress Tests**: Higher standard deviation in predictions for non-human sounds in v.a-trained models indicates less certainty.

### 6. Takeaways
- **Pros**: Audio-based systems are cost-effective and resilient to visual obstructions., Microphones can be deployed in locations where cameras are impractical., The study provides a publicly available dataset for future research.
- **Cons**: Existing models lack interpretability regarding sound characteristics used for detection., Challenges remain in signal separation and data imbalance., Generalizability of models across different urban soundscapes is still unclear.
- **Future Work**: Further research on improving model generalizability across diverse urban environments., Exploration of additional acoustic features for better interpretability., Development of more robust audio-based detection systems.

</details>
