# Daily Paper Digest Â· 2025-09-29
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [Toward a Physics of Deep Learning and Brains](http://arxiv.org/pdf/2509.22649v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- MNIST digit classification

### 2. Motivation & Gaps
- This work explores the relationship between criticality and learning performance in deep neural networks, particularly focusing on Gaussian networks and ResNets.

- **Related work challenges:**
  - Previous studies on biological neuronal systems: Limited understanding of how living neural networks operate near criticality.
  - Crackling noise theory: Need for a unified framework with concrete predictions for observables near critical points.
  - Deep learning architectures: Reliance on architecture-level proxies rather than event statistics to demonstrate criticality.
  - Mean-field theory in neural networks: Characterizing the sensitivity of signal strength to fluctuations in connectivity.
  - Edge of chaos framework: Explaining performance differences across parameter choices in deep networks.
  - Widom-like line analysis: Understanding the relationship between criticality and learning performance.
  - Crackling noise theory: Understanding how power law distributions relate to learning performance in neural networks.
  - Mean-field theory: Mapping the relationship between signal strength and susceptibility in the context of learning.
  - Avalanche statistics: Characterizing avalanches in deep neural networks, which has not been previously done.
  - Previous studies on Gaussian networks: Understanding the impact of initialization on trainability and performance.
  - Research on ResNets: Determining how engineered architectures exhibit criticality compared to Gaussian networks.
  - Crackling noise theory: Applying theoretical predictions to empirical observations in deep learning.
  - N/A: N/A
  - N/A: Limited applicability of constant threshold for Gaussian initialized networks of uniform widths.
  - N/A: Challenges in applying the method to convolutional networks with variable widths and operations.
  - N/A: N/A
  - The perceptron: A probabilistic model for information storage and organization in the brain: N/A
  - A learning algorithm for boltzmann machines: N/A
  - Deep information propagation: N/A
  - Mean field residual networks: On the edge of chaos: N/A
  - Edge of chaos as a guiding principle for modern neural network training: N/A
  - Universal scaling laws of absorbing phase transitions in artificial deep neural networks: N/A
  - Optimal machine intelligence at the edge of chaos: N/A
  - Feature learning and generalization in deep networks with orthogonal weights: N/A
  - Critical dynamics governs deep learning: N/A
  - Deep residual learning for image recognition: N/A
  - Scaling behavior of the directed percolation universality class: N/A
  - Barkhausen noise from zigzag domain walls: N/A
  - Playing with universality classes of barkhausen avalanches: N/A
  - The mnist database of handwritten digits: N/A
  - Quasicriticality explains variability of human neural dynamics across life span: N/A
  - powerlaw: a python package for analysis of heavy-tailed distributions: N/A

### 3. Core Idea
- The study provides a novel framework based on crackling noise theory for artificial intelligence, demonstrating that deep networks can operate near criticality, which can predict performance through quasi-critical plateaus.

### 4. Method
- **Pipeline**: Experiments conducted on fully connected deep neural networks and ResNets with varying initializations and configurations.
- **Architecture / Loss / Training**: Utilized deep neural networks with specific layer configurations and training protocols to assess performance metrics.
- **Complexity / Resources**: Simulations were performed on networks with defined widths and depths, analyzing the effects of initialization on learning outcomes.

### 5. Experiments
- **Datasets & Metrics**: MNIST dataset with 60000 training images and 10000 test images.
- **Baselines**: Fully connected deep neural networks, Gaussian-initialized networks, Mean-field directed percolation models, Mean-field theory predictions, N/A, Previous models of avalanche behavior, ResNet variants, Residual Networks (ResNets), Traditional neural network training metrics
- **Main Results**: Learning performance peaks in regions of heightened susceptibility, with trainability linked to initialization near critical values.
- **Ablations**: Investigated the effects of varying network width and bias variance on training outcomes.
- **Limitations / Stress Tests**: The study acknowledges limitations such as finite system sizes and subsampling that complicate exponent estimation.

### 6. Takeaways
- **Pros**: Provides a theoretical framework that unifies deep learning and neuroscience., Demonstrates that maximal susceptibility is a reliable predictor of learning., Identifies distinct universality classes shared by biological and artificial neural networks.
- **Cons**: Current understanding of criticality in deep networks relies on architecture-level proxies., Not all criticality is alike, leading to potential indistinguishability of different regimes., Deep networks are strongly driven, complicating the analysis of their criticality.
- **Future Work**: Further exploration of the implications of quasi-criticality in deep learning., Development of more refined metrics for assessing criticality in neural networks., Investigation of distinct universality classes and their functional consequences.

</details>

### [StateX: Enhancing RNN Recall via Post-training State Expansion](http://arxiv.org/pdf/2509.22630v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluation of in-context demonstrations and passkey retrieval tasks

### 2. Motivation & Gaps
- The evaluation aims to assess the performance of models on various tasks using different numbers of in-context demonstrations.

- **Related work challenges:**
  - Mamba2: RNNs are trained with relatively small state sizes compared to model sizes, limiting their recall capabilities.
  - GLA: Increasing state size incurs high training costs without significant benefits in short-context scenarios.
  - LSTM (Hochreiter & Schmidhuber, 1997): Limited state size compared to modern architectures.
  - GRU (Cho et al., 2014): Similar limitations in state size affecting performance.
  - MoM (Du et al., 2025): Maintains large state size but with computational overhead.
  - GLA (Yang et al., 2024): Limited state size due to multi-head mechanism.
  - SSM (Dao & Gu, 2024): Inability to apply head merging due to single key vector.
  - Min et al. (2022): Systematic evaluation of in-context learning capabilities.
  - Hsieh et al. (2024): Evaluating recall abilities in long-context tasks.
  - Zhang et al. (2024): Avoiding score saturation in recall tasks.
  - Simple linear attention language models balance the recall-throughput tradeoff: Existing models struggle with recall in long-context scenarios.
  - Just read twice: closing the recall gap for recurrent language models: RNNs have limitations in recall compared to attention-based models.
  - Understanding the skill gap in recurrent language models: The gather-and-aggregate mechanism in RNNs limits their performance.
  - Attention is all you need: Traditional attention mechanisms can be computationally expensive.
  - Linear transformers are secretly fast weight programmers: Existing models may not fully leverage the potential of fast weight programming.
  - An empirical study of mamba-based language models: Mamba-based models require optimization for better performance.
  - N/A: N/A

### 3. Core Idea
- To evaluate models on tasks with varying levels of difficulty and different configurations.

### 4. Method
- **Pipeline**: The training procedure follows common language model pre-training practices, utilizing a cosine learning rate scheduler.
- **Architecture / Loss / Training**: Models consist of 340 million parameters and 24 layers, with variations in the number of attention heads.
- **Complexity / Resources**: Trained on 20B tokens from SlimPajama with a batch size of 0.5M tokens and a sequence length of 4k.

### 5. Experiments
- **Datasets & Metrics**: Evaluation includes commonsense QA, AI2 ARC, SuperGLUE tasks, and custom passkey retrieval tasks.
- **Baselines**: GLA, GRU, LPT GLA, LPT Mamba2, LSTM, Mamba2, N/A, Ordinary LPT versions, Original GLA, Original Mamba2, RNNs with larger state sizes trained from scratch, Traditional RNNs, Traditional two-stage method, Vanilla RNNs
- **Main Results**: The proposed model outperforms existing baselines in common-sense reasoning and information recall tasks.
- **Ablations**: An ablation study on the number of attention heads was conducted.
- **Limitations / Stress Tests**: The model's performance is limited in highly complex reasoning tasks, indicating areas for future improvement.

### 6. Takeaways
- **Pros**: Efficiently enhances recall and in-context learning abilities of RNNs., No significant increase in training costs., Can be seamlessly integrated into existing RNN training pipelines.
- **Cons**: Limited benefits in short-context scenarios., Still underperforms Transformers in certain aspects., Dependence on architectural modifications may limit applicability.
- **Future Work**: Explore further architectural modifications for other RNN variants., Investigate the impact of state expansion on different tasks., Develop more efficient training methods for larger state sizes.

</details>

### [CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach](http://arxiv.org/pdf/2509.22627v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Unsupervised Binocular Depth Estimation

### 2. Motivation & Gaps
- The paper addresses the challenge of estimating depth from binocular images without the need for labeled data.

- **Related work challenges:**
  - ChiTransformer (Su and Ji, 2022): Achieves state-of-the-art metrics but at the expense of runtime and memory, restricting their use in real-time or embedded applications.
  - Monodepth2 (Godard et al., 2019) and ES3Net (Fang et al., 2023): Offer good latency but tend to underperform in accuracy, especially in low-texture or occluded regions.
  - Fu et al. (2018): Proposed a method using ordinal regression for continuous depth estimation, which lacks similarities with self-supervised techniques.
  - Godard et al. (2017): Introduced left-right consistency but faced limitations in self-supervised settings.
  - ChiTransformer (Su and Ji, 2022): Achieves strong performance but compromises runtime due to multiple attention modules.
  - Federated contrastive learning with feature distillation for human activity recognition: Limited supervision and geometric grounding.
  - Contrastive token and label activation for remote-sensing semantic segmentation: Instance or feature-level contrastive objectives without geometric grounding.
  - Monodepth: High-resolution features influence multiple output scales, disturbing fine-grained detail learning.
  - ICEP: Increased computational cost in higher resolution layers.
  - KITTI dataset: Limited evaluation of different weather and illumination conditions.
  - Monodepth2 (Godard et al., 2019): Limited performance in complex scenes.
  - ChiTransformer (Su and Ji, 2022): High computational cost due to multiple self-attention mechanisms.
  - H-Net (Huang et al., 2022): Inability to handle diverse weather conditions effectively.
  - ChiTransformer: Inability to replicate exact training procedures and runtime estimation.
  - ES3Net: Training on the entire KITTI Raw dataset potentially biases results.
  - H-Net: Limited performance in certain metrics compared to CCNeXt.
  - MonoDepth2 (Godard et al., 2019): Performs worse on weather splits compared to CCNeXt.
  - ES3Net (Fang et al., 2023): Shows larger differences in performance across weather conditions.
  - EdgeStereo (Song et al., 2020): Is a supervised strategy that does not match the performance of CCNeXt.
  - Monodepth2: Phantom regions and lack of spatial consistency in depth maps.
  - Swin Transformer: Higher computational costs and less effective local window-based self-attention for dense spatial correlations.
  - Pyramid Vision Transformer v2: Minor degradation in precision-based metrics compared to ConvNeXt.
  - ZoeDepth: Zero-Shot Transfer by Combining Relative and Metric Depth: Limited transferability of depth estimation models across different datasets.
  - Attention-Aware Feature Aggregation for Real-Time Stereo Matching on Edge Devices: Real-time performance constraints on edge devices.
  - DeepPruner: Learning Efficient Stereo Matching via Differentiable Patchmatch: Efficiency in stereo matching without compromising accuracy.
  - Zhou et al. (2017): Unsupervised learning of depth and ego-motion from video.
  - Yang et al. (2019): Large-scale dataset for stereo matching in autonomous driving scenarios.
  - Xu et al. (2023b): Unifying flow, stereo, and depth estimation.

### 3. Core Idea
- The proposed method utilizes cycled networks to progressively fuse information from stereo images for improved depth estimation.

### 4. Method
- **Pipeline**: The method involves a cycled network architecture that iteratively refines depth estimates from stereo image pairs.
- **Architecture / Loss / Training**: The architecture employs a combination of reconstruction loss and depth consistency loss during training.
- **Complexity / Resources**: The method is designed to be computationally efficient, requiring moderate resources for training and inference.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on standard stereo datasets, evaluating performance using metrics such as depth accuracy and consistency.
- **Baselines**: ChiTransformer, DeepPruner, ES3Net, ES3Net (Fang et al., 2023), EdgeStereo, H-Net, H-Net (Huang et al., 2022), IGEV, MonoDepth2, Monodepth, Monodepth2, PFN, PMN, Pyramid Vision Transformer v2, Swin Transformer, Xu et al. (2023b), Yang et al. (2019), Zhou et al. (2017), ZoeDepth
- **Main Results**: The proposed method outperforms existing state-of-the-art approaches in depth estimation accuracy.
- **Ablations**: Ablation studies demonstrate the effectiveness of the progressive fusion strategy and the cycled network architecture.
- **Limitations / Stress Tests**: The method may struggle in highly dynamic scenes or with significant occlusions.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art accuracy while reducing execution time., Balances computational efficiency with competitive accuracy., Utilizes a modern ConvNeXt backbone and lightweight decoder.
- **Cons**: May not generalize well to all datasets., Complexity in architecture may hinder understanding., Dependence on specific training procedures.
- **Future Work**: Further exploration of lightweight architectures., Investigation into real-time applications., Enhancements in feature representation techniques.

</details>

## Gaussian Splatting

### [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](http://arxiv.org/pdf/2509.22615v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Vision-Language Alignment

### 2. Motivation & Gaps
- This work is positioned as a first step toward representation-first multimodal learning, focusing on compact, structured inputs such as 2DGS for alignment in vision-language systems.

- **Related work challenges:**
  - Contrastive Language-Image Pretraining (CLIP): High-density RGB inputs impose substantial data-transfer and energy costs in edgeâ€“cloud deployments.
  - Vision Transformers (ViT): Patch-based tokenization causes a severe token explosion, inflating memory and compute costs.
  - PruMerge: Requires LLM-level fine-tuning for optimal performance.
  - VisionZip: Requires projector-level fine-tuning for optimal performance.
  - ToMe: Integrates token merging but still relies on patch-based tokenization.
  - GaussianToken: Limited to small-scale settings and lacks systematic investigation for large-scale multimodal alignment.
  - CLIP: Requires hundreds of millions of training samples and significant compute to achieve high-quality alignment.
  - Traditional 2D Gaussian splatting approaches: Random initialization of Gaussian parameters is the standard practice, which is impractical without prior geometric knowledge.
  - RGB encoders: Accuracy on ImageNet-1K lags behind RGB encoders by roughly 3â€“5 points under the same training setup.
  - Training from scratch: Training 2DGS encoders from scratch performs significantly worse than the parameter-efficient adaptation approach.
  - Modeling innovations: The design space for architectures tailored to 2DGS representation remains largely unexplored.
  - N/A: N/A
  - Fuyu-8b: A multimodal architecture for ai agents: N/A
  - Gsvc: Efficient video representation and compression through 2d gaussian splatting: N/A
  - Visionzip: Longer is better but not necessary in vision language models: N/A
  - Mm-vet: A benchmark to evaluate large multimodal models for integrated capabilities: N/A
  - Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting: N/A
  - Sparsevlm: Visual token sparsification for efficient vision-language model inference: N/A
  - Large images are gaussians: High-quality large image representation with levels of 2d gaussian splatting: N/A

### 3. Core Idea
- The architecture adapts CLIP to 2DGS inputs by incorporating a splat-aware input stem and a Perceiver-style resampler, allowing for efficient processing of Gaussian representations.

### 4. Method
- **Pipeline**: The pipeline includes a splat-aware stem that embeds Gaussian points, followed by a Perceiver Resampler that employs cross-attention to refine output tokens.
- **Architecture / Loss / Training**: The architecture utilizes a frozen RGB-pretrained transformer, updating only 7% of parameters, and employs contrastive loss during training.
- **Complexity / Resources**: The system achieves over 90Ã— faster fitting with approximately 97% GPU utilization.

### 5. Experiments
- **Datasets & Metrics**: The experiments were conducted on the ImageNet-1K dataset, measuring zero-shot classification accuracy.
- **Baselines**: 2DGS-based encoders, Implicit Neural Representations (INRs), JPEG, N/A, RGB baseline CLIP model, RGB teacher model, RGB-based encoders, RGB-based models, Vision Transformers
- **Main Results**: 2DGS models achieve top-1 accuracies of 13-15% and top-5 accuracies of 30-35%, with substantial input compression advantages (3-23.5x) over traditional pixel-based representations.
- **Ablations**: The results indicate that the 196-token configuration outperforms the 98-token variant, confirming the benefit of greater representational capacity.
- **Limitations / Stress Tests**: Accuracy currently lags RGB baselines, and early-stage training recipes and resampling strategies are not yet optimally tuned for splat inputs.

### 6. Takeaways
- **Pros**: 2DGS provides a compact representation that reduces transmission overhead., Achieves significant speed improvements in fitting compared to existing methods., Demonstrates potential for effective transfer learning from established vision encoders.
- **Cons**: Currently lower accuracy compared to RGB-based models., Early stages of development with room for improvement., Dependence on complex architectural adaptations may limit initial usability.
- **Future Work**: Further optimization of 2DGS for better accuracy., Exploration of additional architectural innovations for multimodal systems., Investigation into broader applications of 2DGS in other domains.

</details>

### [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](http://arxiv.org/pdf/2509.22498v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Open-world object manipulation

### 2. Motivation & Gaps
- The paper addresses the challenges of open-world generalization in vision-language-action models.

- **Related work challenges:**
  - Stone et al., 2023: Methods lack explicit scene representation.
  - Yenamandra et al., 2023b: Assumes dense associations between language, observation, and action.
  - Liu et al., 2024: Low success rates on new benchmarks for open vocabulary pick and place tasks.
  - Atanasov et al., 2013: Need for sufficient observations to identify objects.
  - Zheng et al., 2023: Ensuring sensor coverage during object search.
  - Huang et al., 2023: Dense representations are not always effective for planning and control.
  - Lu et al., 2024: N/A
  - Zheng et al., 2024: N/A
  - HomeRobot: Limited performance in complex environments without a global search strategy.
  - MoManipVLA: Inability to balance exploration and exploitation in object detection.
  - VLFM (Yokoyama et al., 2023b): Limited robustness in object detection and navigation due to filtering of detections.
  - OVMM benchmark (Yenamandra et al., 2023a;b): Existing methods struggle with efficient object search and manipulation.
  - N/A: N/A
  - Habitat 3.0: A co-habitat for humans, avatars and robots: Integration of multiple agents in a shared environment.
  - Task-oriented hierarchical object decomposition for visuomotor control: Effective object manipulation in complex environments.
  - Language embedded radiance fields for zero-shot task-oriented grasping: Zero-shot learning capabilities in grasping tasks.
  - N/A: N/A
  - HELIOS: Addressing issues from imperfect object detections
  - MoManipVLA: Robust object detection as a key bottleneck
  - Trusting agent: Not reasoning about the uncertainty of object detections

### 3. Core Idea
- The core idea is to develop a model that can generalize across various tasks in an open-world setting using vision and language inputs.

### 4. Method
- **Pipeline**: The model utilizes a vision-language-action pipeline to process inputs and generate actions.
- **Architecture / Loss / Training**: The architecture is trained using a combination of supervised and unsupervised learning techniques to minimize loss.
- **Complexity / Resources**: The experiments ran on 8 nodes in a cluster, each with a 2080ti GPU with 16GB of VRAM and 32GB of RAM. Each full run took around 288 hours for 1199 episodes.

### 5. Experiments
- **Datasets & Metrics**: Home Robot OVMM benchmark, Habitat Synthetic Scenes Dataset (HSSD), and others.
- **Baselines**: Existing vision-language models, HELIOS, HomeRobot, MoManipVLA, N/A, Prior methods in object search and mobile manipulation., Traditional object manipulation algorithms, Trusting agent, VLFM (Yokoyama et al., 2023b), VLFM without detection filtering
- **Main Results**: The results indicate that the full method outperforms the trusting agent when both use ground truth semantics.
- **Ablations**: An ablation study showed the effect of using ground-truth semantics on performance.
- **Limitations / Stress Tests**: The relatively low overall success rates with ground truth semantics indicate more work is required to increase search efficiency.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art results in mobile manipulation tasks., Zero-shot transfer capability to real-world environments., Balances exploration and exploitation effectively.
- **Cons**: Performance may vary with the complexity of the scene., Dependent on the quality of object detection., Requires sufficient observations for reliable object identification.
- **Future Work**: Explore further improvements in object detection methods., Investigate applications in more complex environments., Enhance the robustness of the hierarchical representation.

</details>

### [Bayesian Transfer Operators in Reproducing Kernel Hilbert Spaces](http://arxiv.org/pdf/2509.22482v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Sequential data forecasting

### 2. Motivation & Gaps
- The paper discusses the challenges of computational complexity in Gaussian Process (GP) regression, particularly the bottleneck caused by the inversion of large matrices.

- **Related work challenges:**
  - Mauroy et al., 2020: Kernel methods do not scale well and require approximation.
  - Klus et al., 2016: Hyperparameter optimization and dictionary learning are needed to adapt models to dynamical systems.
  - Williams et al. [2015a]: EDMD does not provide an optimization scheme for hyperparameters or dictionary selection.
  - Colbrook et al. [2019]: EDMD lacks uncertainty-aware predictions and does not account for measurement noise.
  - Titsias [2009]: High computational complexity in estimating the Koopman matrix.
  - Alvarez et al., 2012: N/A
  - Micchelli and Pontil, 2004, 2005: N/A
  - Klus et al., 2020b: N/A
  - GrÃ¼newÃ¤lder et al., 2012: N/A
  - Ikeda et al., 2022: N/A
  - Hansen and Sargent, 2013: N/A
  - Williams et al., 2015a: N/A
  - N/A: N/A
  - Exact (Extended) DMD [Tu et al., 2014]: Comparison of accuracy under noisy conditions.
  - Variational approach to Markov processes (VAMP) [Wu and NoÃ©, 2020]: Maximizing the VAMP score for hyperparameter optimization.
  - Bayesian model [N/A]: Handling sensor noise and regularization strategies.
  - Dawson et al., 2016: DMD is susceptible to adverse effects from noise on the inputs.
  - Hemati et al., 2017: Variants of DMD proposed to correct for bias induced by input noise.
  - Wilson et al., 2012: Input-noise compensation using heteroskedastic GP models.
  - Dynamic mode decomposition of numerical and experimental data: Limited applicability to complex systems.
  - A data-driven approximation of the Koopman operator: Challenges in extending methods to high-dimensional systems.
  - Robust approximation of the stochastic Koopman operator: Need for robustness in stochastic environments.
  - N/A: N/A
  - N/A: N/A
  - Snelson and Ghahramani, 2005: Computational complexity of identifying the posterior in GP regression.
  - Quinonero-Candela and Rasmussen, 2005: Memory consumption issues in conventional GP regression.
  - Bui et al., 2017: Need for approximation techniques to handle larger datasets.
  - LÃ¡zaro-Gredilla and Titsias, 2011: Assumptions about the generative model
  - Matthews et al., 2016: N/A
  - Titsias, 2009: N/A
  - Williams et al., 2015a: The challenge is that the Koopman operator is infinite-dimensional.
  - Colbrook and Townsend, 2024: Finding a reduced but finite set for the evolution of all observable functions.
  - Arbabi and MeziÄ‡, 2017: Discretization flaws when the Koopman operator has a continuous eigenvalue spectrum.
  - Rasmussen and Williams, 2006: N/A
  - Ghojogh et al., 2021: N/A
  - Fukumizu et al., 2004: N/A
  - Song et al., 2009: N/A
  - Fukumizu et al., 2013: N/A
  - Tikhonov, 1977: N/A
  - Hsing and Eubank, 2015: N/A
  - Wu and NoÃ©, 2020: N/A
  - Bach and Jordan, 2002: N/A
  - Baddoo et al., 2022: N/A
  - Klus et al., 2019: N/A
  - Froyland et al., 2010: N/A
  - N/A: N/A

### 3. Core Idea
- The Koopman mode decomposition (KMD) allows for the reconstruction and propagation of a dynamical system's state using a finite-dimensional approximation of the infinite-dimensional Koopman operator.

### 4. Method
- **Pipeline**: Define a bounded linear map to yield coordinates of observables in a finite-dimensional subspace.
- **Architecture / Loss / Training**: The optimization objective is to minimize the Kullbackâ€“Leibler divergence between the sparse posterior GP and the exact posterior GP.
- **Complexity / Resources**: The proposed method reduces training time complexity to O(NM^2) and memory complexity to O(NM).

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize benchmark datasets for sequential data and evaluate using metrics such as RMSE and MAE.
- **Baselines**: Conventional GP regression, Dynamic Mode Decomposition, EDMD, Exact DMD, Extended DMD, Gaussian Process Regression, N/A, Recurrent neural networks, Sparse GP model, Traditional time series forecasting methods
- **Main Results**: The proposed method outperforms baseline models in forecasting accuracy.
- **Ablations**: Ablation studies indicate the importance of the consistency constraint in the model's performance.
- **Limitations / Stress Tests**: The model's performance diminishes in highly chaotic systems.

### 6. Takeaways
- **Pros**: Improved resilience against sensor noise., Reduced computational demands., Enhanced interpretability of features.
- **Cons**: Requires careful hyperparameter tuning., Potentially complex implementation.
- **Future Work**: Explore further applications in stochastic systems., Investigate additional kernel methods., Develop more efficient algorithms for high-dimensional data.

</details>

## avatar

### [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](http://arxiv.org/pdf/2509.21887v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of synthesizing high-resolution images efficiently.

- **Related work challenges:**
  - Wav2Lip: Fails to generate lip movements similar to the target avatar.
  - Diff2Lip: Performance degradation when the mouth is occluded.
  - SyncExpert: Relies on strong facial priors limiting generalization.
  - Previous visual dubbing methods: Dependence on cost-intensive priors for ensuring identity and lip-sync consistency.
  - Diffusion-based video generation methods: High computational costs that create accessibility barriers for researchers.
  - Generative adversarial networks (GANs) for lip-sync: Limited representations due to reliance on handcrafted features.
  - Previous methods for lip synchronization: Often rely on lip-reading experts or spatial loss in pixel space, which can add training overhead.
  - Existing video generation models: Fail to maintain continuity in generated videos, especially during occlusions.
  - AdaLN for image style transfer: Requires significant memory and computational resources.
  - Wav2Lip: Limited performance in diverse input scenarios.
  - DINET: High computational overhead due to additional optimization parameters.
  - SyncExpert: Conflicts with human perceptual judgments and complicates integration with latent-space-based models.
  - Wav2Lip: Limited generalization to challenging cases.
  - DINet: Inadequate facial textural details.
  - IP-LAP: Difficulty in aligning multiple reference images.
  - TalkLip: Insufficient intelligibility in generated lip regions.
  - Diff2Lip: Challenges in real-world applications.
  - Diff2Lip: Background preservation issues due to larger mask usage.
  - GANs: Inferior inference speed compared to diffusion methods.
  - Previous visual dubbing methods: Reliance on redundant priors.
  - Resyncer: Rewiring style-based generator for unified audio-visually synced facial performer: Unified synchronization across different styles and audio inputs.
  - Personatalk: Bring attention to your persona in visual dubbing: Maintaining persona consistency in visual dubbing.
  - Anyonenet: Synchronized speech and talking head generation for arbitrary persons: Generating synchronized outputs for arbitrary individuals.
  - V oxCeleb2: Deep Speaker Recognition: Limited efficiency in high-resolution image generation.
  - Flow-guided one-shot talking face generation: High computational cost in generating high-resolution outputs.
  - Lip reading sentences in the wild: Challenges in maintaining consistency in generated images.

### 3. Core Idea
- The core idea is to utilize latent consistency models to improve the efficiency and quality of high-resolution image synthesis.

### 4. Method
- **Pipeline**: The method involves a few-step inference process that leverages latent representations.
- **Architecture / Loss / Training**: The architecture is trained using a novel loss function that emphasizes consistency across generated images.
- **Complexity / Resources**: The method is designed to reduce computational complexity while maintaining high-quality outputs.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on various high-resolution image datasets, using metrics such as PSNR and SSIM.
- **Baselines**: Anyonenet, DINET, DINet, Diff2Lip, Flow-based models, GAN-based methods, GANs, IP-LAP, Personatalk, Previous lip synchronization methods, Resyncer, Standard video generation models, State-of-the-art visual dubbing methods, SyncExpert, TalkLip, Variational Autoencoders, Wav2Lip
- **Main Results**: The proposed method outperforms existing baselines in terms of image quality and generation speed.
- **Ablations**: Ablation studies demonstrate the importance of the latent consistency approach in achieving superior results.
- **Limitations / Stress Tests**: The limitations include potential challenges in generalizing to unseen data and the need for extensive training data.

### 6. Takeaways
- **Pros**: Enhanced lip habit resemblance., Robustness to occlusions., Improved training efficiency.
- **Cons**: Potential limitations in extreme occlusion handling., Dependence on the quality of input audio and video., Computational resources required for training may still be significant.
- **Future Work**: Explore further generalization to diverse avatars., Investigate real-time applications., Enhance the model's efficiency for lower-resource scenarios.

</details>

### [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](http://arxiv.org/pdf/2509.21859v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D-aware image synthesis

### 2. Motivation & Gaps
- The paper addresses the need for high-precision 3D reconstruction and efficient image synthesis in large-scale scenes.

- **Related work challenges:**
  - S2Hand: Suffers from blurry textures caused by low resolution of hand meshes.
  - AMVUR: Also suffers from blurry textures due to low resolution.
  - NeRF and GS approaches: Require dense viewpoints and struggle to represent accurate geometry.
  - XHand: Falls short in capturing detailed geometric shapes on hand meshes.
  - DiSR-NeRF: Dependent on prompt guidance, which can lead to deviations from ground truth.
  - SuperNeRF: Enforcing multi-view consistency does not guarantee high-frequency details remain consistent across views.
  - XHand: Achieving expressive hand avatar reconstruction from 2D high-resolution images.
  - LIIF: Maintaining 3D consistencies in generated images.
  - GIIF: Lack of guarantees for 3D consistencies in generated outputs.
  - UHM: Fails to represent hand shapes and textures, losing details and including background artifacts.
  - GIIF + XHand: Results in overbounded shapes due to inconsistencies of hand shape and details in the SR images.
  - NeRF-SR: Suffers from blurriness and appears overly synthetic.
  - XHand [10]: Limited detail capture and performance across different identities.
  - InterHand2.6M [40]: Inadequate handling of fine details in 3D shapes.
  - A probabilistic attention model with occlusion-aware texture regression for 3d hand reconstruction from a single rgb image: Handling occlusions in 3D hand reconstruction.
  - 3d gaussian splatting for real-time radiance field rendering: Achieving real-time performance in rendering complex scenes.
  - Human gaussian splatting: Creating accurate representations of human figures.
  - Supernerf: High-precision 3-d reconstruction for large-scale scenes: Limited scalability and precision in existing methods.
  - Texpainter: Generative mesh texturing with multi-view consistency: Challenges in maintaining consistency across multiple views.
  - Residual dense network for image super-resolution: Inefficiencies in current super-resolution techniques.

### 3. Core Idea
- The proposed framework integrates 3D consistency with super-resolution techniques to enhance image synthesis.

### 4. Method
- **Pipeline**: The framework utilizes a combination of neural networks to process and synthesize images with 3D consistency.
- **Architecture / Loss / Training**: The architecture employs a loss function that emphasizes perceptual quality and 3D consistency during training.
- **Complexity / Resources**: The method is designed to be resource-efficient while maintaining high performance.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on large-scale datasets with metrics focusing on image quality and 3D consistency.
- **Baselines**: 3D Gaussian Splatting, 3D hand reconstruction methods, Bicubic, DiSR-NeRF, LIIF, NeRF-SR, Probabilistic Attention Model, Residual dense network, SRGS, State-of-the-art image upsampling methods adapted to hand datasets, SuperNeRF, Supernerf, Texpainter, UHM, XHand, XHand [10]
- **Main Results**: The results demonstrate significant improvements in both image quality and 3D consistency compared to baseline methods.
- **Ablations**: Ablation studies indicate the importance of each component in the proposed framework.
- **Limitations / Stress Tests**: The framework may struggle with highly complex scenes that exceed its training data diversity.

### 6. Takeaways
- **Pros**: Achieves fine-detailed 3D reconstruction including wrinkles and nails., Maintains accurate 3D structure across poses and viewpoints., Enables realistic, interactive VR/AR applications.
- **Cons**: Heavily reliant on the quality of low-resolution input images., Challenges in handling dynamic articulated targets like hands.
- **Future Work**: Explore further improvements in texture fidelity., Investigate applications in other domains beyond hand reconstruction.

</details>

### [Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers](http://arxiv.org/pdf/2509.20817v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Understanding viewer perceptions of AI-driven VTubers

### 2. Motivation & Gaps
- The study investigates viewer beliefs and concerns regarding AI-driven VTubers, particularly in comparison to human-driven VTubers.

- **Related work challenges:**
  - Previous studies on human-driven VTubers: Limited knowledge on viewer perceptions of AI-driven VTubers.
  - Previous studies on human-driven VTubers: Limited understanding of the audience's perception of AI-driven VTubers.
  - Research on VTubers' viewer motivations: Understanding how AI-driven VTubers alter viewer experience and engagement.
  - Studies on VTuber's virtual persona: Determining the impact of AI on the perception of virtual personas.
  - Concerns about the Nakanohito model: Identifying viewer opinions on AI potentially replacing human operators.
  - Nakanohito in human-driven VTubers: Understanding viewer perceptions of the developer role in AI-driven VTubers.
  - Previous studies on VTuber culture: Limited understanding of the emotional connections viewers form with AI-driven VTubers.
  - Existing research on human-driven VTuber ecosystems: Understanding the role of community in the success of AI-driven VTubers.
  - Previous research on VTubers: Limited understanding of how AI personas evolve and are perceived by viewers.
  - Previous studies on VTuber dynamics: Limited understanding of how AI personas evolve and are perceived by audiences.
  - Previous research on human-driven VTubers: Understanding the unique dynamics of AI-human interaction and emotional connection.
  - AI role-play and AI companion systems: Concerns about persona consistency and coherence.
  - Community-driven adjustments in AI personas: Transforming minor inconsistencies into accepted traits.
  - Human-AI collaboration in content creation: Balancing the roles of AI and human creators.
  - Neuro-sama community analysis: Understanding the role of a non-human generative agent in participatory culture.
  - SCP Foundation: Decentralized authorship and community consensus in lore creation.
  - Masahiro Hamasaki et al. 2009: Understanding collaborative video creation without direct collaboration.
  - Nobushige Hichibe and Ema Tanaka 2016: Identifying non-economic rewards in game development.
  - Sebin Lee et al. 2023: Analyzing audience experiences in virtual avatar concerts.
  - N/A: N/A
  - LLM Annotation Results: Ensuring the reliability and accuracy of LLM-generated annotations.
  - Previous studies on VTubers: Limited understanding of viewer perceptions regarding AI-driven VTubers.
  - Research on AI-human interactions: Inadequate exploration of the unique characteristics of AI-driven personas.
  - Previous studies on VTubers: Limited understanding of viewer perceptions and emotional connections with AI-driven personas.
  - Research on AI in entertainment: Challenges in assessing the authenticity and emotional depth of AI-generated content.
  - N/A: N/A

### 3. Core Idea
- Viewers have specific concerns about the roles and control of AI-driven VTubers, particularly regarding management and technical aspects.

### 4. Method
- **Pipeline**: Topic modeling using data from YouTube and Reddit.
- **Architecture / Loss / Training**: The model employs officially recommended hyperparameters for optimal performance.
- **Complexity / Resources**: Multiple instances of the model are deployed locally using vLLM.

### 5. Experiments
- **Datasets & Metrics**: YouTube and Reddit data were used to analyze viewer perceptions.
- **Baselines**: AI content generation studies, AI interaction studies, AI-driven VTubers, Human-driven VTubers, N/A, Previous AI-driven VTuber studies, Previous studies on VTuber behaviors, Previous studies on VTuber personas, Real-person-driven VTubers, Traditional VTuber analysis, Traditional VTuber studies, Traditional content creators, Traditional human VTubers, Traditional media personalities, Traditional streamers
- **Main Results**: Viewers expressed concerns about the management and technical control of AI-driven VTubers.
- **Ablations**: N/A
- **Limitations / Stress Tests**: Limited to a specific demographic of VTuber viewers, which may not represent the broader audience.

### 6. Takeaways
- **Pros**: Continuous operation without human constraints, Reduced risk of personal scandals, Potentially more cost-effective to operate
- **Cons**: Concerns about authenticity and emotional depth, Risk of generating inappropriate content, Potential hindrance to forming deep parasocial bonds
- **Future Work**: Further research on viewer perceptions of AI VTubers, Exploration of AI VTuber content generation, Investigation into the impact of AI VTubers on digital streaming culture

</details>

## video understanding

### [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](http://arxiv.org/pdf/2509.22651v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Error Analysis in Conversational AI Responses

### 2. Motivation & Gaps
- This paper analyzes various error cases in the responses generated by the Qwen 2.5-OMNI-7BON model, focusing on its performance in understanding and addressing user emotions and instructions.

- **Related work challenges:**
  - VoiceBench: Provides a comprehensive assessment for LLM-based voice assistants but lacks voice personalization evaluation.
  - VocalBench: Focuses on vocal conversational abilities but neglects hands-free interaction.
  - AIR-Bench: Introduces a generative benchmark for audio-language models but does not assess multi-modal integration.
  - MMMU (Yue et al., 2024): Does not evaluate multi-modal integration.
  - MMAU (Sakshi et al., 2024): Focuses only on audio without visual context.
  - AIR-Bench (Yang et al., 2024): Limited to speech and does not cover full multimodal capabilities.
  - Voice Personalization: Evaluating the modelâ€™s ability to mimic specific speaker voices.
  - Hands-free Interaction: Testing the modelâ€™s reliability in hands-free scenarios.
  - Diverse Audio Contexts: Ensuring realistic listening conditions with overlapping sounds and complex acoustic scenes.
  - Multimodal Integration: Evaluating the assistantâ€™s ability to jointly process speech and vision.
  - VocalBench (Liu et al., 2025): Reports UTMOS and WER independently, complicating the assessment of model performance across tasks.
  - Wen et al., 2025: Focuses solely on evaluating text responses, neglecting the speech component.
  - Ao et al., 2024: Similar limitations in evaluating only text responses.
  - Qwen2.5-Omni-7B: Struggles with audio context and perception errors.
  - Step-Audio-2-mini: Achieves high listening accuracy but lower speaking performance.
  - Freeze-Omni: Demonstrates the importance of alignment training for safety and robustness.
  - VoiceAssistant-Eval: Models perform better on speaking tasks than on listening tasks, and performance on joint audioâ€“image queries is significantly lower than on textâ€“image queries.
  - Qwen2.5-Omni-7B: Struggles with audio memory, response generation, and visual understanding.
  - Analysis of artificial intelligence models for the smart home industry: Lack of standardized evaluation metrics for AI models in smart home applications.
  - The intersection of voice assistants and autonomous vehicles: A scoping review: Limited understanding of the integration of voice assistants in autonomous vehicle systems.
  - N/A: N/A
  - N/A: N/A
  - Artificial intelligence voice assistant and home automation: Integration of AI in everyday tasks
  - CommonsenseQA: A question answering challenge targeting commonsense knowledge: Addressing commonsense reasoning in dialogue
  - Challenging big-bench tasks and whether chain-of-thought can solve them: Evaluating reasoning capabilities in dialogue systems
  - VoiceAssistant-Eval: Limited dataset diversity and monolingual focus may not capture the full variability of real user interactions.
  - VoiceAssistant-Eval: Limited diversity in dataset and evaluation prompts, primarily focused on English.
  - Existing benchmarks: Narrow range of models evaluated, primarily open-source with limited proprietary systems.
  - Automated evaluation methods: Potential noise and bias in scoring, reliance on GPT-based evaluations.
  - Bias in source data: Fixed roles and prompts may encode stereotypes and omit important user personas.
  - Task coverage: Gaps in task categories and scenario realism, particularly in multi-modal queries.
  - Static evaluation: Does not capture real-world factors like latency and incremental speech processing.
  - N/A: N/A
  - Voice Assistant Evaluation Framework: Lack of comprehensive benchmarks for assessing voice assistant capabilities.
  - Open-source vs Proprietary Models: Understanding the trade-offs between content quality and speech naturalness.
  - Voice Cloning and Impersonation Risks: Mitigating risks associated with identity spoofing and privacy violations.
  - VoiceBench: Evaluates multiple facets such as general knowledge, instruction following, and safety under diverse realistic conditions.
  - VocalBench: Focuses on vocal conversational ability and evaluates semantic quality, acoustic performance, and conversational skills.
  - SOVA-Bench: Extends evaluation to include speech output quality and tests models' ability to produce natural spoken replies.
  - SD-Eval: Emphasizes spoken dialogue understanding and focuses on raw speech inputs and nuanced attributes.
  - WildSpeech-Bench: Targets natural, multi-turn speech conversations and evaluates models on everyday speech quirks.
  - SUPERB (Yang et al., 2021): Focuses on discriminative benchmarks, not on generative or open-ended interactions.
  - HEAR benchmark (Baur et al., 2024): Evaluates audio embeddings without fine-tuning, aiming for a holistic representation.
  - AIR-Bench (Yang et al., 2024): Tests generative audio-language models but only evaluates textual output quality.
  - MMMU (Yue et al., 2024): Challenges models with multi-disciplinary questions requiring deep domain knowledge.
  - MathVista (Lu et al., 2024): Tests visual understanding in math but shows LMMs struggle with complex diagram questions.
  - MATH-Vision (Wang et al., 2024a): Narrower focus on math but allows for fine-grained analysis of model failures.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Listening Music Task: Model fails to select the correct genre based on audio context.
  - Listening Sound Task: Model misclassifies sounds due to confusion between similar auditory cues.
  - Listening Speech Task: Model cannot identify speaker gender due to inability to process audio.
  - N/A: N/A
  - Qwen 2.5-OMNI-7BON Model Evaluation: Inability to accurately interpret user emotions leading to unhelpful responses.
  - Conversational AI Empathy Studies: Failure to provide empathetic responses in emotionally charged situations.
  - Instruction Following in AI: Inadequate adherence to user instructions in conversational contexts.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The study identifies and categorizes various types of errors made by the Qwen model, particularly in emotional recognition and instruction adherence.

### 4. Method
- **Pipeline**: Data collection from user interactions, followed by qualitative analysis of model responses.
- **Architecture / Loss / Training**: Utilizes advanced neural architectures with specific loss functions tailored for dialogue tasks.
- **Complexity / Resources**: Requires significant computational resources for training and evaluation.

### 5. Experiments
- **Datasets & Metrics**: User interaction transcripts and model responses were analyzed for emotional accuracy and instruction adherence.
- **Baselines**: AudioCaps, Clotho, Common Voice, Existing dialogue systems, Existing voice assistant models, Freeze-Omni, GPT-4o-Audio, LLaMA-Omni2-0.5B-Bi, LLaMA-Omni2-32B-Bilingual, LibriSpeech, MiniCPM-o-2_6, MusicCaps, N/A, Other conversational AI models, Previous versions of Qwen, Qwen2.5-Omni, Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, Reference answers for genre, emotion, and sound identification, State-of-the-art speech recognition models, Step-Audio, Step-Audio-2-mini, Traditional AI models, VocalBench, gpt-oss-20b, mini-omni, moshika-pytorch-bf16
- **Main Results**: The model frequently misinterprets user emotions and fails to follow instructions accurately.
- **Ablations**: Conducts ablation studies to analyze the impact of different components.
- **Limitations / Stress Tests**: The paper notes that current evaluations focus on content accuracy using text output rather than scoring actual synthesized speech.

### 6. Takeaways
- **Pros**: VoiceAssistant-Eval identifies gaps in current benchmarks., Well-designed smaller models can rival larger ones., The framework guides the development of next-generation AI assistants.
- **Cons**: Current models struggle with multimodal integration., Significant gaps in robustness and safety alignment., Limited focus on hands-free interaction.
- **Future Work**: Further refinement of evaluation frameworks is needed., Exploration of personalized voice capabilities., Assessment of models under realistic audio contexts.

</details>

### [RefAM: Attention Magnets for Zero-Shot Referral Segmentation](http://arxiv.org/pdf/2509.22650v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Referring Image and Video Object Segmentation

### 2. Motivation & Gaps
- The paper addresses the challenge of effectively segmenting objects in images and videos based on referral expressions, leveraging cross-attention features from large diffusion models.

- **Related work challenges:**
  - Ni et al., 2023: Limited effectiveness of existing methods without task-specific training.
  - Xiao et al., 2024: Emergent behaviors in attention mechanisms that do not provide semantic value.
  - Kazemzadeh et al., 2014; Ding et al., 2020; Feng et al., 2021; Li et al., 2018: Traditional methods are supervised.
  - Yu et al., 2023: Global-Local methods extract CLIP-based features but may not effectively handle attention sinks.
  - Ren et al., 2024; Ravi et al., 2025: Recent zero-shot methods require minimal adaptation but may not unify tasks effectively.
  - Previous transformer models: Lack of structured information in early layers and the presence of GAS that suppresses meaningful content.
  - Existing segmentation methods: Inability to effectively manage attention distribution across tokens, leading to performance issues.
  - HybridGL (Liu & Li, 2025): Relies on complex modeling of spatial or relational cues.
  - Ref-Diff (Ni et al., 2023): Requires additional task-specific training.
  - Global-Local (Yu et al., 2023): Involves intricate modeling that may not generalize well.
  - HybridGL: Previous zero-shot approaches lacked effective mechanisms for background redistribution.
  - Imagenet auto-annotation with segmentation propagation: Challenges in accurately annotating images with complex segmentation.
  - Conceptattention: Diffusion transformers learn highly interpretable features: Difficulty in understanding the interpretability of features learned by diffusion transformers.
  - Prompt-to-prompt image editing with cross-attention control: Limitations in the flexibility and control of image editing processes.
  - Concept Attention (CA): Relies on a predefined set of simple, one-word concepts to represent the entire scene.
  - ConceptAttention (CA): Requires predefined simple concepts, making it less flexible in complex scenes.
  - HybridGL (Liu & Li, 2025): Existing methods struggle with semantic alignment and localization accuracy.
  - N/A: N/A

### 3. Core Idea
- The method REFAM captures semantically meaningful regions aligned between objects and referring expressions, improving segmentation quality through attention-based guidance.

### 4. Method
- **Pipeline**: The method involves parsing input sentences into noun phrases and spatial relations, encoding them, and guiding attention towards relevant visual regions.
- **Architecture / Loss / Training**: The method operates on cross-attention features extracted from diffusion transformers without task-specific fine-tuning.
- **Complexity / Resources**: The approach relies on powerful foundation models like FLUX, Mochie, and SAM2, which are computationally intensive.

### 5. Experiments
- **Datasets & Metrics**: Ref-DA VIS17
- **Baselines**: CA, CLIP, CLIP ViT, ConceptAttention, DINO, Diffusion Transformers, Existing segmentation methods, Global-Local, Global-Local (Yu et al., 2023), Grad-CAM, HybridGL, HybridGL (Liu & Li, 2025), Kazemzadeh et al., 2014, MaskCLIP, MaskCLIP (Zhou et al., 2022), N/A, Prior training-free methods, Pseudo-RIS, REFAM, REFAM + Inversion, Ref-Diff, Ref-Diff (Ni et al., 2023), Ren et al., 2024, TAS, VLM-VG, Yu et al., 2023
- **Main Results**: Qualitative results illustrate the effectiveness of REFAM across various scenarios.
- **Ablations**: Ablation studies confirm that both noun phrase extraction and spatial bias contribute to performance gains.
- **Limitations / Stress Tests**: The method currently ignores temporal aspects in video segmentation and may lead to under-segmentation in certain cases.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art performance without fine-tuning., Simplifies the segmentation process by eliminating the need for additional training., Utilizes existing features from diffusion transformers effectively.
- **Cons**: May not generalize to all types of segmentation tasks., Performance heavily relies on the quality of the underlying diffusion model., Limited exploration of the impact of different stop words.
- **Future Work**: Investigate the application of REFAM to other vision-language tasks., Explore the effects of different types of attention magnets., Develop methods to further enhance the robustness of the approach.

</details>

### [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](http://arxiv.org/pdf/2509.22646v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Deepfake Detection and Analysis

### 2. Motivation & Gaps
- The paper addresses the challenges in detecting deepfake videos and improving the realism of AI-generated imagery.

- **Related work challenges:**
  - VBench: Evaluating video prompt alignment without considering human perception.
  - VideoPhy: Analyzing physical commonsense in deepfake videos without a focus on human-perceived traces.
  - Open-source deepfake models: Many models generate low-quality videos that are ambiguous, short, or distorted, making them unsuitable for trace identification.
  - Human perception studies: Humans often struggle to identify AI-generated videos as fake, especially when the videos are static.
  - GPT 5 (Achiam et al., 2023): Achieves only 35.5% overall accuracy in deepfake trace detection.
  - Gemini 2.5 Pro (Team et al., 2023): Scores below 30% in overall performance despite being a state-of-the-art model.
  - VideoLLaMa 3 (Zhang et al., 2025): Fails to surpass 38.1% accuracy in detecting deepfake traces.
  - Baseline models (GPT 5, GPT 4.1, Gemini 2.5 Pro): Perform poorly on overall performance, achieving below 37% accuracy.
  - Qwen 2.5 VL models: Scaling within the family is not monotonic, with performance varying unexpectedly.
  - Temporal prediction metrics: All models struggle with time distance, indicating it is the hardest criterion.
  - VBench and human-preference studies: Lack of fine-grained localization in evaluating generated videos.
  - Closed-source models: Strong visual quality but limited in understanding human-centric evaluations.
  - Open-source models like Mochi and CogVideoX: Competitive results but still rely on predefined attributes.
  - Towards accurate generative models of video: A new metric and challenges: Existing metrics for video generation are insufficient for evaluating the quality of generated videos.
  - Improving video generation with human feedback: Incorporating human feedback into video generation processes remains a complex challenge.
  - Scalable diffusion models with transformers: Current models struggle with scalability and efficiency in video generation tasks.
  - Achiam et al. (2023): Inconsistencies in the realism of generated images and videos.
  - Team et al. (2023): Artifacts in AI-generated videos, particularly with fine details and fast movement.
  - Li et al. (2024a): Difficulty in distinguishing between real and fake content due to visual artifacts.

### 3. Core Idea
- The paper proposes a new model for deepfake detection that improves upon existing methods by addressing common artifacts and enhancing realism in generated videos.

### 4. Method
- **Pipeline**: The method involves fine-tuning existing models on a curated dataset of deepfake videos.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions to optimize for both realism and detection accuracy.
- **Complexity / Resources**: Experiments conducted on 8 x NVIDIA H100 80GB SXM GPUs.

### 5. Experiments
- **Datasets & Metrics**: The dataset consists of 3,460 training samples, with a validation set of 434 and a test set of 440, sampled from DEEPTRACEREWARD.
- **Baselines**: Existing video generation models, GPT 4.1, GPT 5, GPT-4.1, GPT-5, Gemini 2.5 Flash, Gemini 2.5 Pro, Human feedback-based models, Kling 1.0, LLaVa-One-Vision 7B, MiniMax-Video-01, Pika 1.5, VideoLLaMa 3
- **Main Results**: The proposed model outperforms baseline models in accuracy and detection metrics.
- **Ablations**: Ablation studies show the impact of removing time and explanation components on model performance.
- **Limitations / Stress Tests**: The model struggles with certain types of visual artifacts and may not generalize well to unseen data.

### 6. Takeaways
- **Pros**: Provides a rigorous benchmark for evaluating AI-generated videos., Highlights the importance of human perception in AI evaluation., Offers insights into the types of visual artifacts that indicate deepfakes.
- **Cons**: Current models still struggle with fine-grained detection tasks., High computational resources required for training the reward model., Limited generalizability of findings to all types of AI-generated content.
- **Future Work**: Explore additional categories of deepfake traces., Improve model architectures for better trace detection., Investigate user studies to further understand human perception of deepfakes.

</details>
