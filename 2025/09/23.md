# Daily Paper Digest Â· 2025-09-23
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [DESI Strong Lens Foundry II: DESI Spectroscopy for Strong Lens Candidates](http://arxiv.org/pdf/2509.18089v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Identification and analysis of strong lensing systems

### 2. Motivation & Gaps
- The study aims to identify and analyze potential strong lensing systems using data from DESI and other surveys.

- **Related work challenges:**
  - DESI Legacy Imaging Surveys: Need for spectroscopic redshifts for lens modeling to obtain physical parameters.
  - Huang et al. 2020: Identifying new lens candidates using residual neural networks.
  - Dawes et al. 2023: Finding new lensed quasar candidates using an autocorrelation algorithm.
  - Moustakas et al. 2023: Measuring velocity dispersion of lensing galaxies.
  - Moustakas et al. (2012): Varied methodologies from search methods to human grading schemes complicate meaningful evaluation.
  - Diehl et al. (2017): Inconclusive redshifts for some spectra based on DESI observations.
  - Adame et al. (2024): High degree of overlap between DESI Survey Validation and other surveys.
  - Jaelani et al. (2020): Accurate measurement of lens and source redshifts.
  - Petrillo et al. (2019): Visual inspection required for redshift confirmation.
  - Sonnenfeld et al. (2020): Identifying emission features in spectra for redshift determination.
  - Huang et al. (2021): Identifying and confirming lensing systems with varying redshifts.
  - Sharma & Linder (2022): Constraining cosmological parameters using lensing systems.
  - Talbot et al. (2021): Identifying spectral features in complex lensing systems.
  - Shu et al. 2016: Identifying LyÎ± emission in lens and source spectra.
  - Tanaka et al. (2016): Identification of a double-source lensing system with complex redshift measurements.
  - Bolton et al. (2012); Shu et al. (2012): Discrepancies in velocity dispersion measurements.
  - SLACS (Bolton et al. 2006): Challenges in distinguishing between lens and source spectral features.
  - N/A: N/A
  - H20 and H21: Target selection was less optimal compared to the current study.
  - SLACS: Previous lensing systems had lower redshift distributions.
  - BELLS programs: Limited sample sizes and lower redshift values compared to current findings.
  - Previous lens searches using human eye and traditional algorithms: Limited discovery of cluster-scale lenses and reliance on spectroscopic data with smaller fiber diameters.
  - Neural network-based searches: Need for thorough studies on the completeness and effectiveness of these methods.
  - DESI Legacy Imaging Surveys: Integrating data from multiple telescopes and surveys to create a comprehensive dataset.
  - N/A: N/A
  - DESI Strong Lensing secondary target Program: Failed to obtain redshifts for some targeted sources.
  - DESI Strong Lensing Secondary Target Program: Failed to obtain redshifts for several targeted objects.
  - UNWISE BLUE program: Limited success in identifying and confirming lensed arcs.
  - DESI survey: Inconclusive redshift measurements for some putative lenses.
  - HSC DR2 imaging: Difficulty in distinguishing between true lenses and foreground galaxies.
  - N/A: N/A

### 3. Core Idea
- To utilize imaging and spectroscopic data to confirm and characterize strong lensing systems.

### 4. Method
- **Pipeline**: Redrock pipeline for redshift determination.
- **Architecture / Loss / Training**: Residual neural networks (ResNet) are used for identifying lens candidates.
- **Complexity / Resources**: Utilizes resources from the National Energy Research Scientific Computing Center and various funding agencies.

### 5. Experiments
- **Datasets & Metrics**: Utilized HSC DR2 and LS DR9 images for analysis.
- **Baselines**: AGEL survey, BELLS, DESI EDR observations, DESI Legacy Imaging Surveys, Dawes et al. 2023, Existing imaging surveys, H20, H21, HSC SSP, HST observations, Huang et al. 2020, Huang et al. 2021, KiDS, N/A, Pan-STARRS, Previous DESI data, Previous lensing studies, Previous studies on lensing systems, Previous studies on strong lensing systems, Redshift fitting methods, S24, SLACS, Spectral analysis techniques from other surveys, Spectroscopic data from DESI
- **Main Results**: Identification of several strong lens candidates and confirmation of non-lenses.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The sample size is still relatively small, and further observations are needed for some candidates.

### 6. Takeaways
- **Pros**: Provides crucial spectroscopic redshifts for lens modeling., Confirms multiple strong lensing systems., Contributes to the DESI Strong Lens Foundry project.
- **Cons**: Limited to 73 candidate systems., Some candidates were determined not to be lenses., Future data is needed for remaining systems.
- **Future Work**: Further observations to confirm additional lensing systems., Implications for lens searches with neural networks., Future publications to present ongoing data.

</details>

### [RnGCam: High-speed video from rolling & global shutter measurements](http://arxiv.org/pdf/2509.18087v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- High-speed video capture and processing

### 2. Motivation & Gaps
- The paper addresses the challenges in high-speed video capture using rolling and global shutter techniques, aiming to improve the efficiency and quality of video data.

- **Related work challenges:**
  - Previous RS compressive video methods: Struggle to recover dense scenes with bright, detailed backgrounds.
  - Data-driven video interpolation methods: Generalize poorly to out-of-distribution scenarios, particularly with chaotic motions.
  - SuperSlomo: Limited to discrete time steps and struggles with complex motions.
  - EMA-VFI: Requires large datasets for training and has limitations in frame recovery.
  - Timelens: Expensive and saturates with camera motion.
  - Prior work on RS-diffuser measurements: Struggles with dense scenes
  - 3D total variation regularization: Does not explicitly regularize spatio-temporal consistency
  - 3D total variation (3DTV): Struggles to regularize dense scenes due to lack of strong motion and smoothness priors.
  - Video interpolators: Perform poorly on complex motion scenes and are prone to hallucination.
  - Video from stills: Lensless imaging with rolling shutter: High costs and limitations in capturing dynamic scenes.
  - Super SloMo: High quality estimation of multiple intermediate frames for video interpolation: Performance limitations in scenes with complex motion.
  - Dynamic structured illumination microscopy with a neural space-time model: Static assumptions that do not hold in all scenarios.
  - A parallel proximal algorithm for anisotropic total variation minimization.: N/A
  - High spatio-temporal resolution video with compressed sensing.: N/A
  - Amt: All-pairs multi-field transforms for efficient frame interpolation.: N/A
  - Bacon: Band-limited coordinate networks for multiscale scene representation.: N/A
  - Rank minimization for snapshot compressive imaging.: N/A
  - Coded aperture compressive temporal imaging.: N/A
  - Video frame interpolation with transformer.: N/A
  - Video-rate volumetric functional imaging of the brain at synaptic resolution.: N/A
  - Neural sensors: Learning pixel exposures for hdr imaging and video compressive sensing with programmable sensors.: N/A
  - Nerf: Representing scenes as neural radiance fields for view synthesis.: N/A
  - Untrained networks for compressive lensless photography.: N/A
  - Video frame interpolation via adaptive separable convolution.: N/A
  - A comprehensive survey on video frame interpolation techniques.: N/A
  - Random coded sampling for high-speed hdr video.: N/A
  - Programmable pixel compressive camera for high speed imaging.: N/A
  - Deconvolving circular sas images using implicit neural representations.: N/A
  - Compressive acquisition of linear dynamical systems.: N/A
  - Wavelet implicit neural representations.: N/A
  - Rolling shutter imaging on the electric grid.: N/A
  - Diffraction line imaging.: N/A
  - Deconvolving diffraction for fast imaging of sparse scenes.: N/A
  - Dual-shutter optical vibration sensing.: N/A
  - Implicit neural representations with periodic activation functions.: N/A
  - Coordinate-based internal learning for tomographic imaging.: N/A
  - Fourier features let networks learn high frequency functions in low dimensional domains.: N/A
  - Time lens: Event-based video frame interpolation.: N/A
  - Time lens++: Event-based frame interpolation with parametric non-linear flow and multi-scale fusion.: N/A
  - Coded strobing photography: Compressive sensing of high speed periodic videos.: N/A
  - Compressive holographic video.: N/A
  - 100,000 frames-per-second compressive imaging with a conventional rolling-shutter camera by random point-spread-function engineering.: N/A
  - High performance imaging using large camera arrays.: N/A
  - Compressed sensing for practical optical imaging systems: a tutorial.: N/A
  - Disorder-invariant implicit neural representation.: N/A
  - Neural fields in visual computing and beyond.: N/A
  - Event-enhanced snapshot compressive videography at 10k fps.: N/A
  - Extracting motion and appearance via inter-frame attention for efficient video.: N/A
  - Neural fields in visual computing and beyond: Integration of neural fields with traditional video capture methods.
  - Event-enhanced snapshot compressive videography at 10k fps: Achieving high frame rates while maintaining image quality.
  - Extracting motion and appearance via inter-frame attention: Efficient video frame interpolation techniques.
  - N/A: N/A

### 3. Core Idea
- The core idea is to utilize a combination of rolling and global shutter measurements to enhance the speed and quality of video capture, leveraging a novel optical system design.

### 4. Method
- **Pipeline**: The method involves capturing images from both rolling and global shutter sensors, aligning them, and applying corrections for white balance and memory limitations.
- **Architecture / Loss / Training**: The architecture incorporates a neural space-time model to optimize the image reconstruction process.
- **Complexity / Resources**: The system requires careful calibration of optical components and synchronization of multiple camera systems.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various datasets to evaluate the performance of the proposed method against traditional techniques.
- **Baselines**: 3D total variation regularization, 3DTV (Dual shutter), 3DTV (Single shutter), 3DTV-based methods, EMA, EMA-VFI, Global shutter methods, N/A, Previous RS compressive video methods, Previous RS-diffuser recovery methods, Previous state-of-the-art video capture systems, Standard rolling shutter techniques, State-of-the-art frame interpolators, Super SloMo, SuperSlomo
- **Main Results**: Our method achieves a PSNR of 31.99 dB, while various ablations show lower PSNR values.
- **Ablations**: Ablation tests demonstrate the importance of each component in the model, with significant performance drops when components are removed.
- **Limitations / Stress Tests**: Tests reveal limitations in sensor sensitivity and the need for further optimization in specific lighting conditions.

### 6. Takeaways
- **Pros**: High-speed video capture at kHz frame rates., Cost-effective hardware solution., Improved fidelity in dense scenes.
- **Cons**: Complex scenes may still pose challenges for accurate recovery., Dependent on the quality of the optical components used., Limited by the physical constraints of the sensors.
- **Future Work**: Explore further optimizations in INR methods., Investigate applications in various fields like neuroscience and microscopy., Develop more robust hardware setups.

</details>

### [DESI Strong Lens Foundry III: Keck Spectroscopy for Strong Lenses Discovered Using Residual Neural Networks](http://arxiv.org/pdf/2509.18086v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Determine redshifts of lensed galaxies using Keck NIRES spectra

### 2. Motivation & Gaps
- The study aims to obtain accurate redshifts for lensed galaxies using advanced spectroscopic techniques.

- **Related work challenges:**
  - DESI Legacy Imaging Surveys: Finding strong lenses and measuring redshifts in the optical range.
  - Hubble Space Telescope program: Follow-up observations to confirm lensed arcs.
  - Huang et al. 2020: Need for follow-up observations for confirmation of strong lens candidates.
  - Cikota et al. 2023: Challenges in obtaining source redshifts due to emission features being beyond the optical range.
  - DESI Collaboration et al. 2016a,b: Inability to obtain source redshifts for certain systems using DESI.
  - Huang et al. 2020: Previous naming conventions for lensed galaxies differ from the current IAU standards.
  - Huang et al. 2021: Challenges in securing redshifts due to low signal-to-noise ratios in spectra.
  - DESI Collaboration et al. (2024): Random uncertainties for the DESI redshifts vary significantly across different galaxy types.
  - Tran et al. (2022): Inconsistent redshift measurements due to varying observational conditions and equipment.
  - Zhou et al. (2020): Discrepancies between photometric and spectroscopic redshifts for elliptical galaxies.
  - N/A: N/A
  - Huang et al. 2020: Identifying lens candidates with a score of 2.5 or above.
  - Huang et al. 2021: Obtaining source redshifts with shorter exposure times.
  - Shu et al. 2018; Shu et al. 2021: Estimating star formation rates and supernova rates in lensed source galaxies.
  - N/A: N/A
  - Tessore, N., Bellagamba, F., & Metcalf, R. B. 2016: N/A
  - Tran, K.-V. H., Harshan, A., Glazebrook, K., et al. 2022: N/A
  - Vanzella, E., Meneghetti, M., Caminha, G. B., et al. 2020: N/A
  - Vegetti, S., Koopmans, L. V. E., Bolton, A., Treu, T., & Gavazzi, R. 2010: N/A
  - Walsh, D., Carswell, R. F., & Weymann, R. J. 1979: N/A
  - Wilson, J. C., Henderson, C. P., Herter, T. L., et al. 2004: N/A
  - Wojtak, R., Hjorth, J., & Gall, C. 2019: N/A
  - Zhou, R., Newman, J. A., Mao, Y.-Y., et al. 2020: N/A

### 3. Core Idea
- The study aims to measure redshifts of lensed sources beyond the optical range using NIR spectroscopy.

### 4. Method
- **Pipeline**: Keck NIRES spectroscopy for redshift determination.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Utilized multiple observational setups including the Kast spectrograph and DESI imaging.

### 5. Experiments
- **Datasets & Metrics**: Redshift measurements from Keck NIRES spectra and comparison with DESI data.
- **Baselines**: DESI, DESI observations, DESI redshift measurements, HST observations, Keck NIRES, N/A, Optical spectroscopy methods, Photometric redshifts from previous studies, Previous strong lensing studies, Previous studies on lensed galaxies, Standard star observations from CALSPEC database
- **Main Results**: Successful measurement of lensed source redshifts for six out of seven observed systems.
- **Ablations**: N/A
- **Limitations / Stress Tests**: Limited sample size and exposure times affected redshift measurements.

### 6. Takeaways
- **Pros**: Provides essential redshift data for strong lensing applications., Combines multiple spectroscopic techniques for improved accuracy., Enhances automated strong lens searches in future surveys.
- **Cons**: Limited by the observational constraints of NIRES., Some systems had non-detections due to high airmass., Dependence on previous imaging surveys for lens identification.
- **Future Work**: Further refinement of automated lens searches., Exploration of additional astrophysical questions using the data., Integration of more advanced spectroscopic techniques.

</details>

## Gaussian Splatting

### [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](http://arxiv.org/pdf/2509.18090v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Surface Reconstruction

### 2. Motivation & Gaps
- The paper presents GeoSVR, a method for high-quality surface reconstruction that aims to improve accuracy, completeness, and detail preservation.

- **Related work challenges:**
  - 3D Gaussian Splatting (3DGS): Reliance on well-structured point clouds initialization leading to inaccuracies and uncovered regions.
  - Neural Radiance Fields (NeRF): Computationally expensive and often lack clearly defined edges, resulting in geometry ambiguity.
  - Neural Radiance Fields (NeRF): Weak efficiency in rendering and quality due to uniform grid assumptions.
  - 3D Gaussian Splatting (3DGS): View-inconsistent rendering problems and reliance on sparse point clouds leading to uncertainty.
  - Multi-view stereo-based methods: Trade-off between training time and quality for complex scenes.
  - Previous approaches based on SDF and 3DGS: Lack of effective scene constraints for accurate surface formation.
  - Monocular depth studies: Difficulty in maximizing the utilization of monocular depth cues for accurate surface reconstruction.
  - NeuS: High reconstruction time and suboptimal surface alignment.
  - Neuralangelo: Inability to handle complex scenes effectively.
  - PGSR: Limited performance in real-world scenarios.
  - SDF-based Neuralangelo: Limited performance in reflective regions and areas with insufficient coverage.
  - MonoGSDF: Struggles with geometry regularization leading to oversmoothing.
  - VCR-GauS: Underfitting due to reliance on geometry cues.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Geo-Neus: 0.01 improvement on Chamfer distance is significant compared to previous methods.
  - PGSR: Quality is close to ground truth but lacks detail.
  - MonoGSDF: Lack of reported training time and open-sourced code.
  - SVRaster: Inefficiencies in code implementation affecting training time.
  - Gaussian Splatting-based approaches: Limited representation capability for ray tracing.

### 3. Core Idea
- GeoSVR utilizes a voxel-based representation to achieve high-quality surface reconstruction while maintaining efficiency.

### 4. Method
- **Pipeline**: The method involves a series of components that enhance surface reconstruction quality and efficiency.
- **Architecture / Loss / Training**: The architecture focuses on voxel-level regularizations and efficient coding implementations.
- **Complexity / Resources**: The method exhibits superior efficiency with minimal increase in GPU memory occupancy.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize DTU and TnT datasets, measuring accuracy with Chamfer distance and overall quality with F1-Score.
- **Baselines**: 2DGS, 3D Gaussian Splatting (3DGS), 3DGS, 3DGS approaches, 3DGS-based approaches, BakedSDF, Deep Blending, GOF, GS2Mesh, Gaussian Splatting, Geo-NeuS, Instant NGP, Mip-NeRF 360, MonoGSDF, MonoSDF, N/A, NeRF, NeuS, Neural Radiance Fields (NeRF), Neuralan-geo, Neuralangelo, PGSR, Previous monocular depth integration techniques, SDF-based methods, SVRaster, SuGaR, TnT, VCR-GauS
- **Main Results**: GeoSVR achieves competitive rendering speeds and high-quality surface reconstructions across datasets.
- **Ablations**: An ablation study reveals the efficiency of various components, with multi-view regularization being the most time-consuming.
- **Limitations / Stress Tests**: The method struggles with reflections, textureless areas, and transparent surfaces, leading to suboptimal geometry.

### 6. Takeaways
- **Pros**: High geometric accuracy in surface reconstruction., Effective detail preservation., Robust performance across diverse scenarios.
- **Cons**: Challenges in optimizing sparse voxels., Dependence on external depth cues may introduce errors., Complexity in ensuring local geometry consistency.
- **Future Work**: Exploration of additional scene constraints., Improvement of voxel optimization techniques., Integration with other surface reconstruction methods.

</details>

### [Distribution of non-Gaussian states in a deployed telecommunication fiber channel](http://arxiv.org/pdf/2509.18080v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Demonstrate the distribution of non-Gaussian continuous variable quantum states

### 2. Motivation & Gaps
- The study aims to achieve significant progress in the distribution of non-Gaussian continuous variable states within a functional real-world telecommunication setting.

- **Related work challenges:**
  - Previous experiments on coherent transmission of single photon states and squeezed states: Lack of exploration in the transmission of non-Gaussian bosonic codes in real-life environments.
  - N/A: N/A
  - Quantum state tomography: Achieving high fidelity in the reconstruction of non-Gaussian states under transmission losses.
  - Homodyne detection: Maintaining synchronization and minimizing jitter during state transmission.
  - Optical fiber communication: Ensuring efficient transmission of quantum states over long distances.
  - Quantum teleportation of non-Gaussian states: Overcoming losses in transmission
  - Entanglement swapping: Maintaining non-Gaussianity over long distances
  - Quantum repeaters based on bosonic codes: Integrating quantum error correction coding schemes
  - Fault-Tolerant Measurement-Based Quantum Computing with Continuous-Variable Cluster States: N/A
  - Fault-Tolerant Continuous-Variable Measurement-based Quantum Computation Architecture: N/A
  - Fault-tolerant quantum computation with static linear optics: N/A
  - Optical continuous-variable qubit: N/A
  - Quantum tele-amplification with a continuous-variable superposition state: N/A
  - Teleportation of non-classical wave packets of light: N/A

### 3. Core Idea
- Establish a reliable procedure for generating and transmitting non-Gaussian states through an optical fiber network.

### 4. Method
- **Pipeline**: Generate non-Gaussian states using photon-subtracted squeezed vacuum states and transmit through optical fiber.
- **Architecture / Loss / Training**: Utilizes a balanced detector and a mobile measurement station for state reconstruction, correcting for detection losses.
- **Complexity / Resources**: Requires a 70 m single-mode fiber and a deployed optical link of over 300 m, with a total detection efficiency of 88%.

### 5. Experiments
- **Datasets & Metrics**: Measured states and fidelity metrics after transmission through fiber.
- **Baselines**: Background squeezed vacuum state, Coherent transmission of single photon states, Ideal odd SchrÃ¶dinger cat state, N/A, Transmission of squeezed states
- **Main Results**: Achieved a fidelity of 52% after transmission, with a maximum fidelity of 53% for a cat state of amplitude Î±= 0.66.
- **Ablations**: Investigated the effects of detection loss and phase offsets on the reconstruction of Wigner functions.
- **Limitations / Stress Tests**: The study acknowledges limitations in synchronization and potential drift in measurement windows over time.

### 6. Takeaways
- **Pros**: Validates the practical feasibility of distributing non-Gaussian states in real-world settings., Provides a foundation for future quantum networking applications., Demonstrates the potential for high-dimensional, continuous-variable quantum information processing.
- **Cons**: Transmission is affected by loss and phase noise., Limited to a specific wavelength (1550 nm) for telecom applications., Challenges remain in achieving fully coherent quantum networks.
- **Future Work**: Further exploration of non-Gaussian state transmission in various environments., Development of error-correction techniques for bosonic codes., Investigation into the scalability of quantum networks using non-Gaussian states.

</details>

### [On the de Almeida--Thouless Transition Surface in the Multi-Species SK Model with Centered Gaussian External Field](http://arxiv.org/pdf/2509.18066v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyzing the support cardinality of Parisi measures in multi-species spherical mixed p-spin models.

### 2. Motivation & Gaps
- The study addresses the simultaneous replica symmetry breaking (RSB) in multi-species models, building on previous results while aiming to establish bijections between the supports of all species.

- **Related work challenges:**
  - Toninelli's perturbative argument: Establishing the existence of a transition above the conjectured de Almeidaâ€“Thouless line.
  - Batesâ€“Slomanâ€“Sohn's multi-species version: Confirming the expected transition above the AT surface.
  - Talagrand's characterization of phase transition: Proving the absence of a transition inside the AT line for the SK model.
  - Chenâ€“Issaâ€“Mourrat [17]: Confirmed the uniqueness of Parisi measures for positive-definite âˆ†2.
  - Toninelliâ€™s perturbative argument [56]: Achieved RSB outside the AT surface but needed adjustments for irreducible positive-semidefinite âˆ†2.
  - Recent works on spherical models: Limited rigorous results in the Ising case, mainly confined to the high temperature regime.
  - [27, Theorem 1.4]: Previous work established uniqueness of fixed-points in certain regimes, but this paper strengthens the argument for broader conditions.
  - [27, Theorem 1.4]: Strengthening the argument for uniqueness of fixed-points.
  - [11]: Employing an elementary argument for uniqueness in the two-species case.
  - [2, Theorem 3]: Verifying uniqueness for a special indefinite, irreducible âˆ†2.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Chenâ€“Mourrat [18]: Analyzing a critical point equation of a certain functional to determine support cardinality.
  - Batesâ€“Sohn [12]: Establishing simultaneous RSB in multi-species spherical mixed p-spin models.
  - N/A: N/A

### 3. Core Idea
- The paper proves that the projections of a Parisi measure into each species have supports of the same cardinality under certain conditions.

### 4. Method
- **Pipeline**: The proof involves examining identities satisfied by the support of a Parisi measure and utilizing properties of deterministic external fields.
- **Architecture / Loss / Training**: Utilizes the Parisi functional and fixed-point equations to derive conditions for RS solutions.
- **Complexity / Resources**: The complexity is managed through the irreducibility of âˆ†2 and the concavity of the fixed-point map.

### 5. Experiments
- **Datasets & Metrics**: Theoretical framework based on Gaussian random variables and variational formulas.
- **Baselines**: Multi-species SK model with positive-definite interaction matrix, N/A, Previous results on RSB in single and multi-species models, Previous results on fixed-point uniqueness, SK model, Single-species SK model, Spherical mixed p-spin model
- **Main Results**: The support cardinality of a Parisi measure is shown to be the same across all species under the conditions specified.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The converse of the established conditions remains an open question.

### 6. Takeaways
- **Pros**: Provides a rigorous proof of the exactness of the AT surface., Clarifies the phase transition behavior in multi-species SK models., Contributes to the understanding of replica symmetry breaking.
- **Cons**: Limited to models with centered Gaussian external fields., Does not address the uniqueness of the minimizer in detail., May not generalize to all types of external fields.
- **Future Work**: Explore the implications of the findings on neural networks., Investigate other types of external fields in the SK model., Extend the results to more complex multi-species interactions.

</details>

## avatar

### ["I don't like my avatar": Investigating Human Digital Doubles](http://arxiv.org/pdf/2509.17748v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Understanding the impact of realism in virtual characters on user perception

### 2. Motivation & Gaps
- The study aims to explore how different levels of realism in speech and animation affect the perceived personality traits of virtual characters.

- **Related work challenges:**
  - Previous avatar perception studies: Often used generic digital avatars that do not resemble existing humans.
  - Studies on personalized digital avatars: Usually fall short in terms of high-fidelity due to access to high-end systems.
  - Familiarity aspect in avatar perception: Remains underexplored despite its potential impact on user perception.
  - Kang et al. [22]: Limited exploration of how viewers perceive the identity of communication partnersâ€™ avatars during interactions.
  - Gonzalez-Franco et al. [15]: Did not investigate the fidelity of avatar representations in the context of identity perception.
  - Matthew et al. [13]: Focused on realistic avatars but did not consider the impact of familiarity on identification.
  - Garau [14]: Mismatches between appearance and behavior reduce presence.
  - Pakanen et al. [45]: Realistic avatars can enhance both self and social presence, but fidelity was limited.
  - Fraser et al. [12]: Previous studies did not investigate the familiarity dimension in relation to digital doubles.
  - Previous studies on avatar representation: Limited understanding of how familiarity affects social presence.
  - Previous studies on avatar representation: Limited understanding of how familiarity and style influence user interaction.
  - N/A: N/A
  - Amadou et al. [1]: Previous findings on social presence and avatar realism.
  - Higgins et al. [16]: Inconsistencies in the relationship between avatar realism and user affinity.
  - Previous studies on avatar affinity: Contradictory results regarding the attractiveness of realistic avatars.
  - Pakanen et al. [45]: Users prefer photorealistic avatars for others but are critical of their own.
  - [4]: Neural responses vary based on familiarity, indicating personal familiarity amplifies identity-specific processing.
  - Previous studies on avatar realism: Lack of comprehensive understanding of how avatar realism affects user experience.
  - Angela Tinwell et al. (2013): Perception of psychopathy and the Uncanny Valley in virtual characters.
  - Stephen Wonchul Song and Mincheol Shin (2024): Uncanny Valley Effects on Chatbot Trust, Purchase Intention, and Adoption Intention.
  - Sean Thomas et al. (2022): Investigating the influence of speech and animation realism on perceived personality.
  - N/A: N/A

### 3. Core Idea
- The research investigates how variations in speech and animation realism can alter the perception of personality traits in virtual characters.

### 4. Method
- **Pipeline**: The study employs a series of experiments where participants interact with virtual characters exhibiting different levels of speech and animation realism.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Consumer-grade hardware and software were used, but high-end devices are needed for truly realistic avatars.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize a custom dataset of virtual characters with varying realism levels, measuring user perception through surveys.
- **Baselines**: Generic digital avatars, Low realism avatars, N/A, Personalized digital avatars, Previous avatar studies, Previous studies on avatar perception, Previous studies on avatar perception and identity recognition., Previous studies on avatar realism, Previous studies on realistic virtual humans [1, 12, 16, 72], Previous studies on self-identification and avatar perception, Realistic MetaHuman avatar, Standard avatar creation methods, Stylized RPM cartoon avatar, Traditional character models, User experience metrics
- **Main Results**: Results indicate that higher realism in speech and animation significantly enhances the perceived personality traits of virtual characters.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The study acknowledges limitations in the diversity of character designs and participant demographics.

### 6. Takeaways
- **Pros**: Higher realism enhances self/other identification., Increased perceived realism with realistic avatars., Greater social presence with familiar avatars.
- **Cons**: Lower identification and affinity with familiar avatars., Participants dislike their own realistic avatars., Critical perception of self-representations.
- **Future Work**: Further exploration of avatar familiarity in different contexts., Investigate psychological implications of avatar perception., Study the impact of avatar design on user experience in various applications.

</details>

### [Clothing agnostic Pre-inpainting Virtual Try-ON](http://arxiv.org/pdf/2509.17654v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Virtual Try-On

### 2. Motivation & Gaps
- This study aims to improve the virtual inspection model for clothing synthesis by addressing existing limitations in clothing interference and input dependency.

- **Related work challenges:**
  - Leffa: Inaccuracy in bottom detection and existing clothing silhouette interference.
  - CP-VTON: High FID value of 47.36 indicating poor realism.
  - VITON-HD: Limitations in accurate clothing alignment and natural deformation.
  - Leffa: Complexity and fine-detail distortion in flow field regularization.
  - VITON-HD: Weak detection and masking of lower-body clothing areas.
  - Leffa model: Inability to effectively synthesize clothing types due to existing clothing silhouettes.
  - CATVTON: Inability to accurately reproduce the design of reference clothing, leading to distorted outputs.
  - IDM-VTON: Limited ability to reflect the actual clothing style and silhouette.
  - Leffa-VITONHD: Commonly influenced by input clothing, resulting in unnatural distortions.
  - Leffa: Inability to handle complex clothing structures and maintain silhouette consistency.
  - Leffaâ€™s model: Achieved lower performance in silhouette synthesis accuracy.
  - Existing models: Inability to consistently reproduce sleeve length, pattern, and style.

### 3. Core Idea
- The CAP-VTON framework introduces a modular structure that is model-agnostic and supports whole-body synthesis without clothing interference.

### 4. Method
- **Pipeline**: The pipeline includes bottom and dress masking functions for whole-body synthesis.
- **Architecture / Loss / Training**: Utilizes FID, SSIM, and LPIPS as performance indicators for training and evaluation.
- **Complexity / Resources**: Operated in a Google Colab environment with NVIDIA A100 GPU, using Python and PyTorch.

### 5. Experiments
- **Datasets & Metrics**: Dress Code dataset for multi-category clothing detection and silhouette consistency evaluation.
- **Baselines**: CATVTON, CP-VTON, Existing virtual try-on models, IDM-VTON, Leffa, Leffa model, Leffaâ€™s model, OOTDiffusion, VITON-HD
- **Main Results**: Achieved 92.5% accuracy in consistency evaluation, outperforming Leffaâ€™s model by 15.4%.
- **Ablations**: Comparison of the performance of CaP-VTON against existing models showed a trade-off between image quality and silhouette accuracy.
- **Limitations / Stress Tests**: CaP-VTON showed slightly lower quantitative performance due to additional image inference pipeline.

### 6. Takeaways
- **Pros**: Improved accuracy in clothing synthesis., Enhanced skin restoration quality., Model-agnostic characteristics allow for broader application.
- **Cons**: Complexity in model architecture., Potential for fine-detail distortion., Limitations in pre-processing accuracy.
- **Future Work**: Potential for application in e-commerce and custom styling., Further improvements in masking processes., Expansion to other diffusion-based models.

</details>

### [Community Covert Communication - Dynamic Mass Covert Communication Through Social Media](http://arxiv.org/pdf/2509.17508v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Community Detection in Social Networks

### 2. Motivation & Gaps
- The paper discusses the challenges of community detection in social networks, particularly in the context of adversarial sock puppet operations that can manipulate community structures.

- **Related work challenges:**
  - Operation Mockingbird: Historical influence operations have evolved with technology, but current methods lack comprehensive analysis.
  - Sock Puppet Techniques: Existing techniques for influence operations are often limited in scope and effectiveness.
  - Deniable Encryption: Current encryption methods do not adequately address the unique challenges posed by social media communication.
  - Operation Earnest Voice (OEV): Use of fake accounts to spread propaganda on social media.
  - #ChinaAngVirus disinformation campaign: Spreading rumors about the Chinese government using fake identities.
  - Facebook-Cambridge Analytica scandal: Manipulating election results through influence techniques and fake profiles.
  - N/A: N/A
  - Existing covert communication methods: Lack of effective mechanisms to manage and validate sock puppet accounts in real-time.
  - Social network analysis: Difficulty in isolating and identifying communities without detection.
  - One Time Pad (OTP): Keys must be as long as the plaintexts, making OTP impractical for many applications.
  - Existing community detection methodologies: They often rely on stable dynamics of social networks, which may not apply to sock puppet accounts.
  - Existing community detection algorithms: They rely on the stability of communities, which is undermined by sock puppet activities.
  - Brute force community detection methods: These methods are computationally intractable due to the vast number of potential community configurations.
  - Poisoning attacks on identified communities: Such attacks are rendered impossible without knowledge of the secret key controlling the profiles.
  - N/A: N/A

### 3. Core Idea
- The paper proposes a new approach to secure encrypted communication that does not rely on a single channel, addressing the vulnerabilities in community detection due to sock puppet manipulation.

### 4. Method
- **Pipeline**: The method involves encoding ciphertexts with links between communities and using hypergraphs for efficient communication.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The computational complexity is high due to the intractable nature of community partitioning based on numerous attributes.

### 5. Experiments
- **Datasets & Metrics**: The paper utilizes synthetic datasets to demonstrate the effectiveness of the proposed cryptographic methods and community detection.
- **Baselines**: Brute force methods, Existing community detection algorithms, Existing influence operation techniques, Existing social network analysis techniques, N/A, One Time Pad, Traditional community detection algorithms, Traditional covert communication methods
- **Main Results**: The proposed method shows potential in enhancing the security of encrypted communications against sock puppet operations.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The paper discusses the limitations of community detection in the context of sock puppet accounts and the need for continuous updates to the API.

### 6. Takeaways
- **Pros**: Redefines communication security in the context of social media., Enables large-scale covert communication through community dynamics., Utilizes existing social media infrastructure for innovative applications.
- **Cons**: Potential for misuse in malicious influence operations., Complexity in managing dynamic communities effectively., Challenges in ensuring the security of the communication channel.
- **Future Work**: Further research on the implications of CCC in real-world scenarios., Development of more robust security measures for covert communication., Exploration of ethical considerations in the use of influence technologies.

</details>

## video understanding

### [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](http://arxiv.org/pdf/2509.18096v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation and Segmentation

### 2. Motivation & Gaps
- The study investigates the role of different layers in a model for image generation and segmentation, particularly focusing on cross-modal interactions between image features and text.

- **Related work challenges:**
  - U-Net-based models: Attention maps are often noisy and spatially fragmented, limiting mask fidelity.
  - Multi-modal diffusion transformers (MM-DiT): DiT-based models and their characteristics remain underexplored.
  - Cross-attention maps: Limited understanding of their role in visual perception tasks.
  - MM-DiT: Precise interactions within multi-modal attention mechanisms remain largely underexplored.
  - ODISE: Integrates a frozen diffusion backbone with CLIP for segmentation.
  - DiffSegmenter: Mines self- and cross-attention maps from pretrained text-to-image diffusion models.
  - N/A: N/A
  - DiffSegmenter: Achieving competitive results in open-vocabulary semantic segmentation.
  - MaskCLIP: Exploiting emergent semantic grouping for unsupervised segmentation.
  - U-Net diffusion-based segmentation methods: Limited performance on complex datasets without refinement.
  - CLIP-based approaches: Processing entire classnames for prediction, which differs from the evaluation setting.
  - Training-free methods built on diverse backbones: Inability to effectively leverage semantic grouping in unsupervised segmentation.
  - N/A: N/A
  - N/A: N/A
  - CLIP: Limited performance in zero-shot segmentation tasks.
  - Stable Diffusion: Memory constraints when using multiple encoders.
  - N/A: N/A
  - Seg4Diff: Evaluates segmentation without postprocessing or upsampling, limiting performance on extremely small objects.
  - Diffusion Models: Ground class names differently from ground-truth annotations, introducing an inherent mismatch that impacts accuracy.
  - Attention Perturbation: Perturbations applied to layers other than layer 9 result in minor degradation of image fidelity.

### 3. Core Idea
- Layer 9 plays a critical role in cross-modal interaction, particularly in aligning image features with text, which is essential for generating semantically relevant images.

### 4. Method
- **Pipeline**: The method involves applying attention perturbation at different layers during image generation to analyze the effects on image quality.
- **Architecture / Loss / Training**: The loss is applied to a single 'sweet-spot' layer per backbone, focusing on dense perception tasks.
- **Complexity / Resources**: Requires significant computational resources due to the use of multiple vision-language encoders.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize datasets such as COCO, Pascal-Context, and ADE20K for evaluating segmentation and image generation.
- **Baselines**: CLIP, CLIP-H/14, COCO, DINO, DINO-B/8, DiffCut, DiffSegmenter, Flux.1-dev, N/A, ODISE, SA1B, Stable Diffusion 3 (SD3), Stable Diffusion 3.5, Stable Diffusion 3.5 (SD3.5), Standard DiT models, U-Net diffusion models, U-Net-based models
- **Main Results**: The results demonstrate that perturbing layer 9 leads to semantically irrelevant images, while leveraging perturbed samples as negatives improves image quality.
- **Ablations**: Ablation studies on normalization methods and attention layers indicate optimal performance with raw scores after softmax.
- **Limitations / Stress Tests**: The method's limitations include performance issues on small objects and the representation-annotation gap affecting accuracy.

### 6. Takeaways
- **Pros**: Enhanced semantic grounding capabilities in MM-DiT., High-quality segmentation masks generated without explicit class information., Improved image synthesis quality as a by-product of enhanced cross-modal alignment.
- **Cons**: Limited understanding of DiT-based models compared to U-Net., Noisy attention maps in standard models., Dependence on mask-annotated data for fine-tuning.
- **Future Work**: Exploration of unified models that excel in both generation and perception., Further analysis of attention mechanisms in other transformer architectures., Investigation of additional applications for open-vocabulary segmentation.

</details>

### [UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning](http://arxiv.org/pdf/2509.18094v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Segmentation

### 2. Motivation & Gaps
- The paper addresses the need for improved video segmentation techniques by integrating global and local reasoning into a unified model.

- **Related work challenges:**
  - Region-level captioning: Limited to performing either referring or segmentation tasks independently.
  - Referring expression segmentation: Fail to integrate fine-grained perception capabilities into visual reasoning.
  - Reasoning segmentation: Cannot generate mask-grounded responses simultaneously.
  - LISA: Integrating segmentation with implicit queries.
  - Qwen-VL: Narrowing the gap with proprietary models while maintaining spatial granularity.
  - Existing methods: Lack of end-to-end solutions for unifying object referring and mask prediction.
  - Previous studies on region cropping: Direct region cropping can provide better object awareness compared to positional pointers.
  - Existing methods for visual prompts: They often fail to effectively utilize temporal information and struggle with mask prediction quality.
  - ReVOS: Requires complex reasoning abilities based on world knowledge.
  - MeViS: Challenges in predicting masks based on implicit text queries.
  - GroundMoRe: Visual answer generation that requires joint spatial and temporal grounding.
  - MoRA: Limited performance in zero-shot settings compared to UniPixel.
  - State-of-the-art methods on RES datasets: Overwhelmed by limited reasoning segmentation data during training.
  - Existing methods in VideoRefer-Bench: None support the new PixelQA task effectively.
  - Previous models for object segmentation: Lack of integration between referring and segmentation tasks.
  - Existing multi-modal models: Insufficient performance in pixel-level understanding.
  - GPT-4V: Limited support for pixel-level tasks.
  - Video-ChatGPT: Inadequate performance in multi-frame scenarios.
  - Video-LLaMA: Sub-optimal results in holistic-level understanding.
  - Grounded Caption Generation (GCG): Limited computing resources prevented scaling up training data to incorporate more pixel-level tasks.
  - Mask Decoder Mechanism: Current mask decoder predicts the first mask on the first frame and propagates it, lacking flexibility in predicting on the best frame.
  - End-to-end referring video object segmentation with multimodal transformers: Integration of multimodal data for effective segmentation.
  - Activitynet: A large-scale video benchmark for human activity understanding: Lack of comprehensive benchmarks for evaluating video segmentation.
  - Coco-stuff: Thing and stuff classes in context: Difficulty in contextual understanding of objects in video.
  - Microsoft COCO: Common objects in context: Limited context understanding in video segmentation tasks.
  - Generalized referring expression segmentation: Challenges in accurately segmenting objects based on complex expressions.
  - Improved baselines with visual instruction tuning: Inefficiencies in current video understanding models.
  - N/A: N/A

### 3. Core Idea
- The core idea is to unify global and local reasoning in a single large language model to enhance video segmentation performance.

### 4. Method
- **Pipeline**: The method involves a multi-stage pipeline that integrates both global context and local details for video segmentation.
- **Architecture / Loss / Training**: The architecture employs a novel loss function that balances global and local features during training.
- **Complexity / Resources**: The model is designed to be resource-efficient while maintaining high performance.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize standard video segmentation datasets and metrics to evaluate performance.
- **Baselines**: Activitynet benchmark, End-to-end referring video object segmentation with multimodal transformers, Existing video segmentation models, GLUS, GPT-4V, HyperSeg, InstructSeg, InternVL2, LISA, LLaVA-OV, LMPM, MTTR, MoRA, N/A, PixelLM, Previous multi-modal models, Qwen2.5-VL, Reasoning segmentation, ReferFormer, Referring expression segmentation, Region-level captioning, SAM 2.1, Sa2V A, State-of-the-art methods in video understanding, TrackGPT, Traditional object segmentation models, VISA, ViLLa, Video-ChatGPT, Video-LLaMA, VideoLISA
- **Main Results**: The proposed model outperforms existing methods in various segmentation tasks.
- **Ablations**: Ablation studies demonstrate the importance of both global and local reasoning components.
- **Limitations / Stress Tests**: The model shows limitations in handling extremely complex scenes with overlapping objects.

### 6. Takeaways
- **Pros**: Flexibly comprehends visual prompt inputs., Generates mask-grounded responses., Unifies referred and segmented objects for enhanced reasoning.
- **Cons**: Limited to specific tasks and benchmarks., Complexity in integrating multiple modalities., Potential challenges in real-time applications.
- **Future Work**: Explore broader applications in real-world scenarios., Enhance model efficiency for real-time processing., Investigate further integration of user interactions.

</details>
