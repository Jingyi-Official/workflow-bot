# Daily Paper Digest Â· 2025-09-30
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression](http://arxiv.org/pdf/2509.25136v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Post-training quantization and pruning

### 2. Motivation & Gaps
- The paper addresses the need for efficient neural network compression techniques that maintain accuracy while reducing model size.

- **Related work challenges:**
  - Jaderberg et al., 2014: Traditional techniques often require fine-tuning and/or costly search procedures.
  - Wang et al., 2025c: Existing methods primarily focus on fully connected layers, overlooking other settings.
  - Yu & Bouganis, 2022: Fine-tuning-free factorization methods are less common.
  - N/A: N/A
  - Wang et al. (2025c): Activation distortion in linear layers
  - Yang et al. (2020); Liebenwein et al. (2021): Energy-based singular value pruning criterion
  - Idelbayev & Carreira-PerpiÃ±Ã¡n (2020); Yu & Bouganis (2022): Training-based rank learning and NAS-based search
  - SVD-NAS: Requires an expensive search procedure, making it impractical for quick applications.
  - ALDS: Struggles with compact models like MobileNet-V2.
  - DFPC: Does not achieve significant performance improvements without fine-tuning.
  - SVD-NAS: Combining with expensive search methods could yield excellent results but at the cost of compression time.
  - ALDS: Compressing compact models like MobileNet-V2 remains difficult.
  - IFM: Limited reporting of results makes it hard to compare effectiveness.
  - A survey of quantization methods for efficient neural network inference: Existing methods often compromise accuracy for efficiency.
  - Batch normalization: Accelerating deep network training by reducing internal covariate shift: Challenges in maintaining performance during compression.
  - Dynamic low-rank estimation for transformer-based language models: Difficulty in applying compression techniques to transformer models.
  - N/A: N/A
  - Wang et al., 2025c: Cannot compute Cholesky decomposition for singular positive-semidefinite matrices.
  - Wang et al., 2025b: SVD-based whitening does not cover all cases.
  - N/A: N/A
  - Cisse et al. (2017): Constraining the operator norms of the layers of neural networks.
  - N/A: N/A
  - Shumaylov et al. (2025): Provides insights into the factorization of neural networks from an information-geometric point of view.
  - N/A: N/A

### 3. Core Idea
- The method relies on the computation of (uncentered) whitening matrices to estimate optimal low-rank projections, leveraging the i.i.d. assumption for model compression and robustness against distribution shifts.

### 4. Method
- **Pipeline**: The pipeline involves using a calibration dataset to gather activation moments, computing whitening matrices, and applying low-rank projections for model compression.
- **Architecture / Loss / Training**: Utilizes a loss function that accounts for both accuracy and model size during training.
- **Complexity / Resources**: The implementation uses basic PyTorch for low-rank operators and measures throughput on different models at various compression ratios.

### 5. Experiments
- **Datasets & Metrics**: 8192 calibration samples divided into batches of 64 samples.
- **Baselines**: Activation-aware SVD, DFPC, DeiT-B/16, Existing quantization methods, IFM, IterTVSPrune, MobileNet-V2, N/A, Pruning methods, Pruning techniques, Quantization methods, ResNeXt-101 (32Ã—8d), ResNeXt-50 (32Ã—4d), ResNet-18, ResNet-20, ResNet-50, ResNet-56, SVD with energy criterion, SVD-based methods, Standard SVD, ViT-B/16
- **Main Results**: Peak memory usage and runtime breakdown for different models.
- **Ablations**: Conducted ablation studies to assess the impact of different components of the framework.
- **Limitations / Stress Tests**: The method's performance is limited by the lack of specialized implementations for speedup, particularly in convolutional layers.

### 6. Takeaways
- **Pros**: Unified framework compatible with a broad class of layers., Zero-overhead rank allocator for user-specified FLOPs or parameter count budgets., Strong accuracy-compression trade-offs in the fine-tuning-free regime.
- **Cons**: Performance may vary with different architectures., Requires careful selection of compression ratios., Not all models benefit equally from the method.
- **Future Work**: Explore further optimizations for different layer types., Investigate the application of BALF in other domains., Develop more comprehensive evaluations against additional baselines.

</details>

### [Triangle Splatting+: Differentiable Rendering with Opaque Triangles](http://arxiv.org/pdf/2509.25122v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Mesh-based Novel View Synthesis

### 2. Motivation & Gaps
- The paper addresses the need for efficient rendering techniques that can integrate with game engines while maintaining high visual quality.

- **Related work challenges:**
  - Neural Radiance Fields: Long training times and slow inference restrict practical applicability.
  - 3D Gaussian Splatting: Gaussian primitives are not natively compatible with mesh-based graphics pipelines.
  - Triangle Splatting: Results in unstructured triangle soup with no connectivity and soft, semi-transparent triangles.
  - Differentiable mesh for 3D reconstruction: Mainly handles synthetic objects and does not extend to real-world scenes.
  - BakedSDF: Introduces overhead and increases overall training time.
  - MiLo: Optimizes mesh geometry during training but requires separate learning for color.
  - Triangle Splatting: Ensuring gradient flow during optimization with opaque primitives.
  - MiLo: Requires post-processing to texture the mesh.
  - 2DGS: Requires additional post-processing steps for mesh extraction and coloring.
  - GOF: Requires more vertices for lower PSNR and SSIM.
  - Mip-NeRF 360: Achieving high visual quality with opaque primitives.
  - Radiant triangle soup: Maintaining connectivity and visual fidelity in 3D reconstruction.
  - MobileNeRF: Efficient rendering on mobile architectures.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- Triangle Splatting+ optimizes opaque triangles with shared-vertex connectivity, enabling high-quality rendering compatible with game engines.

### 4. Method
- **Pipeline**: The method involves a tailored training strategy for opaque primitives, focusing on optimizing visual quality through effective pruning strategies.
- **Architecture / Loss / Training**: Utilizes a combination of hard and soft pruning strategies to enhance visual quality and performance.
- **Complexity / Resources**: The method is designed to be efficient to train while achieving state-of-the-art performance.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on Mip-NeRF360 and Tanks & Temples datasets using PSNR, LPIPS, and SSIM metrics.
- **Baselines**: 2DGS, 3D Gaussian Splatting, BakedSDF, Baseline, GOF, MiLo, N/A, Neural Radiance Fields, RaDe-GS, Traditional splatting methods, Triangle Splatting, Without blending weight pruning, Without hard pruning step, Without sigma decay
- **Main Results**: Triangle Splatting+ outperforms baseline methods in visual quality metrics.
- **Ablations**: Ablation studies demonstrate the impact of various pruning strategies on visual quality.
- **Limitations / Stress Tests**: Limitations include challenges in accurately recovering backgrounds and handling transparent objects.

### 6. Takeaways
- **Pros**: High visual quality in mesh-based rendering., Immediate integration into VR/AR environments., Supports downstream applications like physics-based simulation.
- **Cons**: Resulting mesh is only semi-connected., Still relies on a differentiable framework which may have limitations.
- **Future Work**: Explore full connectivity in triangle meshes., Investigate further optimizations for real-time applications., Develop additional features for interactive walkthroughs.

</details>

### [Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs](http://arxiv.org/pdf/2509.25121v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dynamic Image Graph Construction (DIGC) on FPGA

### 2. Motivation & Gaps
- The paper addresses the need for efficient graph construction in high-resolution image processing, highlighting the limitations of existing GPU implementations.

- **Related work challenges:**
  - Prior works optimizing graph construction algorithmically: Often compromise DIGCâ€™s flexibility, accuracy, or generality.
  - ViG: None directly address ViG-specific bottlenecks.
  - ViHGNN: N/A
  - DVHGNN: N/A
  - Existing graph construction methods: Inefficiency in processing large-scale graphs
  - Traditional sorting algorithms: High computational overhead in neighbor selection
  - Previous FPGA implementations: Limited scalability and performance
  - Existing GPU implementations for image processing: Memory exhaustion and scalability limits
  - N/A: N/A
  - Mega: A memory-efficient gnn accelerator exploiting degree-aware mixed-precision quantization: Memory efficiency in GNNs
  - Hardware-aware gnn reshaping for acceleration with gpu tensor cores: Acceleration of GNNs using hardware resources
  - Accelerating sparse graph neural networks with tensor core optimization: Optimization of sparse GNNs
  - Gcv-turbo: End-to-end acceleration of gnn-based computer vision tasks on fpga: End-to-end acceleration for computer vision tasks
  - Graphagile: An fpga-based overlay accelerator for low-latency gnn inference: Low-latency inference for GNNs
  - Magnas: A mapping-aware graph neural architecture search framework for heterogeneous mpsoc deployment: Architecture search for heterogeneous deployment
  - Evgnn: An event-driven graph neural network accelerator for edge vision: Event-driven processing for edge vision
  - Accelerating graph convolutional networks through a pim-accelerated approach: PIM acceleration for GCNs
  - Exploiting hybrid stacked memory for energy-efficient processing of graph convolutional networks: Energy efficiency in GCN processing

### 3. Core Idea
- The architecture scales with input resolution, offering greater benefits for high-resolution workloads and supports diverse graph construction strategies.

### 4. Method
- **Pipeline**: The architecture utilizes a deeply pipelined design with static parallelism to minimize critical path delays.
- **Architecture / Loss / Training**: Modular architecture that scales across image resolutions, ViG layer types, and model sizes.
- **Complexity / Resources**: Utilizes Xilinx Alveo U280 with specific configurations for DSP, LUT, BRAM, and URAM.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on various ViG models across different image resolutions, comparing runtime and speedup against CPU and GPU baselines.
- **Baselines**: AMD EPYC 7763 CPU, CPUs, Existing FPGA implementations, GPUs, N/A, NVIDIA RTX A5000 GPU, Optimized CPU and GPU DIGC implementations, Traditional graph construction methods
- **Main Results**: Delivers a 2.13Ã— to 4.62Ã— speedup versus CPU + FPGA and 1.83Ã— to 2.10Ã— versus GPU + FPGA configurations.
- **Ablations**: N/A
- **Limitations / Stress Tests**: N/A

### 6. Takeaways
- **Pros**: High performance improvements even where GPU baselines fail due to out-of-memory errors., Scalable design supporting various image sizes and ViG variants without hardware changes., Efficient resource utilization on the Xilinx Alveo U280 FPGA.
- **Cons**: Complexity in design and implementation, Potential limitations in flexibility compared to software solutions, Dependency on FPGA hardware availability
- **Future Work**: Explore further optimizations for different FPGA architectures., Investigate integration with other vision tasks., Enhance flexibility for real-time applications.

</details>

## Gaussian Splatting

### [Score Distillation of Flow Matching Models](http://arxiv.org/pdf/2509.25127v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation from Prompts

### 2. Motivation & Gaps
- The paper explores the generation of images based on textual prompts, showcasing various styles and themes.

- **Related work challenges:**
  - Flow matching: Practical differences remain, including variations in noise schedules, loss weighting, and architectures.
  - Score distillation: Its applicability to flow-matching T2I models remains unclear.
  - SANA-Sprint: Requires nontrivial finetuning of rectified-flow checkpoints into TrigFlow counterparts.
  - Albergo et al., 2023: Theoretical equivalences in diffusion model objectives.
  - Kingma & Gao, 2023: Understanding the impact of different parameterizations on performance.
  - Ma et al., 2024: Reconciliation of varied parameterizations in diffusion processes.
  - DDPM (Ho et al., 2020a): Claims of superiority in weight-normalized distributions without controlling for p(t) may be misleading.
  - SiD for U-Net: Adversarial enhancement in U-Net faces challenges due to the lack of a natural bottleneck in DiT backbones.
  - Fisher divergence minimization: Extending the few-step SiD method into SiD-DiT requires careful handling of flow-matching losses.
  - SANA-SPRINT: Requires finetuning rectified flow checkpoints, limiting its applicability.
  - SD3 and SD3.5: Comparison metrics can be misleading across different model families and sizes.
  - MMDiT architecture: Maintaining visual fidelity and computational efficiency while distilling models.
  - Diffusion GAN: Enhancing performance through adversarial training with additional high-quality data.
  - FLUX.1-DEV: Performance gap due to guidance-mechanism mismatch.
  - Prior work on score distillation: Misconceptions regarding applicability to flow-based models.
  - N/A: N/A
  - DDIM (Song et al., 2020): Reduces the number of function evaluations (NFEs) without retraining, but performance degrades when NFEs drop below 20.
  - Diffusion distillation (Luhman & Luhman, 2021; Salimans & Ho, 2022): Requires access to real or teacher-synthesized data for trajectory distillation.
  - ReFlow (Liu et al., 2022b): Claims that rectified flow is more amenable to one-step distillation, but this has been challenged.
  - Wang et al. (2025): The approaches remain fundamentally bounded by the teacher modelâ€™s generation quality.
  - Zhou et al. (2024; 2025c): Score distillation can outperform the teacher model even with limited sampling steps.
  - Huang et al. (2024): Flow Generator Matching may not always be necessary due to the linear transformations between velocity and predictions.
  - Previous image generation models: Limited ability to capture complex and nuanced prompts.
  - Style transfer techniques: Struggles with maintaining coherence in generated images.

### 3. Core Idea
- Utilizing a diverse set of prompts to generate high-quality images across various themes and styles.

### 4. Method
- **Pipeline**: The method involves a series of steps to process prompts and generate images using advanced neural networks.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances style and content fidelity during training.
- **Complexity / Resources**: The model requires significant computational resources for training and inference.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize a variety of datasets to evaluate the quality of generated images against established metrics.
- **Baselines**: DDIM, DPM-Solver, Diffusion GAN, EDM Heunâ€™s sampler, FLUX, FLUX.1-DEV, FLUX.1-dev, GANs, N/A, Previous diffusion models, SANA, SANA-SPRINT, SANA-Sprint, SD-Turbo, SD3, SD3-MEDIUM, SD3-Medium, SD3.5, SD3.5-LARGE, SD3.5-LARGE teacher model, SD3.5-Large, SD3.5-MEDIUM, SD3.5-MEDIUM teacher model, SD3.5-Medium, Standard SDE/ODE formulations, StyleGAN, Triflow, VQ-VAE
- **Main Results**: The results demonstrate superior image quality and adherence to prompts compared to baseline models.
- **Ablations**: Ablation studies indicate the importance of prompt diversity in enhancing image generation.
- **Limitations / Stress Tests**: Tests reveal challenges in generating images for highly abstract or vague prompts.

### 6. Takeaways
- **Pros**: SiD applies broadly to text-to-image flow matching models., No teacher finetuning or architectural changes required., Single codebase and hyperparameter configuration suffice across all T2I flow-matching models.
- **Cons**: Practical differences in noise schedules and loss weighting remain., Uncertainty about the necessity of additional adaptation steps., Concerns about stability in score distillation.
- **Future Work**: Further exploration of score distillation techniques., Investigate the impact of different noise schedules., Develop more robust methods to ensure stability.

</details>

### [Equilibrium states for non relativistic Bose gases with condensation](http://arxiv.org/pdf/2509.25101v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyze the construction of equilibrium states in quantum field theory with a background condensate.

### 2. Motivation & Gaps
- The paper addresses the construction of equilibrium states for non-relativistic quantum particles interacting with a background Bose-Einstein condensate, filling a gap in existing literature that primarily focuses on systems without such a background.

- **Related work challenges:**
  - Theoretical research on BEC: Understanding the behavior of bosonic gas near the critical temperature.
  - Rigorous analysis of interacting quantum many-body systems: Limited investigation of thermal equilibrium at positive temperature for weakly interacting particles.
  - Emergence of classical Gibbs measures: Deriving effective descriptions for bosonic gases with two-body non-local interactions.
  - [52]: Rigorous derivation of Gibbs measure in the presence of a background condensate.
  - [26]: Inclusion of existing background condensate phase in the analysis of grand canonical Gibbs states.
  - [44]: Overcoming divergences in thermodynamic limits of Gibbs states.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - [21]: Similar results for the case of Fermi fields have been recently obtained.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The main result of this paper is Theorem 6.8, which proves that the relative partition function and correlation functions converge under certain conditions on physical parameters.

### 4. Method
- **Pipeline**: The method involves constructing equilibrium states using KMS conditions and perturbative techniques, analyzing the interaction Hamiltonian and its effects on the system's partition function.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The complexity arises from the need to manage the growth of interaction terms in the power series and ensuring convergence.

### 5. Experiments
- **Datasets & Metrics**: N/A
- **Baselines**: Gibbs measures for field theories, N/A, Perturbative methods in quantum field theory, Previous theoretical works on BEC, Previous works on Gibbs states without background condensate
- **Main Results**: The convergence of the relative partition function and correlation functions is established under specific bounds on physical parameters.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The convergence of the power series defining the relative partition function is discussed, highlighting challenges in achieving absolute convergence.

### 6. Takeaways
- **Pros**: Provides a rigorous framework for understanding equilibrium states in Bose gases., Introduces innovative methods using auxiliary fields to mediate interactions., Addresses gaps in the literature regarding thermal equilibrium in quantum systems.
- **Cons**: Focuses primarily on weakly interacting particles, limiting broader applicability., Assumes the presence of an external potential, which may not be realistic in all scenarios., The complexity of the mathematical framework may hinder practical applications.
- **Future Work**: Explore the implications of the results in higher-dimensional systems., Investigate the effects of stronger interactions on equilibrium states., Develop numerical methods to simulate the behavior of Bose gases near critical temperatures.

</details>

## avatar

### [SIE3D: Single-image Expressive 3D Avatar generation via Semantic Embedding and Perceptual Expression Loss](http://arxiv.org/pdf/2509.24004v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating expressive 3D avatars from a single image and text description, focusing on maintaining identity and expression fidelity.

- **Related work challenges:**
  - Arc2Avatar: Expression control is non-semantic; it requires users to manipulate abstract numerical parameters rather than using natural language commands.
  - DreamFusion, HeadSculpt, TADA: These methods cannot faithfully reconstruct a specific personâ€™s identity from a photo.
  - Existing methods that accept both image and text: They are rare and generally yield unsatisfactory results.
  - Arc2Avatar: Limited control over semantic attributes like facial expressions or accessories.
  - Wonder3D: Inability to generate meaningful 3D identities.
  - SF3D: Challenges in maintaining a neutral expression.
  - Wonder3D: Fails to preserve quality in side-view perspectives.
  - SF3D: Does not maintain identity consistency across different views.
  - Arc2Avatar: While achieving high ID scores, it lacks in semantic control.
  - Wonder3d: Single image to 3d using cross-domain diffusion: N/A
  - Sf3d: Stable fast 3d mesh reconstruction with uv-unwrapping and illumination disentanglement: N/A
  - Gans trained by a two time-scale update rule converge to a local nash equilibrium: N/A
  - Cosface: Large margin cosine loss for deep face recognition: N/A

### 3. Core Idea
- Introducing a decoupled text conditioning mechanism and an expression-aware loss based on a facial recognition model to achieve fine-grained control over expression and appearance attributes.

### 4. Method
- **Pipeline**: The method involves generating 3D avatars using a single image and text prompts, incorporating semantic embedding and perceptual expression loss.
- **Architecture / Loss / Training**: Utilizes a facial recognition model for expression-aware loss.
- **Complexity / Resources**: The method demonstrates strong expressive generation capabilities with a focus on computational efficiency.

### 5. Experiments
- **Datasets & Metrics**: Celebrity face image dataset
- **Baselines**: Arc2Avatar, DreamFusion, HeadSculpt, N/A, SF3D, Wonder3D
- **Main Results**: SIE3D achieves competitive performance in identity preservation, semantic control, and expression fidelity.
- **Ablations**: Ablation studies show the critical role of 'perceptual expression loss' and the trade-off involved with 'semantic embedding'.
- **Limitations / Stress Tests**: The method's performance is affected by the removal of key components, indicating areas for improvement.

### 6. Takeaways
- **Pros**: Enables fine-grained, semantic control of expressions and attributes., Maintains a high level of identity consistency., Improves accuracy and realism of generated expressions.
- **Cons**: Requires a pre-trained facial expression classifier., Performance may vary based on input image quality.
- **Future Work**: Explore further integration of natural language processing for enhanced control., Investigate real-time generation capabilities., Develop methods to improve stability and reduce failure rates.

</details>

### [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](http://arxiv.org/pdf/2509.21887v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Lip synchronization in style-based generators

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity lip synchronization that can be generalized and personalized using style-based generators.

- **Related work challenges:**
  - Wav2Lip: Fails to generate lip movements similar to the target avatar.
  - DINet: Struggles with occlusion handling, leading to implausible lip geometries.
  - Diff2Lip: Inadequate preservation of idiosyncratic lip habits.
  - Previous visual dubbing methods: Dependence on specific person training and lack of generalization in occlusion scenarios.
  - Diffusion-based video generation methods: High computational costs that limit accessibility for researchers.
  - Generative adversarial networks (GANs): Focus on visual quality and lip sync without comprehensive generalization.
  - Previous methods for lip synchronization: Often rely on lip-reading experts or spatial loss in pixel space, which can introduce additional training overhead.
  - Existing video generation models: Fail to maintain continuity in generated videos, especially during static occlusions.
  - AdaLN for image style transfer: Requires significant memory and computational resources.
  - Wav2Lip: Limited performance in audio-lip synchronization.
  - DINET: High computational complexity due to dual U-Nets architecture.
  - SyncExpert: Conflicts with human perceptual judgments and complicates integration with latent-space-based models.
  - Wav2Lip: Limited generalization to challenging cases.
  - DINet: Inadequate facial textural details.
  - IP-LAP: Difficulty in aligning multiple reference images.
  - TalkLip: Insufficient intelligibility in generated lip regions.
  - Diff2Lip: Challenges in real-world applications.
  - Diff2Lip: Background preservation issues due to larger masks and single reference images.
  - GANs: Inferior inference speed compared to diffusion methods.
  - Resyncer: Rewiring style-based generator for unified audio-visually synced facial performer: Unified synchronization across different styles and performers.
  - Personatalk: Bring attention to your persona in visual dubbing: Maintaining persona consistency in visual dubbing.
  - Diffdub: Person-generic visual dubbing using inpainting renderer with diffusion auto-encoder: Achieving person-generic visual dubbing effectively.
  - V oxCeleb2: Deep Speaker Recognition: N/A
  - Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset: N/A
  - Lip reading sentences in the wild: N/A
  - Vfhq: A high-quality dataset and benchmark for video face super-resolution: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A
  - Gans trained by a two time-scale update rule converge to a local nash equilibrium: N/A
  - Learning audio-visual speech representation by masked multimodal cluster prediction: N/A
  - Towards accurate generative models of video: A new metric & challenges: N/A
  - Visualizing data using t-sne: N/A
  - Latent consistency models: Synthesizing high-resolution images with few-step inference: N/A

### 3. Core Idea
- The core idea is to develop a style-based generator that can produce high-fidelity lip sync outputs that are both generalized and personalized.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates audio input with a style-based generator to produce synchronized lip movements.
- **Architecture / Loss / Training**: The architecture employs a loss function that emphasizes both fidelity to the audio and the stylistic elements of the generated output.
- **Complexity / Resources**: The method requires significant computational resources for training due to the complexity of the style-based generator.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various datasets to evaluate the performance of the lip sync model, focusing on metrics such as synchronization accuracy and visual fidelity.
- **Baselines**: DINET, DINet, Diff2Lip, Diffusion-based video generation methods, Existing lip sync models, GAN-based methods, IP-LAP, N/A, Previous GAN-based approaches, Previous lip synchronization methods, Recent diffusion models, Standard video generation models, State-of-the-art visual dubbing methods, TalkLip, Traditional GAN-based approaches, Wav2Lip
- **Main Results**: The results demonstrate superior performance in lip synchronization accuracy and visual quality compared to baseline models.
- **Ablations**: Ablation studies indicate the importance of style conditioning in achieving high fidelity.
- **Limitations / Stress Tests**: Limitations include challenges in handling diverse accents and speech variations.

### 6. Takeaways
- **Pros**: Enhanced generalization on lip habit resemblance., Improved occlusion robustness., Superior training efficiency without the need for extra priors.
- **Cons**: Potential limitations in extreme occlusion scenarios., Dependence on the quality of input audio and video., Complexity in training the hybrid architecture.
- **Future Work**: Explore further optimizations for low-resource scenarios., Investigate additional applications in real-time video generation., Enhance the model's ability to handle more complex occlusions.

</details>

### [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](http://arxiv.org/pdf/2509.21859v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D-aware image synthesis

### 2. Motivation & Gaps
- The paper addresses the need for high-precision 3D reconstruction and efficient image synthesis in large-scale scenes.

- **Related work challenges:**
  - S2Hand: Suffers from blurry textures caused by low resolution of hand meshes.
  - AMVUR: Also suffers from blurry textures due to low resolution.
  - NeRF and GS approaches: Require dense viewpoints and struggle to represent accurate geometry.
  - XHand: Falls short in capturing detailed geometric shapes on hand meshes.
  - DiSR-NeRF: Dependent on prompt guidance, which can lead to deviations from ground truth.
  - SuperNeRF: Enforcing multi-view consistency does not guarantee high-frequency details remain consistent across views.
  - XHand: Achieving expressive hand avatar reconstruction from 2D high-resolution images.
  - LIIF: Maintaining 3D consistencies in generated images.
  - GIIF: Lack of guarantees for 3D consistencies in generated outputs.
  - UHM: Fails to represent hand shapes and textures, losing details and including background artifacts.
  - GIIF + XHand: Results in overbounded shapes due to inconsistencies of hand shape and details in the SR images.
  - NeRF-SR: Suffers from blurriness and overly synthetic appearances.
  - XHand [10]: Limited performance in capturing fine details and maintaining 3D view/pose consistency.
  - N/A: N/A
  - Supernerf: High-precision 3-d reconstruction for large-scale scenes: High precision in large-scale 3D reconstruction.
  - Texpainter: Generative mesh texturing with multi-view consistency: Maintaining multi-view consistency in generative texturing.
  - Residual dense network for image super-resolution: Improving image super-resolution techniques.

### 3. Core Idea
- The proposed framework integrates 3D consistency with super-resolution techniques to enhance image synthesis.

### 4. Method
- **Pipeline**: The framework utilizes a combination of neural networks to achieve 3D consistency and super-resolution.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances 3D consistency and image quality during training.
- **Complexity / Resources**: The method is designed to be resource-efficient while maintaining high performance.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on various datasets with metrics focusing on 3D consistency and image quality.
- **Baselines**: 3D hand reconstruction methods, Bicubic, DiSR-NeRF, LIIF, N/A, NeRF-SR, Residual dense network, SRGS, State-of-the-art image upsampling methods adapted to hand datasets, SuperNeRF, Supernerf, Texpainter, UHM, XHand, XHand [10]
- **Main Results**: The results demonstrate significant improvements in both 3D consistency and image quality compared to baseline methods.
- **Ablations**: Ablation studies indicate the contributions of different components of the framework.
- **Limitations / Stress Tests**: The limitations include potential challenges in handling extremely large-scale scenes.

### 6. Takeaways
- **Pros**: Achieves fine-detailed 3D reconstruction including wrinkles and nails., Maintains multi-view and pose consistency among upsampled hand images., Enables realistic, interactive VR/AR applications.
- **Cons**: Heavily reliant on the quality of low-resolution input images., Challenges in capturing high-fidelity details in dynamic articulated targets.
- **Future Work**: Explore further improvements in texture fidelity., Investigate applications in other domains beyond hand reconstruction., Develop methods to handle more complex hand poses and interactions.

</details>

## video understanding

### [Quasinormal spectra of higher dimensional regular black holes in theories with infinite curvature corrections](http://arxiv.org/pdf/2509.25141v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the quasinormal modes (QNMs) of regular black holes with higher-curvature corrections.

### 2. Motivation & Gaps
- The study focuses on the perturbations and quasinormal modes of black holes in theories with higher-curvature corrections, highlighting their quantum-gravity motivation and the property of regularity.

- **Related work challenges:**
  - Previous studies on quasinormal modes of black holes: Classical general relativity solutions suffer from singularities, which quantum gravity effects are expected to resolve.
  - Construction of regular black holes in higher-dimensional spacetimes: Understanding the influence of higher-curvature corrections on the quasinormal modes.
  - [20]: Existing models of black holes do not account for higher-curvature corrections.
  - [27]: Previous studies may not have systematically compared the quasinormal spectra across different regularization mechanisms.
  - [29]: The WKB method has limitations in handling complex potentials without higher-order corrections.
  - N/A: WKB accuracy deteriorates in the regime of large D, particularly for the monopole case.
  - N/A: Direct numerical methods can be cumbersome for quasinormal mode calculations.
  - [69â€“74]: The correspondence is broken down when the WKB approach does not reproduce the eikonal quasinormal modes.
  - [75]: Testing the correspondence for grey-body factors.
  - [91â€“103]: Extensive investigation of perturbations and QNMs in higher-curvature theories.
  - Numerous studies have analyzed the spectra of various models of regular black holes.: Distinguishing regular black holes from spacetimes with central singularities.
  - N/A: N/A

### 3. Core Idea
- The introduction of higher-curvature corrections leads to a universal qualitative modification of the spectrum, reducing both oscillation frequencies and damping rates of quasinormal modes compared to general relativity.

### 4. Method
- **Pipeline**: Combination of numerical and analytic methods.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method involves higher-order corrections to the WKB expansion and uses PadÃ© approximants for improved convergence.

### 5. Experiments
- **Datasets & Metrics**: Regular black hole models listed in Table I.
- **Baselines**: Classical general relativity models, Continued fraction method, General Relativity solutions, General relativity, General relativity counterparts, N/A, Previous black hole models, Previous studies on quasinormal modes, WKB method
- **Main Results**: The oscillation frequencies and damping rates decrease with higher-curvature corrections, consistent across all configurations considered.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The study acknowledges the limitations of the WKB method in handling complex potentials.

### 6. Takeaways
- **Pros**: Provides insights into the effects of higher-curvature corrections on black hole dynamics., Enhances understanding of gravitational-wave signals from regular black holes., Offers a framework for testing gravity in the strong-field regime.
- **Cons**: Higher overtones are difficult to resolve with current gravitational-wave detectors., The analysis relies on the existence of an infinite series of terms, which may not be justified in all theoretical frameworks., Potential complexities in the mathematical treatment of higher-dimensional spacetimes.
- **Future Work**: Further exploration of the implications of regular black holes in gravitational-wave astronomy., Investigate additional models of black holes with different curvature corrections., Develop more robust methods for detecting higher overtones in gravitational-wave signals.

</details>

### [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](http://arxiv.org/pdf/2509.25139v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Language Navigation (VLN)

### 2. Motivation & Gaps
- Despite the significant improvement in navigation performance achieved by our analogical reasoning descriptions, several limitations remain.

- **Related work challenges:**
  - MapGPT (Chen et al., 2024): Processes multiple images simultaneously but remains limited when handling highly similar images.
  - Previous methods (Zhou et al., 2024c; Pan et al., 2024): Rely on rigid thresholds to define nuanced spatial concepts.
  - NavGPT: Utilizes VLMs to convert visual images into textual descriptions but treats each image independently, disregarding contextual information.
  - DiscussNav: Similar to NavGPT, it fails to encode contextual and relational differences across observations.
  - MapGPT: Limited performance in navigation tasks due to inadequate scene and spatial understanding.
  - NavGPT: Struggles with generating detailed spatial relationships.
  - REVERIE: Insufficient integration of visual and textual information for navigation.
  - N/A: The quality of the generated descriptions heavily depends on the underlying language model, which may introduce biases or hallucinations that could impact decision-making.
  - N/A: The process of generating analogical descriptions adds an additional computational step, potentially increasing processing costs compared to direct image-based navigation.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The proposed method enhances LLM-based VLN agents' decision-making by generating structured prompts that incorporate spatial relationships, improving navigation performance.

### 4. Method
- **Pipeline**: Compute spatial relations and generate structured prompts for LLMs to analyze spatial relationships.
- **Architecture / Loss / Training**: Utilizes GPT-4o as the backbone for the LLM-based agent, with specific constraints on output generation.
- **Complexity / Resources**: The implementation is designed to ensure deterministic outputs with a maximum of 15 actions and 2000 tokens.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on R2R and REVERIE datasets using metrics like Navigation Error (NE), Success Rate (SR), and Success Rate Weighted Path Length (SPL).
- **Baselines**: DiscussNav, MapGPT, N/A, NavGPT, Ours, Raw images, Raw text
- **Main Results**: The proposed method shows a 4-6% improvement in SR and SPL compared to existing baselines.
- **Ablations**: Ablation studies demonstrate the incremental contributions of scene and spatial descriptions to navigation performance.
- **Limitations / Stress Tests**: Despite the significant improvement in navigation performance achieved by our analogical reasoning descriptions, several limitations remain.

### 6. Takeaways
- **Pros**: Enhanced navigation performance through analogical reasoning., Improved spatial understanding of visually similar scenes., Effective integration of textual descriptions with visual inputs.
- **Cons**: Potential limitations in handling extremely similar images., Dependence on the quality of generated textual descriptions.
- **Future Work**: Explore further enhancements in analogical reasoning techniques., Investigate the application of the method to other embodied AI tasks., Develop more robust models for distinguishing subtle spatial differences.

</details>

### [MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech](http://arxiv.org/pdf/2509.25131v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluation of TTS systems in long-form speech generation

### 2. Motivation & Gaps
- Long-TTS-Eval focuses on assessing TTS systemsâ€™ capabilities in long-form speech generation and complex case handling.

- **Related work challenges:**
  - CosyVoice2: Limited capability to process and understand extended audio sequences.
  - Higgs-Audio-v2: High latency in audio synthesis.
  - Qwen2.5-Omni: Degraded vocal timbre consistency over long durations.
  - Mini-Gemini: Limited audio understanding capabilities.
  - CosyVoice2: Struggles with long-form speech generation.
  - MOSS-TTSD: Maintaining timbre consistency over long sequences.
  - Previous speech generation models: Weak correlation between text and speech tokens as speech length increases.
  - Existing tokenization methods: Higher number of speech tokens compared to text tokens slows inference and harms streaming efficiency.
  - Qwen2.5-Omni (Xu et al., 2025): Limited success rate in long-form audio understanding.
  - Lyra (Zhong et al., 2024): Inadequate multimodal understanding across various input types.
  - Existing TTS benchmarks: Focus primarily on short clips, neglecting long-form performance.
  - MGM-Omni: Balancing quality and speed in speech synthesis while maintaining data efficiency.
  - N/A: N/A
  - Seed-TTS-Eval: Building an evaluation pipeline for TTS systems.
  - Qwen2.5-Omni: Providing complete and detailed responses in long audio summarization.
  - MOSS-TTSD-v0.5: Generating accurate long-form speech with appropriate pausing.

### 3. Core Idea
- To evaluate TTS systems' performance in generating long-form speech and handling complex cases using a comprehensive benchmark.

### 4. Method
- **Pipeline**: The evaluation pipeline segments generated audio into 28-second chunks, transcribes them, and computes error rates.
- **Architecture / Loss / Training**: Utilizes Whisper-large-v3 and Paraformer-zh for ASR.
- **Complexity / Resources**: Involves a diverse dataset of literature, news, and academic papers, totaling 694 samples.

### 5. Experiments
- **Datasets & Metrics**: The benchmark includes 341 Chinese samples and 353 English samples, with metrics based on WER and CER.
- **Baselines**: CosyVoice2, Higgs-Audio-v2, Leading open source Omni LLMs, Lyra-9B, MOSS-TTSD, MOSS-TTSD-v0.5, Mini-Gemini, Mini-Omni2, N/A, Qwen2-Audio, Qwen2.5-Omni, Whisper-large-v3
- **Main Results**: MGM-Omni outperforms competitors in long audio understanding and speech generation.
- **Ablations**: Evaluation of different text types and their impact on TTS performance.
- **Limitations / Stress Tests**: Challenges in handling expressions with multiple valid readings in ASR.

### 6. Takeaways
- **Pros**: Efficient, end-to-end paradigm for omnimodal understanding., Supports zero-shot voice cloning with stable timbre., High-quality, context-aware long-form speech generation.
- **Cons**: Still struggles with real-time cross-modal fidelity., Challenges in maintaining low latency., Limited support for certain audio modalities.
- **Future Work**: Further improvements in long-form audio understanding., Exploration of additional modalities for integration., Enhancements in real-time processing capabilities.

</details>
