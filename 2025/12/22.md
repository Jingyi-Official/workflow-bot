# Daily Paper Digest Â· 2025-12-22
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation](https://arxiv.org/pdf/2512.17717v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction of animatable head avatars

### 2. Motivation & Gaps
- Our work presents a paradigm shift for applications reliant on realistic digital humans.

### 3. Core Idea
- By streamlining avatar creation from minimal input, it democratizes high-quality character generation for VR, gaming and telehealth.

### 4. Method
- **Pipeline**: The model processes input images through a series of encoding, attention, and decoding layers to generate detailed 3D reconstructions.
- **Architecture / Loss / Training**: The architecture includes multiple self-attention and cross-attention layers, with specific hyperparameters optimized for performance.
- **Complexity / Resources**: The model is designed to operate efficiently, achieving real-time processing speeds of 45 FPS.

</details>

### [SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation](https://arxiv.org/pdf/2512.17331v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video-driven portrait animation

### 2. Motivation & Gaps
- Existing methods struggle with facial blurring artifacts and temporal coherence in video-driven portrait animation.

### 3. Core Idea
- A unified framework that integrates explicit warping and attention-guided refinement for high-fidelity portrait animation.

### 4. Method
- **Pipeline**: Utilizes a hybrid framework combining explicit warping and attention-guided correction.
- **Architecture / Loss / Training**: Employs a confidence-guided fusion network to adaptively fuse results from different components.
- **Complexity / Resources**: Trained for 150 epochs using the Adam optimizer with a learning rate of 2Ã—10^-4.

</details>

### [DGH: Dynamic Gaussian Hair](https://arxiv.org/pdf/2512.17094v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Hair motion prediction and rendering for avatars

### 2. Motivation & Gaps
- The framework aims to enhance the realism of hair dynamics in digital avatars by using a Gaussian-based approach, avoiding the complexities of mesh-based methods.

### 3. Core Idea
- The DGH framework operates entirely in the Gaussian domain, allowing for mesh-free deformation prediction and seamless integration into learning-based re-animation pipelines.

### 4. Method
- **Pipeline**: Pre-trained head GS avatar re-animated using head motion inputs, with dynamic Gaussian hair deformations predicted and merged with the head GS.
- **Architecture / Loss / Training**: A lightweight MLP combined with a differentiable rasterizer for high-fidelity novel view rendering.
- **Complexity / Resources**: Current version is not real-time for dense hair (100K-150K hair strands), but future optimizations are planned.

</details>

## video understanding

### [Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing](https://arxiv.org/pdf/2512.17909v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image-Text Pre-training

### 2. Motivation & Gaps
- The paper addresses the challenge of recognizing long-tail visual concepts in image-text pre-training.

### 3. Core Idea
- The core idea is to enhance the recognition of long-tail visual concepts through advanced image-text pre-training techniques.

### 4. Method
- **Pipeline**: The method involves a multi-step pipeline integrating various pre-training techniques.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions tailored for multimodal learning.
- **Complexity / Resources**: The approach requires significant computational resources for training on large datasets.

</details>

### [Dexterous World Models](https://arxiv.org/pdf/2512.17907v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Controllable Robot Video Generation

### 2. Motivation & Gaps
- The paper explores the generation of egocentric interaction videos conditioned on hand manipulation trajectories, aiming to enhance the realism and applicability of robotic video generation.

### 3. Core Idea
- The proposed method utilizes a pretrained text-guided video inpainting diffusion model to generate videos of object motions induced by hand manipulation, with the potential for extension to robot video generation.

### 4. Method
- **Pipeline**: The pipeline involves masking out the human body from egocentric views and rendering a robot arm based on the human hand pose.
- **Architecture / Loss / Training**: The model is trained using the AdamW optimizer with a learning rate of 1Ã—10^-4 and an effective batch size of 56.
- **Complexity / Resources**: Training approximately takes 10 days on 4 NVIDIA A100 GPUs.

</details>

### [Adversarial Robustness of Vision in Open Foundation Models](https://arxiv.org/pdf/2512.17902v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Safety Evaluation of Multimodal Large Language Models

### 2. Motivation & Gaps
- The paper addresses the need for a benchmark to evaluate the safety of multimodal large language models.

### 3. Core Idea
- To create a comprehensive benchmark that evaluates the safety of multimodal large language models across various tasks and scenarios.

### 4. Method
- **Pipeline**: The proposed method involves a series of evaluations using the benchmark on different multimodal models.
- **Architecture / Loss / Training**: Compared architectures (simple projection layer in LLaVA vs. cross-attention adapter in Llama 3.2 Vision) and training scales.
- **Complexity / Resources**: Conducted experiments within an industry practice and formalized threat model.

</details>

## model collapse

### [Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting](https://arxiv.org/pdf/2512.17908v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Depth Estimation

### 2. Motivation & Gaps
- The paper addresses the challenges in depth estimation by comparing different alignment methods in disparity and depth spaces.

### 3. Core Idea
- To provide a fair comparison of depth estimation methods, the paper proposes a least-squares disparity-and-depth alignment procedure.

### 4. Method
- **Pipeline**: Align to obtain absolute disparity, then minimize least-squares errors in the same space used by methods that output relative depth.
- **Architecture / Loss / Training**: Minimizes a combination of a photometric loss and a smoothness loss.
- **Complexity / Resources**: The method is efficient, using a smaller ViT-S backbone for optimization.

</details>
