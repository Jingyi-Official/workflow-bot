# Daily Paper Digest Â· 2025-12-20
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/pdf/2512.16893v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D facial animation

### 2. Motivation & Gaps
- Existing methods struggle with accurate expression transfer and 3D consistency.

### 3. Core Idea
- We investigate how to distill a 2D facial animation diffusion method into a 3D-consistent, efficient yet expressive instant avatar encoder from a single image.

### 4. Method
- **Pipeline**: We propose an animation representation that deforms both the Gaussian appearance and geometry based on the encoded motion basis vectors.
- **Architecture / Loss / Training**: The architecture employs a motion encoder and a discriminator for adversarial training, with a detailed loss function that includes L1 loss, perceptual loss, identity loss, and adversarial loss.
- **Complexity / Resources**: Requires 0.4GB for static model storage during inference.

</details>

### [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/pdf/2512.15711v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Neural rendering

### 2. Motivation & Gaps
- The paper addresses the challenge of rendering dynamic scenes from images using neural networks.

### 3. Core Idea
- The core idea is to learn dynamic renderable volumes from images to improve the quality and efficiency of neural rendering.

### 4. Method
- **Pipeline**: The method involves a pipeline that processes images to create dynamic volumes.
- **Architecture / Loss / Training**: Utilizes a neural network architecture with specific loss functions to optimize rendering quality.
- **Complexity / Resources**: Requires significant computational resources for training and rendering.

</details>

### [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/pdf/2512.15599v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Avatar Creation and Animation

### 2. Motivation & Gaps
- The paper addresses the challenge of creating high-quality avatars from limited input images, focusing on data efficiency during the fitting process.

### 3. Core Idea
- The proposed method improves avatar quality and identity preservation using fewer input frames by leveraging advanced fitting techniques and a flexible architecture.

### 4. Method
- **Pipeline**: The method involves data preparation, background removal, and head-centric coordinate simplification to enhance avatar fitting.
- **Architecture / Loss / Training**: Utilizes a ViT-based architecture with specific hyperparameters for cross-attention and expression sequence MLP.
- **Complexity / Resources**: Requires approximately 58k 3D Gaussians and specific input/output resolutions for training.

</details>

## video understanding

### [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/pdf/2512.16924v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Generation of semantic promptable world events

### 2. Motivation & Gaps
- The paper explores the capabilities of WorldCanvas in generating coherent and physically plausible world events based on given prompts.

### 3. Core Idea
- WorldCanvas generates world events that are temporally coherent and physically plausible based on input prompts.

### 4. Method
- **Pipeline**: Input prompts describing causes lead to the generation of subsequent events by the model.
- **Architecture / Loss / Training**: Utilizes Spatial-Aware Weighted Cross-Attention to improve alignment between text and trajectories.
- **Complexity / Resources**: The model requires significant computational resources for training and evaluation.

</details>

### [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/pdf/2512.16920v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Editing

### 2. Motivation & Gaps
- The paper addresses the need for high-quality synthetic datasets to improve instruction-based video editing.

### 3. Core Idea
- The core idea is to leverage a high-quality synthetic dataset to enhance the performance of instruction-based video editing models.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates synthetic dataset generation with video editing tasks.
- **Architecture / Loss / Training**: Utilizes a novel architecture that minimizes loss during training by focusing on instruction adherence.
- **Complexity / Resources**: The approach requires significant computational resources for dataset generation and model training.

</details>

### [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/pdf/2512.16918v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning and video comprehension

### 2. Motivation & Gaps
- The paper introduces the AdaTooler-V-300k dataset aimed at enhancing multimodal reasoning capabilities, particularly in video and image contexts.

### 3. Core Idea
- To develop a dataset that supports advanced multimodal reasoning tasks by providing a diverse set of video and image-based challenges.

### 4. Method
- **Pipeline**: The method involves training models on the AdaTooler-V-300k dataset using a structured approach to multimodal reasoning.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions tailored for multimodal inputs to enhance learning efficiency.
- **Complexity / Resources**: The training process requires substantial computational resources due to the dataset's size and complexity.

</details>

## model collapse

### [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/pdf/2512.16922v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Self-supervised visual pretraining

### 2. Motivation & Gaps
- This work revisits the core idea of causal next-token prediction in the embedding space of vision models, demonstrating that simple next embedding prediction is sufficient to learn transferable visual representations.

### 3. Core Idea
- NEPA learns to predict the next embedding in a self-supervised manner, allowing it to form global, semantically meaningful dependencies between patches.

### 4. Method
- **Pipeline**: NEPA uses a simple shared embedding layer and directly regresses the next embedding without negatives or a contrastive head.
- **Architecture / Loss / Training**: The architecture consists of a single embedding layer and an autoregressive transformer predictor.
- **Complexity / Resources**: The implementation is based on the ViT code in the Hugging Face ecosystem, utilizing the timm library for prototyping.

</details>
