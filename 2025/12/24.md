# Daily Paper Digest Â· 2025-12-24
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/pdf/2512.20615v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Avatar Generation

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity video avatars that can interact intelligently within their environments.

### 3. Core Idea
- ORCA is a framework for active intelligence that operates on imperfect substrates.

### 4. Method
- **Pipeline**: The method involves a hybrid pipeline combining real-world photography and AI-synthesized imagery to create a diverse benchmark.
- **Architecture / Loss / Training**: Utilizes a goal-first approach for synthetic data generation, focusing on high-level intentions and structured metadata.
- **Complexity / Resources**: The approach requires high-quality real-world images and advanced AI models for data generation and annotation.

</details>

### [ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars](https://arxiv.org/pdf/2512.19546v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- text-to-video generation

### 2. Motivation & Gaps
- Existing methods struggle with poor text-following and temporal misalignment in avatar generation.

### 3. Core Idea
- Introducing Phase-Aware Cross-Attention, Progressive Audio-Visual Alignment, and a two-stage training strategy to improve action control and synchronization.

### 4. Method
- **Pipeline**: The framework utilizes structured textual conditioning to achieve phase-level temporal precision.
- **Architecture / Loss / Training**: Two-stage training strategy that decouples audio-visual learning from action control injection.
- **Complexity / Resources**: Operates at 720p with 5B parameters, achieving performance comparable to 14B models.

</details>

### [FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation](https://arxiv.org/pdf/2512.17717v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction of animatable head avatars

### 2. Motivation & Gaps
- Our work presents a paradigm shift for applications reliant on realistic digital humans.

### 3. Core Idea
- Streamlining avatar creation from minimal input to democratize high-quality character generation.

### 4. Method
- **Pipeline**: The model processes input images through a series of encoding, attention, and decoding layers to generate detailed head avatars.
- **Architecture / Loss / Training**: The architecture includes multiple self-attention and cross-attention layers, with specific hyperparameters for each component to optimize training.
- **Complexity / Resources**: The model is designed to operate efficiently, achieving real-time processing speeds of 45 FPS.

</details>

## video understanding

### [SemanticGen: Video Generation in Semantic Space](https://arxiv.org/pdf/2512.20619v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- SemanticGen aims to generate high-quality videos that adhere to text prompts while maintaining long-term consistency and alleviating drifting issues.

### 3. Core Idea
- The core idea of SemanticGen is to utilize compact semantic representations mapped into a VAE latent space for generating videos.

### 4. Method
- **Pipeline**: The pipeline involves using pre-trained video understanding tokenizers as semantic encoders to generate videos in the semantic space.
- **Architecture / Loss / Training**: The architecture employs a 3D VAE and transformer blocks with RMSNorm & Scale and self-attention mechanisms.
- **Complexity / Resources**: The method requires significant computational resources for training and inference due to the complexity of the model architecture.

</details>

### [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/pdf/2512.20618v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video understanding

### 2. Motivation & Gaps
- The paper addresses the challenges of efficiently understanding long videos using large language models.

### 3. Core Idea
- The core idea is to leverage large language models to enhance the efficiency of long video understanding.

### 4. Method
- **Pipeline**: The method involves a novel pipeline that integrates large language models with video processing techniques.
- **Architecture / Loss / Training**: The architecture is designed to minimize loss during training, focusing on long-range dependencies.
- **Complexity / Resources**: The method is optimized for lower computational complexity and resource usage.

</details>

### [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/pdf/2512.20617v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Navigation Action Sequence Generation

### 2. Motivation & Gaps
- The paper addresses the need for language models to incorporate 3D awareness, which is crucial for understanding spatial relationships and dynamics in various applications.

### 3. Core Idea
- Generate a sequence of actions to navigate a robot from a starting visual state to a target visual state based on provided visual information.

### 4. Method
- **Pipeline**: The method involves a hierarchical analysis of spatial capabilities and the instantiation of a benchmark for evaluation.
- **Architecture / Loss / Training**: Incorporates adaptive rewards based on the capability level, focusing on intuitive perception and complex reasoning.
- **Complexity / Resources**: The approach utilizes various datasets and evaluation metrics to assess model performance.

</details>

## model collapse

### [Dynamical Dark Energy models in light of the latest observations](https://arxiv.org/pdf/2512.20616v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Revisiting dark energy models and their implications based on recent data

### 2. Motivation & Gaps
- The study aims to explore the potential preference for evolving dark energy models over a constant cosmological term, based on modern cosmological data.

### 3. Core Idea
- This work investigates the dynamics of dark energy using modern cosmological data, focusing on models with interactions between dark matter and vacuum.

### 4. Method
- **Pipeline**: The analysis was performed using the standard Boltzmann code CLASS and focused on three standard sources of modern cosmological data: SNIa, BAO data, and CMB data.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Utilization of the standard Boltzmann code CLASS for analysis.

</details>
