# Daily Paper Digest Â· 2025-12-21
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/pdf/2512.16893v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D facial animation

### 2. Motivation & Gaps
- Existing methods struggle with accurate expression transfer and 3D consistency.

### 3. Core Idea
- We investigate how to distill a 2D facial animation diffusion method into a 3D-consistent, efficient yet expressive instant avatar encoder from a single image.

### 4. Method
- **Pipeline**: We propose an animation representation that deforms both the Gaussian appearance and geometry based on the encoded motion basis vectors.
- **Architecture / Loss / Training**: The architecture employs a motion decoder with a single-layer AdaLN and a multi-layer perceptron (MLP) for predicting delta feature vectors, with a complex loss function combining multiple objectives.
- **Complexity / Resources**: Requires only 0.4GB for static model storage during inference.

</details>

### [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/pdf/2512.15711v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Decode and render avatars

### 2. Motivation & Gaps
- The proposed hybrid approach leverages the efficiency of mesh and the expressivity of 3D Gaussian Splatting to model thin and complex regions like hair, which is crucial for avatars.

### 3. Core Idea
- A hybrid approach that combines mesh efficiency with the expressivity of 3D Gaussian Splatting to create high-quality avatars.

### 4. Method
- **Pipeline**: Unified pipeline for decoding and rendering avatars.
- **Architecture / Loss / Training**: Utilizes a combination of normal loss and Laplacian smoothness regularization for training.
- **Complexity / Resources**: Allows rendering on low-compute, low-latency devices like VR headsets.

</details>

### [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/pdf/2512.15599v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D head avatar generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic 3D head avatars with limited input data.

### 3. Core Idea
- FlexAvatar utilizes partial supervision to create realistic 3D head avatars, allowing for expressive animations and efficient data usage.

### 4. Method
- **Pipeline**: The method involves obtaining avatar codes from input portraits and performing interpolation between them.
- **Architecture / Loss / Training**: Utilizes a ViT architecture with specific hyperparameters for cross-attention layers and StyleGAN-PixelShuffle layers.
- **Complexity / Resources**: Requires only a fraction of the input frames compared to competitive baselines for achieving competitive performance.

</details>

## video understanding

### [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/pdf/2512.16924v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Generation of semantic promptable world events

### 2. Motivation & Gaps
- The paper explores the capabilities of WorldCanvas in generating coherent and physically plausible world events based on given prompts.

### 3. Core Idea
- WorldCanvas can generate temporally coherent and physically plausible events based on semantic prompts.

### 4. Method
- **Pipeline**: Input prompts describing causes lead to the generation of subsequent events.
- **Architecture / Loss / Training**: Utilizes Spatial-Aware Weighted Cross-Attention to enhance alignment between text and trajectories.
- **Complexity / Resources**: The model requires significant computational resources for training and evaluation.

</details>

### [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/pdf/2512.16920v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Editing

### 2. Motivation & Gaps
- The paper addresses the need for high-quality synthetic datasets to improve instruction-based video editing.

### 3. Core Idea
- The core idea is to leverage a high-quality synthetic dataset to enhance the performance of instruction-based video editing models.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates synthetic dataset generation with video editing tasks.
- **Architecture / Loss / Training**: Utilizes a novel architecture that minimizes loss during training by focusing on instruction adherence.
- **Complexity / Resources**: The approach requires significant computational resources for dataset generation and model training.

</details>

### [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/pdf/2512.16918v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning and video comprehension

### 2. Motivation & Gaps
- The paper introduces the AdaTooler-V-300k dataset, designed to enhance multimodal reasoning capabilities in AI models, particularly focusing on video and image comprehension.

### 3. Core Idea
- The core idea is to provide a diverse dataset that includes various reasoning tasks across different modalities, enabling better training and evaluation of multimodal models.

### 4. Method
- **Pipeline**: The dataset is structured to facilitate training and evaluation of models on multimodal reasoning tasks, including video and image-based questions.
- **Architecture / Loss / Training**: The architecture employs a reinforcement learning framework that incorporates a Tool Benefit Score to modulate rewards.
- **Complexity / Resources**: The dataset includes 300k samples across multiple categories, requiring significant computational resources for training.

</details>

## model collapse

### [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/pdf/2512.16922v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Self-supervised visual pretraining

### 2. Motivation & Gaps
- This work revisits the core idea of causal next-token prediction in the embedding space of vision models, demonstrating that simple next embedding prediction is sufficient to learn transferable visual representations.

### 3. Core Idea
- NEPA learns transferable visual representations by treating patch embeddings as prediction targets, avoiding brittle design choices tied to handcrafted pretext tasks.

### 4. Method
- **Pipeline**: NEPA uses a simple shared embedding layer and directly regresses the next embedding without negatives or a contrastive head.
- **Architecture / Loss / Training**: The architecture consists of a single embedding layer and an autoregressive transformer predictor.
- **Complexity / Resources**: The implementation is based on the ViT code in the Hugging Face ecosystem, utilizing Google Cloud Platform and Lambda Labs for computing resources.

</details>
