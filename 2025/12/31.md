# Daily Paper Digest Â· 2025-12-31
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/pdf/2512.23576v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the need for real-time interactive long video generation, which is a significant challenge in the field of video synthesis.

### 3. Core Idea
- The core idea is to develop a framework that allows for the generation of long videos interactively and in real-time, leveraging autoregressive models.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates audio inputs and user interactions to generate video outputs dynamically.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances quality and speed, optimizing for real-time performance.
- **Complexity / Resources**: The method is designed to be resource-efficient, allowing for deployment on standard hardware.

</details>

### [SoulX-LiveTalk Technical Report](https://arxiv.org/pdf/2512.23379v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Audio-driven avatar video generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating audio-driven avatars in real-time with infinite length.

### 3. Core Idea
- The proposed method leverages advanced diffusion techniques to generate high-quality, real-time audio-driven avatars that can sustain long durations.

### 4. Method
- **Pipeline**: The method involves a diffusion-based approach that integrates audio input to drive avatar animations.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions tailored for audio-visual synchronization and avatar realism.
- **Complexity / Resources**: The model is designed to be resource-efficient, allowing for real-time processing on standard hardware.

</details>

### [StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars](https://arxiv.org/pdf/2512.22065v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- talking avatar generation

### 2. Motivation & Gaps
- This work focuses on talking avatar generation for constructive, human-centered applications.

### 3. Core Idea
- The system supports real-time generation of talking avatars while ensuring ethical considerations are addressed.

### 4. Method
- **Pipeline**: The method involves a denoising process and variational autoencoder (VAE) for video generation.
- **Architecture / Loss / Training**: The architecture incorporates adversarial distillation and consistency-aware discriminators to enhance performance.
- **Complexity / Resources**: The model is evaluated using two H800 GPUs.

</details>

## video understanding

### [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/pdf/2512.23709v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Super-Resolution

### 2. Motivation & Gaps
- Existing video super-resolution methods struggle with temporal consistency and visual fidelity, particularly in challenging real-world sequences.

### 3. Core Idea
- Stream-DiffVSR effectively mitigates flickering artifacts and maintains stable texture reconstruction, demonstrating superior temporal coherence compared to existing VSR methods.

### 4. Method
- **Pipeline**: The method employs a diffusion-based approach to enhance video frames while ensuring temporal consistency.
- **Architecture / Loss / Training**: Utilizes a combination of perceptual loss and temporal coherence loss during training.
- **Complexity / Resources**: Requires significant GPU resources, particularly for high-resolution video processing.

</details>

### [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/pdf/2512.23705v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video depth and normal estimation for transparent and highly reflective scenarios

### 2. Motivation & Gaps
- The work addresses the challenging problem of estimating depth and normals in scenarios involving transparent and highly reflective objects, which is crucial for robotic perception.

### 3. Core Idea
- Introduction of DKT, a model finetuned from a video diffusion model using a LoRA training strategy, to improve depth and normal estimation in challenging scenarios.

### 4. Method
- **Pipeline**: Capture RGB images with a RealSense D435 camera, process with depth estimation models, rescale to metric depth, and generate grasp poses for robotic manipulation.
- **Architecture / Loss / Training**: LoRA fine-tuning strategy applied to scale model size and improve performance.
- **Complexity / Resources**: DKT-1.3B achieves high efficiency with an inference time of 167.48ms per frame and a peak GPU memory occupancy of 11.19 GB.

</details>

### [Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation](https://arxiv.org/pdf/2512.23703v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Progress Estimation and Robustness in Robotic Tasks

### 2. Motivation & Gaps
- The paper addresses the need for effective progress estimation in robotic tasks, particularly in the presence of disturbances.

### 3. Core Idea
- The General Reward Model (GRM) provides a robust framework for estimating progress in robotic tasks, even under disturbances.

### 4. Method
- **Pipeline**: The GRM processes visual feedback to adjust the robot's actions based on progress estimation.
- **Architecture / Loss / Training**: Utilizes a loss function that emphasizes accurate progress estimation.
- **Complexity / Resources**: Requires moderate computational resources for real-time inference.

</details>

## model collapse

### [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/pdf/2512.23707v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Define host transcriptomic programs through which Aeromonas sp. H1 promotes coriander growth

### 2. Motivation & Gaps
- The study aims to understand the molecular mechanisms by which Aeromonas sp. H1 enhances growth in coriander, focusing on the differentiation between early signaling and sustained growth responses.

### 3. Core Idea
- Integrate high-resolution transcriptomics with physiological validation to identify molecular mechanisms of probiotic-mediated growth promotion in coriander.

### 4. Method
- **Pipeline**: Isolate Aeromonas sp. H1, validate plant growth-promoting traits, and conduct RNA-seq on treated coriander seedlings.
- **Architecture / Loss / Training**: Use negative binomial regression and mixed effects models to analyze diagnostic delay and site variability.
- **Complexity / Resources**: Use of RNAlater for RNA stabilization, Illumina sequencing, and DESeq2 for differential expression analysis.

</details>
