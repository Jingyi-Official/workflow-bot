# Daily Paper Digest Â· 2025-12-11
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Adaptive Optimal Control for Avatar-Guided Motor Rehabilitation in Virtual Reality](https://arxiv.org/pdf/2512.09667v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Motor rehabilitation using adaptive control frameworks

### 2. Motivation & Gaps
- The control frameworkâ€™s primary strength is its inherent adaptability, allowing it to adjust assistance based on patient capabilities.

### 3. Core Idea
- The framework adapts the level of assistance and guidance to the patientâ€™s current motor capabilities and rehabilitation progress.

### 4. Method
- **Pipeline**: The method involves a parameter adaptation mechanism that updates the weights of the cost function across sessions.
- **Architecture / Loss / Training**: The control input is generated by minimizing a cost functional that synchronizes the avatar's motion with the patient's trajectory.
- **Complexity / Resources**: The system requires a VR platform and a mechanism for real-time adaptation of control parameters.

</details>

### [Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video](https://arxiv.org/pdf/2512.09335v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Modeling detailed human avatars for rendering novel poses/views and enabling relighting under arbitrary environmental light

### 2. Motivation & Gaps
- The paper addresses the challenge of modeling fine-grained geometric details of human avatars from monocular videos, particularly focusing on dynamic skinning weights and pose-variant transformations.

### 3. Core Idea
- The core idea is to learn pose-variant deformation for fine-grained geometric details of human avatars using dynamic skinning weights derived from input body motion.

### 4. Method
- **Pipeline**: The method involves capturing human motion videos under diverse lighting conditions and using a dynamic skinning weight for skeleton-driven deformation.
- **Architecture / Loss / Training**: A novel regularization is introduced to enhance geometric consistency.
- **Complexity / Resources**: The method balances performance and efficiency by setting the pose sequence length to 10.

</details>

### [UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/pdf/2512.09327v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Audio-driven 3D conversational avatar generation

### 2. Motivation & Gaps
- The paper addresses the need for realistic audio-driven avatars that can engage in natural conversations.

### 3. Core Idea
- The proposed UniLS model generates audio-driven 3D avatars that can listen and speak, utilizing a two-stage training process to enhance motion diversity and naturalness.

### 4. Method
- **Pipeline**: The method consists of a two-stage training process where the first stage generates motion without audio input, and the second stage refines this motion using audio guidance.
- **Architecture / Loss / Training**: The architecture employs a combination of loss functions to ensure realistic motion generation and synchronization with audio.
- **Complexity / Resources**: The model requires moderate computational resources for training and inference, leveraging existing frameworks for 3D animation.

</details>

## video understanding

### [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/pdf/2512.09924v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Editing Evaluation

### 2. Motivation & Gaps
- The paper addresses the limitations of existing video editing methods that fail to capture intended transformations or introduce inconsistencies due to a lack of world-model understanding.

### 3. Core Idea
- The proposed evaluation framework utilizes GPT-4o to score video editing examples based on Semantic Consistency and Perceptual Quality, providing a comprehensive assessment of edit accuracy and realism.

### 4. Method
- **Pipeline**: Utilizes GPT-4o for scoring diverse editing examples against proposed dimensions.
- **Architecture / Loss / Training**: The model uses a combination of flow-matching and reasoning-aware objectives, with a specific focus on cross-entropy loss for generative tasks.
- **Complexity / Resources**: Requires access to large foundational models for enhanced performance.

</details>

### [LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating](https://arxiv.org/pdf/2512.09920v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Social Navigation

### 2. Motivation & Gaps
- Filling a critical evaluation gap in socially aware navigation beyond path efficiency and collision avoidance.

### 3. Core Idea
- Introducing a VLM-driven fast-slow architecture that modulates costmaps and SFM controller parameters to translate semantic instructions and social norms into low-dimensional control objectives.

### 4. Method
- **Pipeline**: Fast-slow system architecture for action generation and navigation.
- **Architecture / Loss / Training**: The architecture employs a social force model for local planning and a dynamic social costmap for real-time navigation.
- **Complexity / Resources**: Utilizes ROS Noetic and Gazebo for simulation, with multiple models and tools for object segmentation and point prediction.

</details>

### [VisualActBench: Can VLMs See and Act like a Human?](https://arxiv.org/pdf/2512.09907v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Action Reasoning

### 2. Motivation & Gaps
- Current Vision-Language Models (VLMs) struggle with proactiveness and value-sensitive prioritization, limiting their applicability in safety-critical environments.

### 3. Core Idea
- Introduce VisualActBench, a benchmark to evaluate proactive reasoning capabilities of VLMs in a vision-only setting, challenging models to make initiative-driven decisions based on visual context.

### 4. Method
- **Pipeline**: Evaluation of 29 state-of-the-art VLMs using a novel Action Prioritization Level (APL) metric across diverse scenarios.
- **Architecture / Loss / Training**: Reinforcement learning techniques like Mixed Preference Optimization (MPO) are applied to enhance model performance.
- **Complexity / Resources**: Evaluation involves 3,733 annotated actions across four scenarios, requiring substantial computational resources for model training and evaluation.

</details>

## model collapse

### [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/pdf/2512.09929v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Gradient-Based Planning in World Models

### 2. Motivation & Gaps
- The paper addresses the challenges in planning using world models, particularly focusing on the train-test gap.

### 3. Core Idea
- To improve the performance of world models in gradient-based planning by analyzing the effects of perturbation radii and scaling factors.

### 4. Method
- **Pipeline**: Utilizes both Open-Loop and MPC (Closed-Loop) planning with varying perturbation settings.
- **Architecture / Loss / Training**: Employs the Adam optimizer with consistent parameters across experiments.
- **Complexity / Resources**: Requires additional computational resources for adversarial training but improves planning accuracy.

</details>

### [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/pdf/2512.09928v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-world manipulation tasks using a robotic system

### 2. Motivation & Gaps
- The paper addresses the need for robots to effectively predict future actions while simultaneously executing tasks, enhancing their decision-making capabilities in real-world scenarios.

### 3. Core Idea
- The HiF-VLA architecture combines foresight motion prediction with action prediction to enable robots to make informed decisions while executing tasks, achieving a 'think-while-acting' capability.

### 4. Method
- **Pipeline**: The method involves training a model that integrates foresight motion and action prediction streams to enhance decision-making in robotic tasks.
- **Architecture / Loss / Training**: The model is trained using a loss function that balances motion prediction and action prediction to ensure stable convergence.
- **Complexity / Resources**: The experiments are conducted on a single NVIDIA RTX 4090 GPU with data collected at 20 Hz.

</details>

### [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/pdf/2512.09927v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Vision-Language-Action (VLA) modeling

### 2. Motivation & Gaps
- Existing methods for VLA often require prior knowledge or temporal cues, which limits their applicability in real-time scenarios.

### 3. Core Idea
- TEAM-VLA introduces a density-expansion strategy to reconstruct object foreground regions and the robot arm from sparse similarity signals, enabling efficient token merging without prior knowledge.

### 4. Method
- **Pipeline**: The method involves identifying foreground tokens and performing action-guided merging to enhance inference speed and maintain semantic content.
- **Architecture / Loss / Training**: Utilizes a soft bipartite matching scheme to preserve semantic structure during token merging.
- **Complexity / Resources**: TEAM-VLA significantly reduces the number of tokens while maintaining performance, achieving state-of-the-art results among training-free approaches.

</details>
