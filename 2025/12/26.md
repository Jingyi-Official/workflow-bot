# Daily Paper Digest Â· 2025-12-26
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/pdf/2512.21099v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Reconstructing photorealistic 3D head avatars using 3DGS from multi-view images

### 2. Motivation & Gaps
- The research aims to advance virtual communication and immersive digital experiences through high-fidelity 3D head avatars.

### 3. Core Idea
- Incorporating perceptual VGG loss enhances the reconstruction of fine-grained details, such as facial hair, by encouraging high-frequency fidelity.

### 4. Method
- **Pipeline**: Utilizes a lightweight MLP and two decoders for geometry and appearance, processing input signals from FLAME parameters and EMOPortraits embeddings.
- **Architecture / Loss / Training**: Detailed architecture of TexAvatars with LeakyReLU (0.2) applied after every layer except the final output layer.
- **Complexity / Resources**: Employs a relatively shallow architecture to balance performance and computational efficiency.

</details>

### [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/pdf/2512.20858v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Interactive Lecture System Development

### 2. Motivation & Gaps
- ALIVE aims to enhance traditional recorded lectures with content-aware question answering and neural talking-head explanations.

### 3. Core Idea
- Integrating timestamp-aligned retrieval, locally hosted large language models, and avatar generation to provide interactive learning experiences.

### 4. Method
- **Pipeline**: Local deployment of AI components for interactive lecture delivery.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Avatar synthesis performance depends heavily on GPU resources.

</details>

### [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/pdf/2512.20615v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Avatar Generation

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity video avatars that can interact intelligently within their environments.

### 3. Core Idea
- ORCA is a framework for active intelligence that operates on imperfect substrates.

### 4. Method
- **Pipeline**: The method involves a hybrid pipeline combining real-world photography and AI-synthesized imagery to create a diverse benchmark for training.
- **Architecture / Loss / Training**: The architecture employs a goal-first approach for synthetic data generation, focusing on high-level intentions and structured metadata.
- **Complexity / Resources**: The method requires high-quality real-world images and advanced AI models for data generation and annotation.

</details>

## video understanding

### [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/pdf/2512.21338v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of maintaining visual fidelity in video generation while reducing computational load.

### 3. Core Idea
- HiStream employs an asymmetric strategy to enhance video generation by dedicating computation to the initial chunk, resulting in high-fidelity outputs with reduced computational requirements.

### 4. Method
- **Pipeline**: The method involves a four-step process focusing on the initial chunk to create a robust anchor cache.
- **Architecture / Loss / Training**: The final generator is updated using distribution matching gradients calculated from the divergence between real and fake diffusion models.
- **Complexity / Resources**: The approach effectively cuts computational steps in half for later chunks with minimal sacrifice in visual fidelity.

</details>

### [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/pdf/2512.21337v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Building age estimation

### 2. Motivation & Gaps
- This work offers deeper insights into the performance of models with varying capacities on the building age estimation task.

### 3. Core Idea
- Utilizing Large Language Models (LLMs) to assist in architectural age estimation through predefined reasoning prompts and dataset annotation.

### 4. Method
- **Pipeline**: LLMs were used for prompt generation, dataset annotation, and writing assistance.
- **Architecture / Loss / Training**: Utilizes a combination of Fine-grained Cross-modal Ranking-based Contrastive Loss (FCRC) and weighting of negative samples based on label distance.
- **Complexity / Resources**: The model complexity varies across CNN-based, Transformer-based, and CLIP-based methods, with performance evaluated using Mean Absolute Error (MAE).

</details>

### [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/pdf/2512.21336v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Bridging the divergence gap in generative models

### 2. Motivation & Gaps
- The algorithms E-BON and E-SMC aim to align the expected path entropy of the generated distribution with that of the reference distribution to reduce distributional divergence.

### 3. Core Idea
- The paper investigates the context-sensitivity property of state entropy in masked diffusion models, demonstrating that increasing context information properly decreases prediction entropy.

### 4. Method
- **Pipeline**: The method involves an evaluation step of the E-SMC algorithm, which includes defining rewards based on state entropy and using a Gibbs potential function for resampling probabilities.
- **Architecture / Loss / Training**: The algorithms utilize a unique temperature parameter to control entropy and ensure convergence to the reference distribution.
- **Complexity / Resources**: The complexity involves managing the resampling intervals and the temperature parameter to optimize the generative process.

</details>

## model collapse
