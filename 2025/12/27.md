# Daily Paper Digest Â· 2025-12-27
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/pdf/2512.21099v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar rendering and expression reenactment

### 2. Motivation & Gaps
- The paper addresses the need for realistic and expressive 3D avatars that can accurately represent facial expressions in real-time.

### 3. Core Idea
- Incorporating perceptual VGG loss enhances the reconstruction of fine-grained details, such as facial hair, by encouraging high-frequency fidelity.

### 4. Method
- **Pipeline**: The pipeline includes a custom UV layout for facial structures, a lightweight MLP for expression encoding, and two decoders for geometry and appearance.
- **Architecture / Loss / Training**: Detailed architecture of TexAvatars with LeakyReLU (0.2) applied after every layer except the final output layer.
- **Complexity / Resources**: The model is designed to be computationally efficient, allowing for real-time rendering while maintaining high fidelity.

</details>

### [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/pdf/2512.20858v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Interactive Lecture System Development

### 2. Motivation & Gaps
- ALIVE integrates content-aware interaction, timestamp-sensitive retrieval, and real-time avatar-delivered question answering within a single unified system.

### 3. Core Idea
- ALIVE enhances traditional recorded lectures with content-aware question answering and neural talking-head explanations.

### 4. Method
- **Pipeline**: Timestamp-aligned retrieval, locally hosted large language models, Whisper-based automatic speech recognition, offline text-to-speech synthesis, and SadTalker-based avatar generation.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Avatar synthesis performance depends heavily on GPU resources.

</details>

### [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/pdf/2512.20615v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Avatar Generation

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity video avatars that can interact intelligently within their environments.

### 3. Core Idea
- ORCA is a framework for active intelligence that operates on imperfect substrates.

### 4. Method
- **Pipeline**: The method involves a hybrid pipeline combining real-world photography and AI-synthesized imagery to create a diverse benchmark.
- **Architecture / Loss / Training**: ORCA is training-free and leverages pre-trained models for video generation.
- **Complexity / Resources**: The approach utilizes high-quality real-world images and AI-generated data, leveraging models like Gemini-2.5-Pro for annotation.

</details>

## video understanding

### [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/pdf/2512.21338v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of maintaining visual fidelity in video generation while reducing computational load.

### 3. Core Idea
- HiStream employs an asymmetric strategy to enhance video generation by dedicating computation to the initial chunk, resulting in high-fidelity outputs with reduced computational requirements.

### 4. Method
- **Pipeline**: The method involves a four-step process focusing on the initial chunk to create a robust anchor cache.
- **Architecture / Loss / Training**: The final generator is updated using distribution matching gradients derived from the divergence between real and fake diffusion models.
- **Complexity / Resources**: The approach effectively cuts computational steps in half for later chunks with minimal sacrifice in visual fidelity.

</details>

### [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/pdf/2512.21337v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Building age estimation

### 2. Motivation & Gaps
- This work offers deeper insights into the performance of models with varying capacities on the building age estimation task.

### 3. Core Idea
- Utilizing Large Language Models (LLMs) to assist in architectural age estimation through predefined reasoning prompts and dataset annotation.

### 4. Method
- **Pipeline**: LLMs were used for prompt generation, dataset annotation, and writing assistance.
- **Architecture / Loss / Training**: Utilizes a combination of Fine-grained Cross-modal Ranking-based Contrastive Loss (FCRC) and weighting of negative samples based on label distance.
- **Complexity / Resources**: The model complexity varies across CNN-based, Transformer-based, and CLIP-based architectures, with performance evaluated on the YearGuessr dataset.

</details>

### [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/pdf/2512.21336v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Bridging the divergence gap in generative models

### 2. Motivation & Gaps
- The algorithms E-BON and E-SMC aim to align the expected path entropy of the generated distribution with that of the reference distribution to reduce distributional divergence.

### 3. Core Idea
- The paper investigates the context-sensitivity property of state entropy in masked diffusion models, demonstrating that increased context information reduces prediction entropy.

### 4. Method
- **Pipeline**: The method involves a reward definition based on state entropy, potential function computation, and resampling probabilities for particle selection.
- **Architecture / Loss / Training**: The algorithms utilize a unique temperature parameter to control entropy and ensure convergence to the reference distribution.
- **Complexity / Resources**: The complexity involves managing the resampling intervals and the temperature parameter, which requires careful tuning.

</details>

## model collapse
