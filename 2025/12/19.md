# Daily Paper Digest Â· 2025-12-19
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/pdf/2512.16893v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D facial animation

### 2. Motivation & Gaps
- Existing methods struggle with accurate expression transfer and 3D consistency.

### 3. Core Idea
- We investigate how to distill a 2D facial animation diffusion method into a 3D-consistent, efficient yet expressive instant avatar encoder from a single image.

### 4. Method
- **Pipeline**: We propose an animation representation that deforms both the Gaussian appearance and geometry based on the encoded motion basis vectors.
- **Architecture / Loss / Training**: Utilizes a synthetic dataset for training.
- **Complexity / Resources**: Requires 0.4GB for static model storage and achieves 107.31 FPS.

</details>

### [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/pdf/2512.15711v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Neural rendering

### 2. Motivation & Gaps
- The paper addresses the challenge of rendering dynamic scenes from images using neural networks.

### 3. Core Idea
- The core idea is to learn dynamic renderable volumes from images to improve the quality and efficiency of neural rendering.

### 4. Method
- **Pipeline**: The method involves a pipeline that processes images to create dynamic volumes.
- **Architecture / Loss / Training**: Utilizes a neural network architecture with specific loss functions for training.
- **Complexity / Resources**: Requires significant computational resources for training and rendering.

</details>

### [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/pdf/2512.15599v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D head avatar generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic 3D head avatars with limited input data.

### 3. Core Idea
- FlexAvatar utilizes partial supervision to create realistic 3D head avatars, allowing for expressive animations and efficient data usage.

### 4. Method
- **Pipeline**: The method involves obtaining avatar codes from input images and performing interpolation between them.
- **Architecture / Loss / Training**: Utilizes a ViT architecture with cross-attention layers and StyleGAN-PixelShuffle layers.
- **Complexity / Resources**: Requires only a fraction of the input frames compared to competitive methods.

</details>

## video understanding

### [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/pdf/2512.16924v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Generation of semantic promptable world events

### 2. Motivation & Gaps
- The paper explores the capabilities of WorldCanvas in generating coherent and physically plausible world events based on given prompts.

### 3. Core Idea
- WorldCanvas generates temporally coherent and physically plausible events based on semantic prompts.

### 4. Method
- **Pipeline**: Input prompts describing causes lead to the generation of subsequent events.
- **Architecture / Loss / Training**: Utilizes Spatial-Aware Weighted Cross-Attention to enhance alignment between text and trajectories.
- **Complexity / Resources**: The model requires significant computational resources for training and evaluation.

</details>

### [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/pdf/2512.16920v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Editing

### 2. Motivation & Gaps
- The paper addresses the need for high-quality synthetic datasets to improve instruction-based video editing.

### 3. Core Idea
- The core idea is to leverage a high-quality synthetic dataset to enhance the performance of instruction-based video editing models.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates synthetic dataset generation with video editing tasks.
- **Architecture / Loss / Training**: Utilizes advanced neural network architectures with specific loss functions tailored for video editing.
- **Complexity / Resources**: The method requires significant computational resources for training and inference.

</details>

### [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/pdf/2512.16918v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning and video comprehension

### 2. Motivation & Gaps
- The paper introduces the AdaTooler-V-300k dataset aimed at enhancing multimodal reasoning capabilities in AI models, particularly focusing on video and image comprehension.

### 3. Core Idea
- The core idea is to provide a diverse dataset that facilitates the training of models capable of reasoning across various modalities, including video and images, while addressing existing limitations in multimodal AI.

### 4. Method
- **Pipeline**: The method involves collecting a wide range of video and image data, categorizing it into specific reasoning tasks, and utilizing it to train multimodal models.
- **Architecture / Loss / Training**: The architecture is designed to optimize for both visual and textual reasoning tasks, employing a loss function that balances performance across modalities.
- **Complexity / Resources**: The dataset is extensive, requiring significant computational resources for training and evaluation, particularly for large-scale models.

</details>

## model collapse

### [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/pdf/2512.16922v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Classification

### 2. Motivation & Gaps
- The paper addresses the limitations of existing models in understanding complex spatial layouts and reasoning-intensive tasks.

### 3. Core Idea
- The NEPA model aims to improve image classification by enhancing attention mechanisms and training dynamics.

### 4. Method
- **Pipeline**: Pre-training followed by fine-tuning with layer-wise learning rate decay.
- **Architecture / Loss / Training**: Utilizes AdamW optimizer with cosine decay learning rate schedule.
- **Complexity / Resources**: Requires significant computational resources for training with large batch sizes.

</details>
