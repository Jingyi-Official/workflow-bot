# Daily Paper Digest Â· 2025-12-25
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars](https://arxiv.org/pdf/2512.21099v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar rendering and expression reenactment

### 2. Motivation & Gaps
- The paper addresses the need for realistic and expressive 3D avatars that can accurately represent facial expressions in real-time.

### 3. Core Idea
- Incorporating perceptual VGG loss enhances the reconstruction of fine-grained details, such as facial hair, by encouraging high-frequency fidelity.

### 4. Method
- **Pipeline**: The pipeline includes a custom UV layout for facial structures, a lightweight MLP for expression encoding, and two decoders for geometry and appearance.
- **Architecture / Loss / Training**: Detailed architecture of TexAvatars with LeakyReLU (0.2) applied after every layer except the final output layer.
- **Complexity / Resources**: The model is designed for real-time performance while maintaining visual fidelity, using a compact expression code and efficient data handling.

</details>

### [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/pdf/2512.20858v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Interactive lecture system with content-aware question answering

### 2. Motivation & Gaps
- Existing systems for interactive learning and video question answering often operate in isolation and lack real-time, context-aware interaction.

### 3. Core Idea
- ALIVE integrates timestamp-aligned retrieval, locally hosted large language models, and avatar generation to provide interactive, context-aware learning experiences.

### 4. Method
- **Pipeline**: Local deployment of AI components for interactive lecture support.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Avatar synthesis performance depends heavily on GPU resources.

</details>

### [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/pdf/2512.20615v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Avatar Generation

### 2. Motivation & Gaps
- The paper addresses the need for high-fidelity video avatars that can interact intelligently within their environments.

### 3. Core Idea
- ORCA is a framework for active intelligence that aims to improve performance as VLM spatial reasoning and I2V controllability improve.

### 4. Method
- **Pipeline**: The method involves a hybrid pipeline combining real-world photography and AI-synthesized imagery to create a diverse benchmark.
- **Architecture / Loss / Training**: The architecture is designed to separate strategic reasoning from execution grounding, which is critical for reliable control.
- **Complexity / Resources**: The approach utilizes high-quality real-world images and AI-generated data, requiring significant computational resources for training.

</details>

## video understanding

### [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/pdf/2512.21338v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- Existing methods struggle with maintaining visual fidelity while reducing computational load.

### 3. Core Idea
- HiStream employs an asymmetric strategy to enhance video generation by focusing computation on the initial chunk, leading to high-fidelity results with reduced computational load.

### 4. Method
- **Pipeline**: The method involves a four-step process where the initial chunk is prioritized for high-quality output.
- **Architecture / Loss / Training**: The final generator is updated using distribution matching gradients derived from the divergence between real and fake diffusion models.
- **Complexity / Resources**: The approach effectively cuts computational steps in half for later chunks.

</details>

### [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/pdf/2512.21337v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Building age estimation

### 2. Motivation & Gaps
- This work offers deeper insights into the performance of models with varying capacities on the building age estimation task.

### 3. Core Idea
- Utilizing Large Language Models (LLMs) to assist in architectural age estimation through predefined reasoning prompts and dataset annotation.

### 4. Method
- **Pipeline**: LLMs were used for prompt generation, dataset annotation, and writing assistance.
- **Architecture / Loss / Training**: Utilizes a combination of Fine-grained Cross-modal Ranking-based Contrastive Loss (FCRC) and weighting of negative samples based on label distance.
- **Complexity / Resources**: The model complexity varies across CNN-based, Transformer-based, and CLIP-based architectures, impacting prediction accuracy.

</details>

### [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/pdf/2512.21336v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Bridging the divergence gap in generative models

### 2. Motivation & Gaps
- The algorithms E-BON and E-SMC aim to align the expected path entropy of the generated distribution with that of the reference distribution to reduce distributional divergence.

### 3. Core Idea
- The paper investigates the context-sensitivity property of state entropy in masked diffusion models, demonstrating that increasing context information properly decreases prediction entropy.

### 4. Method
- **Pipeline**: The method involves an evaluation step of the E-SMC algorithm, which includes defining rewards based on state entropy and using a Gibbs potential function for resampling probabilities.
- **Architecture / Loss / Training**: The algorithms utilize a unique temperature parameter to control entropy during the sampling process.
- **Complexity / Resources**: The complexity involves managing the resampling steps and ensuring the entropy remains low, which may require significant computational resources.

</details>

## model collapse
