# Daily Paper Digest Â· 2025-12-23
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars](https://arxiv.org/pdf/2512.19546v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- text-to-video generation

### 2. Motivation & Gaps
- Existing methods struggle with poor text-following and temporal misalignment in avatar generation.

### 3. Core Idea
- Introducing Phase-Aware Cross-Attention, Progressive Audio-Visual Alignment, and a two-stage training strategy to enhance avatar generation.

### 4. Method
- **Pipeline**: Two-stage training with phase-aware attention and audio-visual alignment.
- **Architecture / Loss / Training**: Utilizes a combination of action control and lip synchronization metrics.
- **Complexity / Resources**: Requires substantial computational resources for training and evaluation.

</details>

### [FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation](https://arxiv.org/pdf/2512.17717v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction of animatable head avatars

### 2. Motivation & Gaps
- The paper addresses the need for efficient and high-quality avatar creation from minimal input images, particularly in applications like VR, gaming, and telehealth.

### 3. Core Idea
- Our work presents a paradigm shift for applications reliant on realistic digital humans.

### 4. Method
- **Pipeline**: The model processes input images through a series of encoding, attention, and decoding layers to generate detailed head avatars.
- **Architecture / Loss / Training**: The architecture includes multiple self-attention and cross-attention layers, with specific hyperparameters optimized for performance.
- **Complexity / Resources**: The model is designed to operate efficiently, achieving 45 FPS, which is suitable for interactive applications.

</details>

### [SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation](https://arxiv.org/pdf/2512.17331v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video-driven portrait animation

### 2. Motivation & Gaps
- Existing methods struggle with facial blurring artifacts and temporal coherence in video-driven portrait animation.

### 3. Core Idea
- A unified framework that integrates explicit warping and attention-guided refinement to enhance video-driven portrait animation.

### 4. Method
- **Pipeline**: Utilizes a hybrid framework combining explicit warping and attention-guided correction.
- **Architecture / Loss / Training**: Employs a confidence-guided fusion network to adaptively fuse results from different components.
- **Complexity / Resources**: Trained on VFHQ and evaluated on HDTF datasets using Adam optimizer with specific hyperparameters.

</details>

## video understanding

### [Ionizing Photon Production Efficiencies and Chemical Abundances at Cosmic Dawn Revealed by Ultra-Deep Rest-Frame Optical Spectroscopy of JADES-GS-z14-0](https://arxiv.org/pdf/2512.19695v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the properties of the galaxy JADES-GS-z14-0 and its implications for early galaxy formation.

### 2. Motivation & Gaps
- The study aims to understand the early universe's galaxy formation and evolution by analyzing the rest-frame optical emission lines of JADES-GS-z14-0.

### 3. Core Idea
- JADES-GS-z14-0 is a prolific producer of ionizing photons, suggesting rapid metal enrichment and significant implications for understanding galaxy formation in the early universe.

### 4. Method
- **Pipeline**: Analysis of emission lines to infer gas-phase metallicity, ionization parameters, and electron density.
- **Architecture / Loss / Training**: Assumes a polynomial for the continuum and Gaussian profiles for emission lines, with different orders based on continuum detection.
- **Complexity / Resources**: Utilized JWST observations and high-performance computing resources.

</details>

### [Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning](https://arxiv.org/pdf/2512.19687v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Understanding

### 2. Motivation & Gaps
- The paper addresses the need for open-access data and models to enhance visual understanding.

### 3. Core Idea
- To provide a comprehensive framework for visual understanding through open-access resources.

### 4. Method
- **Pipeline**: The method involves collecting diverse datasets and training models on them.
- **Architecture / Loss / Training**: Utilizes advanced neural architectures with loss functions tailored for visual tasks.
- **Complexity / Resources**: Requires significant computational resources for training and evaluation.

</details>

### [Zero-shot Reconstruction of In-Scene Object Manipulation from Video](https://arxiv.org/pdf/2512.19684v1)
  (summary failed: 'utf-8' codec can't encode characters in position 7417-7420: surrogates not allowed)


## model collapse

### [The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding](https://arxiv.org/pdf/2512.19693v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in multi-scale autoregressive image generation, focusing on frequency ordering.

### 3. Core Idea
- The core idea is to utilize frequency ordering in a multi-scale autoregressive framework to enhance image generation.

### 4. Method
- **Pipeline**: The method involves a multi-scale autoregressive pipeline that incorporates frequency ordering.
- **Architecture / Loss / Training**: The architecture is designed to minimize frequency-related losses during training.
- **Complexity / Resources**: The method requires significant computational resources due to its multi-scale nature.

</details>

### [Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models](https://arxiv.org/pdf/2512.19692v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Human motion generation

### 2. Motivation & Gaps
- The study aims to improve human motion generation by evaluating the alignment of generated motions with textual descriptions and the quality of hand generation.

### 3. Core Idea
- The proposed method, Interact2Ar, utilizes a Mixed Memory approach to generate seamless transitions in human motion while accounting for the complete action history.

### 4. Method
- **Pipeline**: The method involves a user study for evaluation, followed by the implementation of a Mixed Memory approach for generating human interactions.
- **Architecture / Loss / Training**: The architecture includes a transformer encoder with specific configurations and employs multiple loss functions to ensure accurate motion representation.
- **Complexity / Resources**: The model was trained for 5000 epochs with a batch size of 128, utilizing a learning rate of 1Ã—10^-4 and weight decay of 2Ã—10^-5.

</details>
