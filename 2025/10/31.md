# Daily Paper Digest Â· 2025-10-31
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications](http://arxiv.org/pdf/2510.26251v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the impact of avatar characteristics on user ratings in healthcare applications

### 2. Motivation & Gaps
- The integration of virtual avatars in mental healthcare has potential benefits, but the effectiveness and design features remain underexplored.

### 3. Core Idea
- The study highlights the influence of gender stereotypes on the perception of virtual avatars in healthcare applications, emphasizing the importance of competence over warmth and attractiveness.

### 4. Method
- **Pipeline**: Participants interacted with six different avatars in a healthcare questionnaire application, rating their experiences.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

</details>

### [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](http://arxiv.org/pdf/2510.25234v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D facial animation

### 2. Motivation & Gaps
- The paper addresses the need for improved techniques in 3D facial animation that can effectively disentangle speech and emotional expressions.

### 3. Core Idea
- The core idea is to develop a method that can disentangle speech and emotional expressions to create more realistic and expressive 3D facial animations.

### 4. Method
- **Pipeline**: The method involves a pipeline that processes audio input to generate corresponding facial animations by separating speech and emotional components.
- **Architecture / Loss / Training**: Utilizes a neural network architecture with specific loss functions designed to optimize the disentanglement of speech and expressions.
- **Complexity / Resources**: The method requires significant computational resources for training and inference, including high-performance GPUs.

</details>

### [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](http://arxiv.org/pdf/2510.23929v1)
  (summary failed: 'utf-8' codec can't encode characters in position 7033-7034: surrogates not allowed)


## video understanding

### [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](http://arxiv.org/pdf/2510.26802v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video reasoning enhancement

### 2. Motivation & Gaps
- The paper addresses the need for improved reasoning capabilities in multi-modal large language models (MLLMs) specifically for video content.

### 3. Core Idea
- The core idea is to enhance video reasoning capabilities in MLLMs through reinforcement learning techniques.

### 4. Method
- **Pipeline**: The method involves a reinforcement learning framework that integrates video content with reasoning tasks.
- **Architecture / Loss / Training**: Utilizes a custom loss function tailored for video reasoning tasks during training.
- **Complexity / Resources**: The approach requires moderate computational resources, primarily for training the reinforcement learning model.

</details>

### [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](http://arxiv.org/pdf/2510.26796v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of synthesizing novel views of dynamic scenes using a monocular camera, focusing on achieving globally coherent depth estimation.

### 3. Core Idea
- The proposed method utilizes a monocular camera to synthesize novel views by leveraging globally coherent depth information.

### 4. Method
- **Pipeline**: The pipeline involves capturing monocular video input, estimating depth, and synthesizing novel views based on the depth information.
- **Architecture / Loss / Training**: The architecture employs a loss function that encourages depth coherence across frames during training.
- **Complexity / Resources**: The method requires moderate computational resources, primarily for depth estimation and view synthesis.

</details>

### [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](http://arxiv.org/pdf/2510.26794v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- text-to-motion generation

### 2. Motivation & Gaps
- The paper addresses the need for improved text-to-motion generation models that can effectively interpret complex prompts and generate high-quality motion sequences.

### 3. Core Idea
- Integrating a new text-to-motion network architecture into the Motion Latent Diffusion framework to enhance text-motion consistency and motion quality.

### 4. Method
- **Pipeline**: The proposed method integrates a full-transformer denoiser into the MLD framework, utilizing a pre-trained motion VAE for latent space operations.
- **Architecture / Loss / Training**: The model was trained using the AdamW optimizer with a cosine learning rate scheduler for 36000 iterations, retaining most original hyperparameters.
- **Complexity / Resources**: The implementation requires a larger batch size and adjusted learning rate for optimized hardware performance.

</details>

## model collapse

### [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](http://arxiv.org/pdf/2510.26800v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Guided Panorama Perception and Panorama Completion

### 2. Motivation & Gaps
- The paper addresses the challenges in generating accurate and coherent 3D scenes from panoramic images.

### 3. Core Idea
- OmniX generates accurate and locally coherent results for masked areas in panoramic images while ensuring consistency with RGB references.

### 4. Method
- **Pipeline**: Input masked images and corresponding masks to generate RGB outputs and other attributes like normal, albedo, roughness, and metallic.
- **Architecture / Loss / Training**: The architecture employs joint modeling of materials and geometry using cross-attention mechanisms to enhance performance.
- **Complexity / Resources**: The method requires training on three-channel RGB inputs and utilizes additional resources for material and geometry modeling.

</details>

### [Masked Diffusion Captioning for Visual Feature Learning](http://arxiv.org/pdf/2510.26799v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image captioning and visual-language pretraining

### 2. Motivation & Gaps
- The paper addresses the need for improved visual-language models that can generate more descriptive captions by leveraging location-aware features.

### 3. Core Idea
- The core idea is to enhance image captioning by incorporating location-aware features into the pretraining process of visual-language models.

### 4. Method
- **Pipeline**: The method involves a pretraining phase using location-aware captioners followed by fine-tuning on specific datasets.
- **Architecture / Loss / Training**: Utilizes a sigmoid loss function for training the models to improve caption generation.
- **Complexity / Resources**: The models are trained using ViT-B and ViT-L architectures with specific hyperparameters for optimization.

</details>
