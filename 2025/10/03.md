# Daily Paper Digest Â· 2025-10-03
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](http://arxiv.org/pdf/2510.02314v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-view Attack in 3D Generative Systems

### 2. Motivation & Gaps
- Existing methods struggle with embedding illusory objects while maintaining scene consistency and fidelity in multi-view scenarios.

- **Related work challenges:**
  - IPA-NeRF: Limited applicability to explicit 3D scene representations like 3DGS.
  - Poison-Splat: Focuses on computational cost attacks rather than visible illusion embedding.
  - IPA-NeRF: Pioneered poisoning attacks against NeRF by inserting crafted samples at specific viewing angles.
  - Poison-Splat: Targeted 3DGS efficiency by generating samples that increase memory consumption.
  - Geometry Cloak: Prevents unauthorized 3D reconstruction from copyrighted images.
  - 3D Gaussian Splatting (3DGS): Ensuring illusion visibility on the target view while minimizing perception on innocent views.
  - IPA-NeRF: Accelerated training and rendering
  - Naive 3DGS: Low performance in embedding illusory objects
  - IPA-Splat: Maintaining high fidelity in innocent views
  - IPA-NeRF: Ineffective in preserving quality across multiple viewpoints.
  - Density-Guided Point Cloud Poisoning: Requires optimal bandwidth for effective density estimation.
  - Noise Scheduling Strategies: Balancing initial noise strength and decay strategies for optimal attack effectiveness.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - IPA-NeRF (Nerfacto): Poor convergence on complex scenes
  - IPA-NeRF (Instant-NGP): Produces heavily distorted illusory objects
  - IPA-Splat: Limited effectiveness in high view overlap environments

### 3. Core Idea
- Our method demonstrates superior robustness and computational efficiency in generating illusory objects for backdoor attacks.

### 4. Method
- **Pipeline**: Evaluate density-guided attacks across multiple viewpoints while preserving innocent view quality.
- **Architecture / Loss / Training**: Utilizes KDE bandwidth and noise scheduling to optimize attack performance.
- **Complexity / Resources**: Reduces GPU memory usage by 41% and Gaussian points by 88%.

### 5. Experiments
- **Datasets & Metrics**: COCO 2017 dataset for illusory objects; evaluated on Mip-NeRF 360 dataset.
- **Baselines**: 3D Gaussian Splatting, IPA-NeRF, IPA-NeRF (Instant-NGP), IPA-NeRF (Nerfacto), IPA-NeRF with Instant-NGP, IPA-NeRF with Nerfacto, IPA-Splat, N/A, Naive 3DGS, NeRF, Poison-Splat
- **Main Results**: Our method achieves success rates ranging from 64% to 83% across different threshold combinations, significantly outperforming existing approaches.
- **Ablations**: Combining direct replacement, density-guided poisoning, and multi-view consistency disruption achieves superior illusion embedding.
- **Limitations / Stress Tests**: Struggles with scenes having highly overlapping views or complex camera trajectories.

### 6. Takeaways
- **Pros**: First work addressing data poisoning attacks on 3D Gaussian Splatting., Identifies and analyzes the robustness of 3DGS against prior poisoning techniques., Proposes a novel density-guided poisoning method with adaptive noise scheduling.
- **Cons**: Requires careful optimization and understanding of the victim model., Potential for high computational resource usage., Limited to specific types of attacks and scenarios.
- **Future Work**: Explore further enhancements in poisoning attack strategies., Investigate the application of the method in other 3D representation techniques., Develop more robust defenses against such poisoning attacks.

</details>

### [Interactive Training: Feedback-Driven Neural Network Optimization](http://arxiv.org/pdf/2510.02297v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Neural network training optimization

### 2. Motivation & Gaps
- The paper presents a framework that allows for real-time interventions in neural network training, enabling both humans and AI agents to adjust training parameters dynamically.

- **Related work challenges:**
  - Bergstra and Bengio, 2012: Static training paradigms do not allow for real-time adjustments.
  - Takase et al., 2023: Unstable loss dynamics and underperformance on specific tasks require human intervention.
  - Zhang et al., 2022: Prematurely terminating training jobs leads to inefficiencies and wasted computational resources.
  - Traditional static training methods: Lack of adaptability to changing training dynamics.
  - Existing training frameworks: Limited interactivity and real-time feedback mechanisms.
  - Human-in-the-Loop Machine Learning: Existing methods often rely on predefined schedules or specific forms of input rather than real-time intervention.
  - Automated ML and Adaptive Optimization: Traditional methods for hyperparameter tuning do not incorporate real-time feedback from training dynamics.
  - Population-Based Training (PBT): PBT learns an automatic dynamic schedule of hyperparameters but does not allow real-time human intervention.
  - AutoML methods: Traditional AutoML methods automate hyperparameter tuning but treat training as a black-box process.
  - AI agents in training: Current AI agents serve as advisory tools and do not directly modify training processes.
  - Transformers: State-of-the-art natural language processing: N/A
  - Learning an adaptive learning rate schedule: N/A
  - Opt: Open pre-trained transformer language models: N/A

### 3. Core Idea
- Interactive Training reimagines neural network training as an interactive, feedback-driven process, allowing for dynamic control of training strategies.

### 4. Method
- **Pipeline**: The framework integrates real-time monitoring and intervention capabilities for both human and AI agents during training.
- **Architecture / Loss / Training**: The architecture includes a learning rate scheduler that can be adjusted by human experts or an AI agent to stabilize training.
- **Complexity / Resources**: The method requires a setup that includes a dashboard for monitoring training dynamics and an AI agent capable of processing training logs.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilized a large synthetic dataset and real user interaction data collected from the NeuralOS model.
- **Baselines**: Fixed learning rate schedule, Interactive training with human intervention, N/A, Static baseline with fixed learning rate, Traditional static optimization methods, Traditional static training paradigms
- **Main Results**: Interactive Training shows improved accuracy, reduced sensitivity to initial hyperparameters, and real-time adaptation to evolving application needs.
- **Ablations**: The study included comparisons of different learning rate schedules and the impact of human versus AI interventions.
- **Limitations / Stress Tests**: The paper discusses limitations related to reproducibility and the expertise required for effective interventions.

### 6. Takeaways
- **Pros**: Enables real-time adjustments to training parameters., Improves training stability and adaptability., Facilitates both human and AI-driven interventions.
- **Cons**: Requires a control server setup., May introduce complexity in training workflows.
- **Future Work**: Further development of automated AI agents for training., Exploration of fully interactive training paradigms., Integration of dynamic loss function adjustments.

</details>

### [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](http://arxiv.org/pdf/2510.02278v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Traffic Forecasting

### 2. Motivation & Gaps
- The proposed datasets contain important road attributes that strongly affect traffic speed and volume, which are necessary for precise traffic forecasting.

- **Related work challenges:**
  - Existing traffic forecasting benchmarks: Limited number of locations with available measurements and lack of graph structure based on road connectivity.
  - Current datasets: Fail to capture complex urban traffic due to sensors being mostly located on intercity highways.
  - Attention-based Spatial-Temporal Graph Convolutional Networks (ASTGCN): Complex implementation and computational overhead.
  - Graph WaveNet (GWN): Difficulty in learning latent spatial structure and long-range temporal dependencies.
  - Adaptive Graph Convolutional Recurrent Network (AGCRN): Decoupling model performance from predefined graph structures.
  - Previous Traffic Forecasting Datasets: Heuristic edge construction based on travel distance rather than actual road connectivity.
  - Existing Neural Spatiotemporal Models: Inability to handle large and complex datasets effectively.
  - Torch Spatiotemporal (Cini & Marisca, 2022): Limited models can be trained on large datasets due to high resource demands.
  - LargeST (Liu et al., 2023): Existing models require long training times and are not scalable.
  - DCRNN: Exhibits significantly longer training times and fails to complete within a 250 hours time limit for larger lookback windows.
  - STGCN: Scalability issues complicate its application in real-world systems.
  - GWN: Long training times as lookback window increases.
  - Graph neural network for traffic forecasting: A survey: Lack of comprehensive evaluation metrics for GNNs in traffic forecasting.
  - Dynamic spatial-temporal aware graph neural network for traffic flow forecasting: Difficulty in capturing dynamic changes in traffic patterns.
  - Diffusion convolutional recurrent neural network: Data-driven traffic forecasting: Challenges in integrating temporal dependencies effectively.
  - city-traffic-M: Higher density and branching structure compared to city-traffic-L.
  - city-traffic-L: Scalability issues due to larger size and complex urban structure.
  - Previous datasets: Do not include important road attributes affecting traffic forecasting.

### 3. Core Idea
- Utilizing learnable node embeddings and additional temporal features for improved traffic forecasting.

### 4. Method
- **Pipeline**: Use of learnable node embeddings for road segments along with static and temporal features.
- **Architecture / Loss / Training**: Trained using the AdamW optimizer with a fixed learning rate of 0.0003 for 5 epochs.
- **Complexity / Resources**: Experiments conducted on a single NVIDIA A100 GPU with 80GB of VRAM.

### 5. Experiments
- **Datasets & Metrics**: Datasets include various road attributes and traffic metrics.
- **Baselines**: Attention-based Spatial-Temporal Graph Convolutional Networks (ASTGCN), DCRNN, Diffusion Convolutional Recurrent Neural Network (DCRNN), GNN-Mean, GNN-TrfAttn, GRUGCN, GWN, Global mean/median, Graph WaveNet (GWN), METR-LA, N/A, Node-wise mean/median, Other deep learning models like LSTMs and CNNs, PEMS-BAY, Previous 1 day/week ago, Previous strategy, STGCN, Spatiotemporal Graph Convolutional Network (STGCN), Traditional time series forecasting methods, linear model, naive baselines
- **Main Results**: Traffic volume and speed vary significantly based on road features such as speed limits and conditions.
- **Ablations**: Ablation studies indicate the importance of spatial-temporal features in improving forecasting accuracy.
- **Limitations / Stress Tests**: Models exceeding memory limits reported OOM.

### 6. Takeaways
- **Pros**: Datasets provide rich road features and fine-grained data about traffic volume and speed., Proposed approach achieves better scalability., Demonstrates stronger forecasting performance.
- **Cons**: Existing models struggle to scale to larger datasets., Current datasets do not capture complex urban traffic patterns., Limited availability of realistic benchmarks.
- **Future Work**: Encourage further advancements in traffic forecasting., Support progress in urban computing and smart city development., Explore additional modeling insights for improved forecasting.

</details>

## Gaussian Splatting

### [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](http://arxiv.org/pdf/2510.02308v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Tangent space estimation and embedding of noisy data

### 2. Motivation & Gaps
- The paper addresses the challenges in estimating tangent spaces and embeddings of noisy data, particularly focusing on the limitations of existing methods like LPCA.

- **Related work challenges:**
  - Local Principal Component Analysis (LPCA): Struggles with noise and requires prior knowledge of geometric and noise characteristics.
  - Adaptive neighborhood size selection: Hindered by unknown geometric quantities, making it a challenging problem.
  - LPCA: LPCA is not robust to noise, resulting in poor tangent space estimates.
  - Previous methods for tangent space estimation: Struggled with noise and spurious edges in nearest neighbor graphs.
  - Graph Laplacian approaches: Convergence issues when applied to noisy data.
  - Eigenvector gradient methods: High gradients in noise directions affecting accuracy.
  - Previous methods for tangent space estimation: Often fail to account for the complexities introduced by varying geometries and eigenfunction behaviors.
  - Davis-Kahan theorem: Ensuring that low-frequency eigengaps do not vanish too quickly to maintain robustness against noise.
  - LPCA: Highly sensitive to noise, leading to inaccurate tangent space estimates.
  - LEGO: Requires robust performance across varying noise levels and hyperparameters.
  - LPCA: Sensitivity to noise and inability to accurately recover local intrinsic geometry.
  - LEGO: Requires effective noise handling to produce interpretable embeddings.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The introduction of a tear-enabled alignment framework to produce injective embeddings of data lying on closed manifolds.

### 4. Method
- **Pipeline**: Data dimensionality reduction followed by tangent space estimation using LPCA and LEGO, leading to a torn 2D embedding.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Computational constraints necessitated dimensionality reduction before tangent space estimation.

### 5. Experiments
- **Datasets & Metrics**: Noisy datasets generated by adding uniformly distributed noise to clean images, evaluated using explained variance ratios.
- **Baselines**: LEGO, LPCA, Local PCA (LPCA), Local Principal Component Analysis (LPCA), N/A, Other tangent space estimation methods, Previous eigenfunction-based approaches, Principal Component Analysis (PCA), Standard tangent space estimation methods
- **Main Results**: LEGO effectively captures the underlying 2D structure, concentrating functional variance in the first two directions, while LPCA distributes variance across multiple dimensions.
- **Ablations**: Noise ablation studies confirmed that LPCA estimates degrade rapidly with noise, while LEGO remains stable.
- **Limitations / Stress Tests**: The method's performance may degrade in cases of very high curvature or when the eigenvalues are not well-separated.

### 6. Takeaways
- **Pros**: LEGO provides more robust tangent space estimates in noisy environments., Utilizes global structure of data for improved local estimation., Theoretical justifications support the effectiveness of the method.
- **Cons**: Requires understanding of graph Laplacian eigenvectors.
- **Future Work**: Explore further applications in manifold learning and boundary detection., Investigate adaptive methods for neighborhood size selection., Enhance the algorithm for even higher dimensional data.

</details>

### [ALMA Deep Field in SSA22: Reconstructed [CII] Luminosity Function at z = 6](http://arxiv.org/pdf/2510.02303v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Examine completeness and contamination for ADF22 data and discuss non-detection explanations.

### 2. Motivation & Gaps
- The study aims to understand the detection of [Cii] line emitters in high-redshift galaxies and the challenges faced in confirming these detections.

- **Related work challenges:**
  - Previous studies on high-z line-emitting sources: False detections due to spurious sources and underestimation of contamination rates.
  - Lyman-break method for high-redshift galaxies: Biases due to dust obscuration and stellar mass.
  - Hollenbach and McKee 1989: Understanding the enhancement of [Cii] line emission in star-forming regions.
  - Matsuda et al. 2015: Constraining the [Cii] LF with blind and serendipitous detections.
  - Hayatsu et al. 2017: Searching for faint emission-line sources in ALMA data.
  - N/A: N/A
  - Hayatsu et al. (2017): Detection reliability and contamination rate evaluation.
  - Carilli and Walter (2013): Conversion of flux to luminosity.
  - Williams, de Geus, and Blitz (1994): Clump finding in data.
  - Williams, de Geus, and Blitz 1994: Detection reliability and contamination in source identification
  - Hatsukade et al. 2016: Approximating detection functions with error functions
  - Kohandel et al. 2019: Correlation between [Cii] luminosity and FWHM
  - DÃ­az-Santos et al. 2013: N/A
  - Hemmati et al. 2017: Estimation correction to effective volume and sample number
  - Swinbank et al. 2012: N/A
  - Hayatsu et al. 2019: N/A
  - Capak et al. 2015: N/A
  - Aravena et al. 2016: Assumption of contamination rate affecting real [Cii] emitters
  - N/A: N/A
  - Hemmati et al. (2017): N/A
  - De Looze et al. (2014): N/A
  - Farra et al. (2013): N/A
  - N/A: False detection of high-SN ratio is unavoidable due to existing clump-like structures.
  - N/A: N/A

### 3. Core Idea
- The reconstruction method for line LF can be applied to other blind line surveys using ALMA deep field datacubes.

### 4. Method
- **Pipeline**: Detection of sources using clumpfind and correction for contamination and completeness.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Utilized 100 blank mock-mosaic datasets for analysis.

### 5. Experiments
- **Datasets & Metrics**: ALMA data from various cycles including Cycle-2 and Cycle-5.
- **Baselines**: ALMA Cycle 2 data, Mock data generated by the replacing process, Mock observational data, N/A, Original data, Previous source-finding methods
- **Main Results**: Confirmed that previously detected emitters/candidates are classified as unreliable.
- **Ablations**: Comparison of clump-finding results between TDM and FDM correlator datacubes.
- **Limitations / Stress Tests**: Correction for number count is required up to one order of magnitude at a luminosity range of â‰¥5Ã—10^8 LâŠ™.

### 6. Takeaways
- **Pros**: Improved understanding of false detection rates in high-redshift surveys., Method can be applied to future blind line surveys., Confirmed the need for correction in luminosity count estimates.
- **Cons**: Dependence on mock datasets may limit generalizability., Potential biases in previous detection methods., Technical challenges in data collection and analysis.
- **Future Work**: Further refinement of the luminosity function estimation method., Exploration of additional line emissions beyond [Cii]., Investigation of other high-redshift galaxy formation mechanisms.

</details>

## avatar

### [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](http://arxiv.org/pdf/2510.01619v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dynamic human avatar reconstruction from multi-view videos

### 2. Motivation & Gaps
- The method enables physically accurate dynamic human avatar reconstruction, supporting applications in virtual reality, digital fashion, and entertainment.

- **Related work challenges:**
  - PhysAvatar: Fails when animation inputs have a small degree of self-penetration, causing simulation failures.
  - Existing methods using piecewise linear transformations: Limited in accurately capturing complex deformations and tend to overfit to motions observed during training.
  - Xiang et al.: Relies on a time-consuming manual parameter search to approximate reasonable cloth behavior.
  - C-IPC: Fails to resolve collisions for noisy colliders during Continuous Collision Detection.
  - Learning-based simulation methods: Limited generalizability beyond training dynamics and cannot guarantee physically plausible deformations.
  - DiffAvatar: Omitted appearance modeling and tailored for scan-based asset preparation rather than dynamic avatar reconstruction.
  - Material Point Method (MPM): MPM is primarily used for general object dynamics and lacks effective garment-body collision handling.
  - Existing physics-based avatar models: They do not adequately model the anisotropic properties of garments.
  - PhysAvatar [78]: Existing methods assume ideal conditions for mesh tracking, limiting their applicability in real-world scenarios.
  - PhysAvatar [78]: Fails when driving body mesh colliders have self-penetrations.
  - N/A: N/A
  - PhysAvatar: Achieving accurate garment dynamics and high rendering quality.
  - PhysAvatar [78]: Struggles with accurate garment dynamics and rendering quality.
  - Gaussian Garments [55]: Fails to produce physically accurate deformations.
  - MMLPHuman [74]: Exhibits unnatural surface artifacts or discontinuities under challenging poses.
  - Gaussian Garments [55]: Struggles to capture physical laws under settings where physical parameters must be estimated from only one second of motion, leading to high geometric error.
  - MMLPHuman [74]: Lacks explicit surface modeling and physical understanding, producing unrealistic surface artifacts or broken geometry when encountering unseen poses.
  - Finite-Difference Optimization: Scalability with increasing number of parameters.
  - Relighting-aware extensions for Gaussian avatars: Current framework does not support relightable rendering.
  - Generative priors for inpainting unobserved regions: Rendering quality may degrade for previously occluded or unseen parts.

### 3. Core Idea
- The use of an anisotropic constitutive model to accurately simulate garment behavior with directionally varying stiffness.

### 4. Method
- **Pipeline**: The pipeline optimizes appearance in visible regions from multi-view training frames.
- **Architecture / Loss / Training**: The model applies QR decomposition and reparameterizes the energy as a function of the upper-triangular matrix R.
- **Complexity / Resources**: The finite-difference optimization scales linearly with the number of parameters, which may increase computational cost for fine-grained parameterizations.

### 5. Experiments
- **Datasets & Metrics**: Experiments were conducted on the ActorsHQ [14] dataset, measuring geometry and appearance fidelity, as well as physical plausibility metrics.
- **Baselines**: ARAH [65], C-IPC, Existing MPM collision handling algorithms, GS-Avatar [12], Gaussian Garments [55], MMLPHuman [74], N/A, PhysAvatar, PhysAvatar [78], TA V A [32], XPBD
- **Main Results**: The method achieves state-of-the-art performance in appearance and physical dynamics modeling.
- **Ablations**: Ablation studies validate the importance of key components such as the constitutive model and physical parameter learning.
- **Limitations / Stress Tests**: The method has limitations in scalability, relighting, and occlusion-aware generalization.

### 6. Takeaways
- **Pros**: Supports physically realistic and robust animations for loose garments., Achieves high-quality rendering using 3D Gaussian Splatting., Demonstrates zero-shot generalizability to novel scene interactions.
- **Cons**: Requires significant computational resources., May not generalize well to all types of garment dynamics., Complexity in tuning physical parameters for different materials.
- **Future Work**: Explore further applications of zero-shot generalization., Enhance the robustness of the simulation under more complex scenarios., Investigate additional garment types and their dynamics.

</details>

### [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](http://arxiv.org/pdf/2510.01182v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Empirical analysis of bug reports in multi-user XR systems

### 2. Motivation & Gaps
- The study aims to understand the unique challenges and bugs in multi-user XR systems, which differ from traditional software due to real-time synchronization and immersive interactions.

- **Related work challenges:**
  - Research on software bugs in various domains: Significant gap in understanding bugs specifically arising from multi-user interactions in XR environments.
  - Bug reporting across diverse platforms: Fragmented nature complicates aggregation of knowledge and identification of common patterns.
  - Existing studies on distributed systems: Do not account for the unique complexities of multi-user XR environments.
  - Technical literature on XR: Lacks comprehensive taxonomies for multi-user XR bugs.
  - Previous studies on XR systems: Lack of comprehensive analysis on multi-user bug symptoms and their implications.
  - Developer Forums and User Communities: Insufficient quality of user reviews for symptom analysis
  - Previous studies on software bugs: Traditional software bugs are often tolerable, unlike XR bugs that break immersion.
  - Existing QA methodologies: Holistic QA approaches may be more effective than targeting individual symptoms.
  - Hubs-Foundation/hubs issue #5586: Orphaned state of objects when both creator and owner leave the session.
  - Hubs-Foundation/hubs issue #1000: Implicit ownership leading to invisibility of objects for new users.
  - Hubs-Foundation/hubs issue #4892: Pinned objects disappearing when their creator leaves.
  - Configuration Sensitivity: Many bugs are resolved through configuration adjustments rather than code changes, indicating high sensitivity to deployment configurations.
  - Layered Complexity: Bugs often have multiple contributing root causes, requiring a comprehensive understanding of the entire system stack.
  - Implicit Assumptions: Assumptions valid in single-user contexts often fail in multi-user scenarios, leading to bugs.
  - Traditional software analysis techniques: Fail to capture the unique characteristics of multi-user XR bugs.
  - Current debugging tools: Ill-equipped to handle the distributed, real-time nature of multi-user XR applications.
  - Existing testing frameworks: Lack comprehensive support for multi-user scenarios.
  - Rodriguez and Wang [70]: Analyzed trends and challenges in VR software projects.
  - Adams et al. [45]: Revealed privacy and security threats in VR applications.
  - Xanthidou et al. [77]: Identified major challenges in VR collaboration.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The paper develops comprehensive taxonomies categorizing symptoms, root causes, and consequences of XR defects based on an analysis of 2,649 real-world bug reports.

### 4. Method
- **Pipeline**: Empirical analysis of bug reports to identify and categorize defects in multi-user XR systems.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The study utilized a hierarchical taxonomy and various analytical methods including frequency analysis and cross-tabulation.

### 5. Experiments
- **Datasets & Metrics**: Analysis of 2,649 real-world bug reports from various XR platforms.
- **Baselines**: Current testing frameworks, Existing debugging tools, Hubs-Foundation/hubs, Multi-user XR systems, N/A, Single-user XR application bug analysis, Single-user XR applications, Single-user XR bug reports, Single-user XR systems, Traditional distributed systems studies, Traditional multi-user applications, Traditional online collaboration tools, Traditional software analysis techniques, Traditional software bug analysis, Traditional software bug analysis methods, Unity SDK, Unreal Engine
- **Main Results**: Over 34% of defects cause severe disruptions like crashes and interaction breakdowns, with synchronization inconsistencies and performance issues being prominent.
- **Ablations**: N/A
- **Limitations / Stress Tests**: Findings may not fully reflect emerging technologies or practices due to the rapidly evolving XR landscape.

### 6. Takeaways
- **Pros**: Provides actionable recommendations for developers and platform vendors., Develops a comprehensive taxonomy for multi-user XR bugs., Highlights unique challenges in multi-user XR systems.
- **Cons**: Limited understanding of multi-user XR bugs compared to other domains., Fragmented bug reporting complicates knowledge aggregation., Potential privacy and health implications remain concerning.
- **Future Work**: Further research on automated quality assurance tools for multi-user XR., Exploration of specialized testing and debugging approaches., Investigation into user experience impacts of multi-user XR bugs.

</details>

### [Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/pdf/2510.01176v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Animation Generation

### 2. Motivation & Gaps
- Existing methods for facial animation often produce spatiotemporally inconsistent results, leading to unnatural avatars.

- **Related work challenges:**
  - Fan et al. 2022; Richard et al. 2021; Xing et al. 2023: Existing methods lack sufficient detail for conveying subtle facial cues essential in social interactions.
  - Li et al. 2024b; Ng et al. 2024: Primarily operate in an offline manner requiring entire audio sequences as input rather than processing audio streams in real-time.
  - GSTalker: Limited generalization to multiple identities due to training on personalized data.
  - GaussianTalker: Real-time rendering achieved, but audio-based deformations computed offline.
  - EmoTalk3D: High fidelity achieved but remains computationally expensive and lacks real-time performance.
  - Universal Relightable Prior Model [Li et al. 2024a]: Existing models may not effectively handle real-time performance and the integration of gaze direction.
  - Diffusion models for facial expression generation: Inherent slowness in generating realistic and expressive motions.
  - Graph-based gaze synthesis: Ensuring plausible transitions and consistency in gaze movements.
  - Audio encoders for real-time applications: Maintaining causality in audio processing without future information.
  - GaussianTalker: Generates high-fidelity 3D face animation but is not designed for real-time applications.
  - TalkingGaussian: Focuses on person-specific 3D deformation and rendering, lacking universality.
  - GaussianTalker [Cho et al. 2024]: Generates high-fidelity 3D face animation but learns person-specific deformations.
  - TalkingGaussian [Li et al. 2024b]: Similar to GaussianTalker, it focuses on person-specific rendering.
  - DiffPoseTalk [Sun et al. 2024]: Employs style conditioned diffusion models but does not operate in real-time.
  - wav2vec 1.0: Non-causal encoding limitations
  - wav2vec 2.0: Maintaining accuracy while achieving real-time performance
  - N/A: N/A
  - FaceFormer: High latency and unsuitability for real-time applications.
  - CodeTalker: High computational overhead and autoregressive nature.
  - AniPortrait: Blurry and distorted artifacts in generated images.
  - Wav2vec 1.0: Employs causal convolutional neural networks which may limit the quality of audio processing.
  - Wav2vec 2.0: Utilizes non-causal CNN layers, which can disrupt causality in audio processing.
  - HuBERT: Relies on non-causal CNN layers and includes an offline clustering step, which may not ensure full causality.
  - N/A: N/A

### 3. Core Idea
- The proposed method generates natural, high-fidelity avatars with accurately synchronized lip movements by utilizing a novel online transformer architecture and gaze synthesis techniques.

### 4. Method
- **Pipeline**: The method involves capturing audio and gaze data, processing it through a transformer architecture, and synthesizing facial animations based on the processed data.
- **Architecture / Loss / Training**: The architecture employs a self-attention mechanism with a windowed mask to maintain temporal coherence and reduce boundary issues.
- **Complexity / Resources**: distributed data-parallel (DDP) using two A100 GPUs; distillation training and emotion-conditioning training are done with a single A100 GPU

### 5. Experiments
- **Datasets & Metrics**: Out of 265 capture subjects, 237 for training and 28 for testing; data segmented into sequences of frame length 100 (in 30FPS)
- **Baselines**: AniPortrait, Audio2Photoreal, Audio2Photoreal [Ng et al. 2024], Audio2Photoreal-Face, CodeTalker, DiffPoseTalk, DiffPoseTalk [Sun et al. 2024], EmoTalk3D, Existing diffusion models, Existing offline state-of-the-art methods, FaceFormer, GSTalker, GaussianTalker, HuBERT, N/A, TalkShow, TalkShow [Yi et al. 2023], TalkShow-Face, Traditional regression models for facial expression generation, Wav2vec 1.0, Wav2vec 2.0, wav2vec 1.0, wav2vec 2.0
- **Main Results**: Participants evaluated facial expressions for naturalness and synchronization with audio, collecting 240 responses per set.
- **Ablations**: Ablation studies were conducted to evaluate the impact of different components of the model on performance.
- **Limitations / Stress Tests**: Out of 28 test subjects, two were excluded for excessive frame drops and less than 70 segments can be used for freeform speech; two were excluded for less than 30 segments for sentence reading.

### 6. Takeaways
- **Pros**: High fidelity and universal 3D facial avatars., Real-time performance suitable for social interactions., Versatile framework for multimodal applications.
- **Cons**: Challenges in maintaining photorealism under all conditions., Dependency on audio quality for accurate expression generation., Potential computational limitations in diverse scenarios.
- **Future Work**: Exploration of additional multimodal inputs., Enhancements in real-time performance and fidelity., Broader applications in various social telepresence scenarios.

</details>

## video understanding

### [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](http://arxiv.org/pdf/2510.02313v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- sounding object detection

### 2. Motivation & Gaps
- The paper addresses the challenge of aligning audio and visual modalities for sound source localization.

- **Related work challenges:**
  - Existing multimodal techniques: Struggle to differentiate subtle sound impacts caused by different materials.
  - Current learning methods: Rely on global features that take the whole scene as input, which is inefficient for localized object interactions.
  - Multimodal object-centric representation learning: Often trained on synthetic or laboratory-collected data, limiting scalability to real-world interactions.
  - Slot Attention Models: Compressing image features into slot vectors for object segmentation.
  - Multimodal Datasets: Data is often generated synthetically or collected in controlled lab environments.
  - Audiovisual Localization: Requires precise boundary predictions which may not align with the task of identifying involved objects.
  - Previous object-centric learning methods: Directly applying object segmentation masks to input visual data may not yield optimal results.
  - SoundingActions: Learning from global representations rather than object-aware approaches.
  - DenseA V: Localization of both sounds and speech in audiovisual segmentation.
  - SLA VC: Using a popular localization framework that may not be optimized for sounding object detection.
  - SoundingActions: Achieving better performance on the task by integrating object-aware training.
  - ImageBind: Marginally better performance than chance, indicating the need for improved alignment of modalities.
  - LanguageBind: Similar to ImageBind, it shows the necessity of localized approaches for better performance.
  - N/A: N/A
  - Learning object permanence from video: N/A
  - Understanding human hands in contact at internet scale: N/A
  - Semantic object prediction and spatial sound super-resolution with binaural sounds: N/A
  - N/A: N/A

### 3. Core Idea
- Extract ground truth object masks using visual embeddings that correspond to sounds.

### 4. Method
- **Pipeline**: The method involves a multi-modal approach that integrates audio and visual embeddings for improved localization.
- **Architecture / Loss / Training**: The architecture employs a slot attention model for visual encoding and uses contrastive learning for training.
- **Complexity / Resources**: The model complexity is managed by using pretrained encoders and a common embedding space.

### 5. Experiments
- **Datasets & Metrics**: Epic Kitchens, Ego4D
- **Baselines**: Audiovisual Localization Models, DenseA V, Existing multimodal action understanding tasks, ImageBind, LanguageBind, N/A, SLA VC, SSLAlign, Slot Attention Models, SoundingActions
- **Main Results**: Additional qualitative results for sounding object detection.
- **Ablations**: Ablation studies indicate the importance of cross-modal alignment in enhancing performance.
- **Limitations / Stress Tests**: Limitations include potential biases in the datasets and challenges in real-world application.

### 6. Takeaways
- **Pros**: Achieves state of the art performance on new tasks., Scalable to large datasets with diverse object interactions., Incorporates an object-aware approach to enhance learning.
- **Cons**: Requires significant computational resources., Limited by the quality of the training data., May not generalize well to unseen object interactions.
- **Future Work**: Explore additional modalities for richer context., Investigate real-time applications in interactive systems., Enhance the model's ability to generalize across different environments.

</details>

### [Inferring Dynamic Physical Properties from Video Foundation Models](http://arxiv.org/pdf/2510.02311v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Comparative analysis of elasticity in video frames

### 2. Motivation & Gaps
- The study aims to analyze and compare the elasticity characteristics of different objects as observed in video frames.

- **Related work challenges:**
  - Wu et al., 2015; Ding et al., 2021; Jatavallabhula et al., 2021: Early methods rely on simulation supervision and handcrafted heuristics.
  - Voleti et al., 2022; Lu et al., 2023: Unsupervised learning captures latent dynamics but lacks interpretability in terms of concrete physical quantities.
  - Kawabe et al., 2014; Paulun et al., 2015: Later works infer attributes from task-specific visual cues, which may not generalize well.
  - Bordes et al., 2018: Proposes broad benchmarks but does not address quantitative physical property estimation.
  - Tung et al., 2025: Focuses on qualitative assessments rather than quantitative evaluations.
  - Bear et al., 2021: Similar limitations in addressing quantitative physical understanding.
  - Classical computer vision techniques: Limited accuracy in estimating physical properties due to reliance on heuristics.
  - Video generative models: Need for effective feature extraction and representation learning for physical property estimation.
  - Multimodal large language models (MLLMs): Integrating visual and textual prompts for accurate predictions.
  - DynamiCrafter (Xing et al., 2024): Generalizing from synthetic datasets to real-world scenarios.
  - V-JEPA-2 (Assran et al., 2025): Effective feature extraction from video data.
  - Shtedritski et al. (2023): Mitigating the sim-to-real gap in model training.
  - Video Generative Model: Struggles with generalization to real-world data for friction due to reliance on visual references.
  - Video Self-Supervised Model: Similar struggles with friction estimation and performance degradation on real-world data.
  - Multimodal Large Language Models (MLLMs): Performance drops significantly on synthetic splits, indicating a reliance on semantic rather than visual cues.
  - Learning to poke by poking: Experiential learning of intuitive physics: Models fall short of the oracle, particularly in absolute value prediction.
  - V-jepa: Latent video prediction for visual representation learning: Generative and self-supervised models have similar performance but need improvement in physical reasoning.
  - Physion: Evaluating physical prediction from vision in humans and machines: MLLMs perform worse overall but improve with more informative prompting.
  - Sam 2: Segment anything in images and videos: N/A
  - Grounding dino 1.5: Advance the 'edge' of open-set object detection: N/A
  - Grounded sam: Assembling open-world models for diverse visual tasks: N/A
  - N/A: N/A
  - N/A: N/A
  - Previous methods of estimating restitution coefficients: Inaccurate identification of key points in the object's trajectory can lead to incorrect restitution estimates.
  - Previous studies on elasticity analysis: Limited accuracy in estimating restitution coefficients from visual data.
  - Comparative analysis of elasticity in materials: Difficulty in determining which material exhibits higher elasticity based solely on visual input.
  - Previous studies on material elasticity: Lack of quantitative analysis methods for elasticity comparison.
  - Previous studies on elasticity analysis: Limited interpretability and stability in few-shot examples.
  - DynamiCrafter: Reducing the sim2real gap
  - V-JEPA-2: N/A
  - Qwen2.5VL-max: Limitation of resources
  - GPT-4o: Limitation of resources
  - Gemini-2.5-pro: Limitation of resources

### 3. Core Idea
- Using red circles to improve the performance of models in sim2real tasks.

### 4. Method
- **Pipeline**: Analysis of video frames to assess elasticity characteristics.
- **Architecture / Loss / Training**: MLP networks are trained with L1 loss for absolute value prediction and binary cross-entropy loss for relative value prediction.
- **Complexity / Resources**: Random subset of 100 samples used due to resource limitations.

### 5. Experiments
- **Datasets & Metrics**: Videos showcasing different objects with varying elasticity properties.
- **Baselines**: Baseline, Baseline model for elasticity comparison, Baseline model for elasticity estimation, Classical estimation methods, Few-Shot Examples, Frame Index Provided, GPT-4o, GPT-4o (Hurst et al., 2024), Gemini 2.5 Pro (Comanici et al., 2025), Gemini-2.5-pro, Generative models, Heuristic-based approaches, Multi-modal large language models (MLLMs), N/A, Oracle Estimation Teaching, Oracle Estimator, Oracle estimation teaching model, Oracle-guided analysis, Previous elasticity analysis methods, Previous estimation methods, Qwen2.5-VL-Max (Hui et al., 2024), Qwen2.5VL-max, Self-supervised models, Video Generative Model, Video Self-Supervised Model, With Red circle, Without red circle
- **Main Results**: Performance improved from 0.47 to 0.84 on real test split test-3.
- **Ablations**: Ablation study conducted using DynamiCrafter for elasticity property.
- **Limitations / Stress Tests**: Challenges in accurately estimating coefficients due to visual ambiguities in frame analysis.

### 6. Takeaways
- **Pros**: Introduces a novel dataset for evaluating dynamic physical properties., Demonstrates the effectiveness of prompting strategies for MLLMs., Shows that video foundation models can infer physical properties with reasonable accuracy.
- **Cons**: MLLMs currently underperform compared to other models., The approach may not generalize well to all physical properties., Dependence on visual cues may limit inference capabilities.
- **Future Work**: Explore more robust prompting strategies for MLLMs., Investigate generalization to a wider range of physical properties., Enhance the dataset with more diverse real-world scenarios.

</details>

### [Astrophysical Consequences of an Electroweak $\etaw$ Pseudo-Scalar](http://arxiv.org/pdf/2510.02310v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the astrophysical implications of the Î·w boson and its potential as a dark matter candidate.

### 2. Motivation & Gaps
- The paper discusses the astrophysical constraints on the ultra-light pseudo-scalar Î·w and its implications for dark matter.

- **Related work challenges:**
  - Ref. [1]: The theoretical arguments supporting the presence of Î·w within the Standard Model need further examination.
  - Ref. [2]: Current astrophysical bounds related to stellar emission of new light particles may constrain the basic properties of Î·w.
  - Ref. [6]: The identification of Î·w with an atomic state does not conform with the expected mass scaling.
  - Dvali et al. (2025): The need for new physics beyond the Standard Model to account for dark matter.
  - CAST experiment (2017): Constraints on axion-photon coupling affecting the viability of Î·w as a dark matter candidate.
  - N/A: N/A

### 3. Core Idea
- The Î·w boson can emerge in the Standard Model due to non-perturbative electroweak interactions, but its properties are constrained by astrophysical observations.

### 4. Method
- **Pipeline**: Theoretical analysis of the coupling of Î·w to photons and electrons, and estimation of bounds based on astrophysical observations.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

### 5. Experiments
- **Datasets & Metrics**: Astrophysical data from Supernova 1987A and the CAST experiment.
- **Baselines**: Astrophysical bounds from stellar emissions, N/A, Previous bounds on axion-photon coupling, Standard Model predictions, Standard Model predictions for dark matter candidates
- **Main Results**: The strongest bound on the decay constant of Î·w is fÎ·w â‰³1000TeV, which is significantly higher than previous expectations.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The non-perturbative nature of Î·w dynamics may defy naive expectations for its properties.

### 6. Takeaways
- **Pros**: Potentially significant implications for understanding electroweak interactions., Challenges existing predictions and motivates further theoretical investigations., Explores the relationship between Î·w and astrophysical phenomena.
- **Cons**: The status of Î·w as a firm prediction of the Standard Model is not well-established., Identifying Î·w with atomic states does not conform with expected mass scaling., Current astrophysical bounds may limit the viability of Î·w as a dark matter candidate.
- **Future Work**: Further theoretical and phenomenological investigations of Î·w are warranted., Exploration of the implications of Î·w for dark matter and dark energy., Assessment of the impact of astrophysical constraints on the properties of Î·w.

</details>
