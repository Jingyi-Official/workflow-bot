# Daily Paper Digest Â· 2025-10-12
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [ReSplat: Learning Recurrent Gaussian Splats](http://arxiv.org/pdf/2510.08575v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View Synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of efficient and high-quality view synthesis using a recurrent Gaussian splatting model.

- **Related work challenges:**
  - Charatan et al., 2024: Models predict one or multiple Gaussians for each pixel, leading to scalability issues.
  - Kerbl et al., 2023: Per-scene optimization methods are expensive and not efficient for real-time applications.
  - Zhang et al., 2024a: Single-step feed-forward inference limits achievable quality for complex scenes.
  - SplatFormer (Chen et al., 2025a): Introduces a single-step refinement network but is evaluated only on object-centric datasets, making it difficult to apply to complex scenes.
  - DeepView (Flynn et al., 2019): Requires explicit gradient computation, which is not needed in our method.
  - G3R (Chen et al., 2024d): Struggles with sparse points and requires well-covered 3D points for initialization.
  - Zhao et al., 2021: NaÃ¯ve prediction of Gaussian parameters leads to performance loss.
  - Vaswani et al., 2017: N/A
  - Xu et al., 2024: N/A
  - Chen et al., 2024a: N/A
  - 3DGS (Kerbl et al., 2023): Requires thousands of iterations to converge, leading to inefficiency.
  - MVSplat (Chen et al., 2024b): Produces millions of Gaussians, resulting in slower rendering speeds.
  - DepthSplat (Xu et al., 2025): Similar inefficiencies in rendering and convergence.
  - Long-LRM (Chen et al., 2025b): Uses Gaussian pruning based on opacity values, leading to a reduction in the number of Gaussians but slower rendering speed.
  - LVSM (Jin et al., 2025): Encoder-decoder architecture that is outperformed by ReSplat in terms of PSNR and rendering speed.
  - MVSplat (Xu et al., 2025): Previous feed-forward 3DGS models that ReSplat outperforms significantly.
  - N/A: N/A
  - Depth anything v2: N/A
  - No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images: N/A
  - gsplat: An open-source library for gaussian splatting: N/A
  - Mip-splatting: Alias-free 3d gaussian splatting: N/A
  - Gs-lrm: Large reconstruction model for 3d gaussian splatting: N/A
  - Gaussian graph network: Learning efficient and generalizable gaussian representations from multi-view images: N/A
  - Point transformer: N/A
  - Stereo magnification: learning view synthesis using multiplane images: N/A

### 3. Core Idea
- ReSplat leverages a recurrent Gaussian splatting approach to achieve efficient view synthesis while maintaining high quality by reducing the number of Gaussians and improving rendering speed.

### 4. Method
- **Pipeline**: The method involves a recurrent process that utilizes rendering error as feedback and operates in a compact 3D space.
- **Architecture / Loss / Training**: The architecture employs kNN attention and global attention mechanisms to enhance performance.
- **Complexity / Resources**: The model is trained on 4 GH200 GPUs, and the computational cost increases with the number of Gaussians.

### 5. Experiments
- **Datasets & Metrics**: Experiments are conducted on the DL3DV dataset and RealEstate10K, using metrics such as PSNR, SSIM, and LPIPS.
- **Baselines**: 3DGS, DeepView, DepthSplat, G3R, GS-LRM, Long-LRM, MVSplat, N/A, Point Transformer (Zhao et al., 2021), SplatFormer, gsplat (Ye et al., 2025b)
- **Main Results**: Our ReSplat reconstructs cleaner Gaussians and renders higher-quality images than DepthSplat.
- **Ablations**: Ablation studies show the importance of kNN attention and global attention in maintaining performance.
- **Limitations / Stress Tests**: The model's reliance on kNN-based point attention incurs high computational costs, especially with large numbers of Gaussians.

### 6. Takeaways
- **Pros**: Significantly faster rendering speed compared to optimization-based methods., Improved generalization to unseen data distributions., Reduced computational overhead with fewer Gaussians.
- **Cons**: Initial model complexity may still be high for certain applications., Recurrent updates may require careful tuning for optimal performance., Dependence on the quality of the initial Gaussian predictions.
- **Future Work**: Explore further optimizations for real-time applications., Investigate the applicability of the method to other domains., Enhance the model's ability to handle even more complex scenes.

</details>

### [Reconstructing the local density field with combined convolutional and point cloud architecture](http://arxiv.org/pdf/2510.08573v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Recovering the local density field from peculiar velocities

### 2. Motivation & Gaps
- The study aims to improve predictions of high-density small-scale features in cosmology using machine learning techniques.

- **Related work challenges:**
  - Traditional linear methods for density reconstruction: Limited to large scales and cannot optimally use dense information from modern peculiar velocity data.
  - Machine learning approaches using convolutional neural networks: Suboptimal use of gridded radial velocities and neglect of small-scale information.
  - Direct inversion methods: Often yield biased low amplitude results.
  - 3D Wiener filter: Uses full 3D velocities, which is not realistic in many observational scenarios.
  - U-Net architectures: May not effectively capture small-scale features without additional information.
  - Wiener Reconstruction of Large-Scale Structure from Peculiar Velocities: Linear Wiener filtering does not capture small-scale features effectively.
  - Three-dimensional Velocity and Density Reconstructions of the Local Universe with Cosmicflows-1: Existing methods struggle with moderate tracer density.
  - Field-based physical inference from peculiar velocity tracers: Incorporating observational errors remains a challenge.

### 3. Core Idea
- A hybrid model combining U-Net and confidence network-controlled DeepSets to enhance the recovery of local density fields.

### 4. Method
- **Pipeline**: The model processes peculiar velocities to reconstruct the local density field.
- **Architecture / Loss / Training**: Utilizes a combination of convolutional networks and point-cloud networks with a focus on confidence estimation.
- **Complexity / Resources**: Moderate computational resources required for training and inference.

### 5. Experiments
- **Datasets & Metrics**: Utilizes data from multiple surveys on peculiar velocities to evaluate performance.
- **Baselines**: 3D Wiener filter, Confidence U-Net (Âµprediction), Direct inversion (1hâˆ’1Mpc smoothing), Direct inversion (4hâˆ’1Mpc smoothing), Existing density reconstruction methods, Linear Wiener filtering, Normal U-Net, U-Net-only approach
- **Main Results**: The hybrid model significantly outperforms linear Wiener filtering, especially in recovering small-scale features.
- **Ablations**: Further analysis needed to understand the impact of each component in the hybrid model.
- **Limitations / Stress Tests**: Observational errors could affect the results, which will be addressed in future work.

### 6. Takeaways
- **Pros**: Improved reconstruction quality on small scales., Efficient use of both small-scale and large-scale information., Potential for application in future cosmological parameter inference.
- **Cons**: High computational cost due to DeepSets evaluation., Limited to numerical simulations in this work., Neglect of stochasticity in the current architecture.
- **Future Work**: Incorporate stochasticity in the network architecture., Evaluate on observational data to address systematic effects., Explore joint inference of cosmological parameters and the matter density field.

</details>

### [Who Said Neural Networks Aren't Linear?](http://arxiv.org/pdf/2510.08570v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Learning invertible coordinate transformations for neural networks

### 2. Motivation & Gaps
- The paper discusses the learning of invertible coordinate transformations that allow neural networks to act as exact linear operators, leading to applications in flow matching, style transfer, and idempotency enforcement.

- **Related work challenges:**
  - Hornik et al., 1989: Neural networks are famously nonlinear, complicating their analysis and manipulation.
  - Strang, 2022: Linear systems have well-understood properties that do not apply to nonlinear systems.
  - Strogatz, 2024: Iterating nonlinear mappings can lead to intractable dynamics.
  - Flow matching (Lipman et al., 2023; Song et al., 2021b): Traditional velocity integration over time is slow.
  - V AEs (Kingma & Welling, 2014): Lack of a natural encoder for mapping data back into the prior space.
  - Inversion methods for diffusion (Dhariwal & Nichol, 2021; Song et al., 2021a; Huberman-Spiegelglas et al., 2024): Approximate techniques suffer from reconstruction errors and computational overhead.
  - Gatys et al. (2016): Initial style transfer methods were computationally intensive and not practical for real-time applications.
  - Johnson et al. (2016): Perceptual-loss training improved efficiency but still faced issues with idempotency and reconstruction quality.
  - Shocher et al. (2024): Idempotent Generative Networks (IGNs) struggled with enforcing true idempotency across the training data.
  - Koopman operator theory: Linearizes nonlinear dynamics but does not address learned mappings.
  - Neural Tangent Kernel: Achieves linearity in parameter space but remains nonlinear in input-output mapping.
  - Invertible Neural Networks: Inherently more challenging to train than standard architectures.
  - N/A: N/A
  - Denoising diffusion implicit models: N/A
  - Score-based generative modeling through stochastic differential equations: N/A
  - Consistency models: N/A
  - N/A: N/A

### 3. Core Idea
- The core idea is to utilize invertible coordinate transformations to enable neural networks to function as exact linear operators, which can enhance various applications in generative modeling.

### 4. Method
- **Pipeline**: Learn invertible coordinate maps and a finite matrix to achieve exact linearity.
- **Architecture / Loss / Training**: Requires careful design due to the complexity of training invertible networks.
- **Complexity / Resources**: Involves significant computational resources for training and implementation.

### 5. Experiments
- **Datasets & Metrics**: Utilizes various datasets to demonstrate the feasibility of the Linearizer framework.
- **Baselines**: Gatys et al. (2016), Johnson et al. (2016), Multi-step flow matching, N/A, Shocher et al. (2024), Standard neural network architectures
- **Main Results**: Demonstrates the ability to achieve exact linearity and new applications in one-step flow matching and style transfer.
- **Ablations**: Ablation studies confirm the effectiveness of the Linearizer in preserving information and achieving idempotency.
- **Limitations / Stress Tests**: Identifies challenges in training invertible networks and the need for further theoretical exploration.

### 6. Takeaways
- **Pros**: Enables the application of linear algebra techniques to nonlinear neural networks., Facilitates efficient training and manipulation of neural networks., Provides a new perspective on the structure and capabilities of neural networks.
- **Cons**: The framework may not be applicable to all types of neural networks., Potential complexity in understanding the induced vector spaces., Limited empirical validation provided in the paper.
- **Future Work**: Explore the application of Linearizers in various neural network architectures., Investigate the implications of Linearizers on model interpretability., Develop further empirical studies to validate the proposed framework.

</details>

## Gaussian Splatting

### [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](http://arxiv.org/pdf/2510.08566v1)
  (summary failed: 'utf-8' codec can't encode characters in position 8413-8414: surrogates not allowed)


### [ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation](http://arxiv.org/pdf/2510.08551v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction and localization from monocular image sequences

### 2. Motivation & Gaps
- The paper presents ARTDECO, a framework that combines feed-forward priors and structured Gaussian representations for efficient and accurate 3D reconstruction.

- **Related work challenges:**
  - Per-scene optimization methods: Achieve high accuracy but are computationally expensive.
  - Feed-forward models: Enable real-time inference but struggle with accuracy and robustness.
  - MÃ¼ller et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Xu et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Sun et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Lin et al., 2021: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Fu et al., 2024: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Lin et al., 2025b: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Prior 3DGS-based SLAM methods: Relying only on keyframes for reconstruction, which limits the use of available information.
  - MASt3R predictions: Instability near object boundaries affecting the accuracy of pose estimation.
  - Loop detection methods: Robustness against weak correspondences and noisy inputs.
  - 3D Gaussian Splatting approaches: Maintaining high-quality rendering details in complex environments.
  - SLAM systems: Achieving accurate pose estimation in diverse scene conditions.
  - MASt3R-SLAM: Limited performance under varying viewpoints and low-texture surfaces.
  - 3DGS-based methods: Struggles with small-parallax inputs leading to ghosting and blur.
  - Existing SLAM systems: Dependence on consistent illumination and sufficient parallax.
  - N/A: N/A
  - N/A: N/A
  - MASt3R: Predicts two-view correspondences and metric pointmaps with limited temporal context.
  - Ï€3: Not explicitly metric-aware, introducing challenges in maintaining scale and temporal consistency.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- ARTDECO integrates feed-forward 3D foundation models with structured Gaussian representations to enhance the accuracy and efficiency of 3D reconstruction.

### 4. Method
- **Pipeline**: The method involves a feed-forward model for pose estimation and a structured Gaussian representation for reconstruction.
- **Architecture / Loss / Training**: Utilizes a combination of PSNR, SSIM, and LPIPS metrics for evaluating reconstruction quality.
- **Complexity / Resources**: The method is designed to be efficient, achieving strong results with a runtime that is competitive with existing methods.

### 5. Experiments
- **Datasets & Metrics**: ScanNet, Waymo, VR-NeRF, KITTI, ScanNet++
- **Baselines**: 3D Gaussian Splatting, Classical visual SLAM systems, DPV-SLAM, DROID-SLAM, Feed-forward models, Go-SLAM, LongSplat, MASt3R, MASt3R-SLAM, MonoGS, N/A, NeRF-based SLAM methods, OnTheFly, OnTheFly-NVS, Ours, Per-scene optimization methods, Prior 3DGS-based SLAM methods, S3PO-GS, SEGS-SLAM, SLAM, Ï€3
- **Main Results**: Ours consistently outperforms the baselines across various datasets and metrics.
- **Ablations**: Replacing the pairwise correspondence model MASt3R with the multi-frame visual-geometry model Ï€3.
- **Limitations / Stress Tests**: The method's performance is affected by noise, blur, and lighting changes, which can lead to drift or artifacts.

### 6. Takeaways
- **Pros**: Combines efficiency and robustness in 3D reconstruction., Achieves high fidelity while maintaining real-time performance., Utilizes a principled level-of-detail mechanism.
- **Cons**: Generally achieves lower accuracy than per-scene optimized methods., Challenges with maintaining global consistency., Struggles with handling high-resolution inputs.
- **Future Work**: Explore further optimizations for large-scale environments., Investigate additional applications in AR/VR., Enhance robustness against challenging scene conditions.

</details>

## avatar

### [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](http://arxiv.org/pdf/2510.07685v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-time reasoning in interactive e-commerce applications

### 2. Motivation & Gaps
- The paper addresses the need for low-latency, high-performance reasoning models in interactive e-commerce settings.

- **Related work challenges:**
  - Retrieval-Augmented Generation (RAG) systems powered by Large Reasoning Models (LRMs): High inference delays despite strong reasoning capabilities.
  - Reinforcement Learning for LLMs: Aligning LLMs with human preferences and addressing hallucinations and harmful content generation.
  - Efficient Reasoning: Overthinking phenomenon leading to verbose reasoning paths in LLMs.
  - Teacher-based Data Generation: Ensuring the quality of distillation data through effective filtering of generated trajectories.
  - Student MoE Model Fine-tuning: Balancing the load across experts in a Mixture of Experts model to ensure stable training.
  - Reinforcement Learning Optimization: Optimizing for reasoning efficiency, correctness, and helpfulness simultaneously.
  - DeepSeek-R1: High computational cost and inefficiency in generating responses.
  - Qwen3-30B-A3B: Limited ability to optimize reasoning paths effectively.
  - Existing AI models for livestreaming: Lack of efficiency and quality in reasoning
  - Knowledge distillation: Standard distillation inherits verbose reasoning trajectories from the teacher model.
  - MoE architecture: Justifying the performance-to-cost ratio against traditional dense models.
  - Reinforcement learning for reasoning: Balancing correctness and helpfulness while minimizing computational costs.
  - Demystifying long chain-of-thought reasoning in llms: N/A
  - Efficient rl training for reasoning models via length-aware optimization: N/A
  - Large language models for information retrieval: A survey: N/A
  - N/A: N/A

### 3. Core Idea
- Introducing LiveThinking, a two-stage training framework that adapts large reasoning models to real-time applications by balancing quality and latency.

### 4. Method
- **Pipeline**: The approach leverages RFT to distill reasoning ability from a 670B teacher into a lightweight 30B student model, followed by GRPO to compress reasoning paths.
- **Architecture / Loss / Training**: Utilizes a multi-objective reward function to enhance correctness and helpfulness while compressing reasoning paths.
- **Complexity / Resources**: Achieves a 30-fold reduction in computational cost compared to the teacher model.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on proprietary Tblive-E-Commerce QA dataset and the public MuSiQue dataset.
- **Baselines**: 14B dense production model, 670B teacher model, 670B teacher model (DeepSeek-R1), DeepSeek-R1, DeepSeek-R1-Distilled-Llama-8B, Dense models ranging from 4B to 32B parameters, Existing AI models in e-commerce reasoning, N/A, Qwen3-235B-A22B, Qwen3-30B-A3B, Qwen3-32B
- **Main Results**: LiveThinking outperforms its larger teacher model on correctness and helpfulness while being more computationally efficient.
- **Ablations**: Ablation studies confirm the effectiveness of RFT and RL in improving model performance.
- **Limitations / Stress Tests**: The model's performance is sensitive to the target reasoning length, with trade-offs between brevity and quality.

### 6. Takeaways
- **Pros**: Significantly reduces computational cost., Improves response correctness and helpfulness., Demonstrates effectiveness in high-traffic production environments.
- **Cons**: Initial model inherits verbose reasoning paths., Latency may still exceed industrial requirements without optimization.
- **Future Work**: Explore further optimizations for reasoning paths., Investigate additional applications in real-time systems., Enhance user engagement metrics through improved interaction.

</details>

### [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](http://arxiv.org/pdf/2510.05488v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Gaussian head avatar creation

### 2. Motivation & Gaps
- The paper addresses the need for real-time and continuous adjustment of the level of detail (LOD) in 3D Gaussian head avatars.

- **Related work challenges:**
  - 3D Gaussian Splatting (3DGS): Struggles to maintain real-time performance when rendering multiple avatars simultaneously.
  - Conventional LOD methods: Provide only a few discrete levels, leading to unsmooth visual effects when switching between levels.
  - UV-based strategies: Do not capture sufficient local information for detailed 3D head appearance.
  - 3D Morphable Models (3DMMs): Less effective at modeling non-rigid facial features like hair.
  - Neural radiance field (NeRF)-based methods: Computationally intensive and less accurate with geometry.
  - LoDAvatar: Only supports discrete LOD control and relies on synthetic multi-view images for training.
  - N/A: N/A
  - GaussianAvatars: Limited detail preservation at high LOD.
  - FlashAvatar: Quality degradation at lower LOD.
  - RGBAvatar: Inability to maintain reasonable quality across varying LOD.
  - FLAME tracking: Relies on accurate tracking for reliable 3D-2D alignment, which is essential for maintaining 3D consistency.
  - Existing methods: Some expression modes appear only under large head poses, leading to overfitting and artifacts.
  - N/A: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A
  - Headgap: Few-shot 3d head avatar via generalizable gaussian priors: N/A
  - Pointavatar: Deformable point-based head avatars from videos: N/A
  - Instant volumetric head avatars: N/A

### 3. Core Idea
- ArchitectHead introduces a framework that allows for continuous LOD control in 3D Gaussian head avatars by parameterizing Gaussians in UV feature space.

### 4. Method
- **Pipeline**: The method uses a neural decoder to generate Gaussian attributes based on the LOD as an additional condition.
- **Architecture / Loss / Training**: A learnable UV latent feature map is introduced alongside the UV position map to provide more representative information.
- **Complexity / Resources**: The design extends to a multi-level latent feature field, enabling weighted resampling across resolutions.

### 5. Experiments
- **Datasets & Metrics**: Experiments were conducted on monocular video datasets to evaluate the performance of ArchitectHead.
- **Baselines**: 3D Morphable Models (3DMMs), Conventional LOD methods, Existing 3D Gaussian head methods, Existing 3DGS-based avatars, FlashAvatar, Gaussian Dejavu, GaussianAvatars, LoDAvatar, N/A, Neural radiance field (NeRF)-based methods, RGBAvatar
- **Main Results**: ArchitectHead achieves state-of-the-art quality in generating 3D head avatars.
- **Ablations**: Ablation studies show that using a multi-level feature field outperforms single-resolution feature maps.
- **Limitations / Stress Tests**: The method's reliance on accurate FLAME tracking and the tendency to overfit rare expression modes under large head poses.

### 6. Takeaways
- **Pros**: Supports continuous LOD control for better rendering efficiency., Achieves high-quality rendering with fewer Gaussians., Enables real-time performance for 3D head avatars.
- **Cons**: Initial training requires high computational resources., UV position map may not capture all local details., Quality degradation at lower LODs may affect visual fidelity.
- **Future Work**: Explore further optimizations for real-time performance., Investigate additional applications of continuous LOD in other domains., Enhance the UV feature field to capture more detailed local information.

</details>

### [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](http://arxiv.org/pdf/2510.04822v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D Virtual Try-On

### 2. Motivation & Gaps
- The paper addresses the challenges of achieving realistic 4D virtual try-on (VTON) using single in-shop garment references, focusing on dynamic pose control and multi-view rendering.

- **Related work challenges:**
  - Image-based VTON models: Lack intrinsic 3D perceptual understanding, leading to discontinuous try-on results across changing viewpoints and poses.
  - Animatable avatar-based garment transfer approaches: Dependence on large-scale datasets limits scalability and practical use.
  - 3D VTON methods: Do not support dynamic manipulation.
  - VITON: Relying primarily on 2D data, leading to inconsistent results.
  - ViViD: Requires continuous video input, increasing computational and memory demands.
  - GaussianEditor: Lacks precision in controlling detailed textures.
  - Video-based VTON methods: Lack of 3D structural awareness and high computational costs.
  - Image-based VTON methods: Inability to effectively handle temporal coherence across poses and viewpoints.
  - ViViD: Lacks explicit 3D structural reasoning, resulting in texture flickering.
  - IDM-VTON: Limited input sequence length degrades temporal continuity.
  - GaussianEditor: Requires per-frame optimization, leading to high computational costs.
  - IDM-VTON combined with AG: Exhibits noticeable temporal flickering and inconsistent texture patterns across frames.
  - ViViD: Lacks genuine 3D spatial reasoning and requires substantial computational resources.
  - N/A: N/A

### 3. Core Idea
- The proposed AvatarVTON framework utilizes a Reciprocal Flow Rectifier for optical flow correction and a Non-Linear Deformer for adaptive deformations, enhancing rendering quality and stability.

### 4. Method
- **Pipeline**: The framework integrates a prior-free optical flow correction strategy and a pose-aware Gaussian decomposition framework.
- **Architecture / Loss / Training**: Incorporates adversarial loss to recover fine visual details lost during consistency enforcement.
- **Complexity / Resources**: Achieves training in approximately three hours on an RTX 4090 GPU, significantly more efficient than competing methods.

### 5. Experiments
- **Datasets & Metrics**: Utilizes datasets from AvatarReX, ActorsHQ, DressCode, and VITON-HD, evaluating garment texture fidelity, human identity preservation, video temporal coherence, and overall realism.
- **Baselines**: Animatable Gaussians (3DGS-based counterpart), Diffusion models, GaussianEditor, GaussianEditor (3D editing method), GaussianVTON, IDM-VTON, IDM-VTON (2D image-based VTON), IDM-VTON + AG, IDM-VTON + LHM, IDM-VTON + SCARF, LHM (4D approach), N/A, SCARF (NeRF-based animatable human reconstruction), VITON, ViViD, ViViD (2D video-based VTON)
- **Main Results**: AvatarVTON consistently achieves higher scores than all competitors across all evaluation dimensions.
- **Ablations**: Demonstrates the impact of removing RFR and L_adv on texture clarity and color accuracy.
- **Limitations / Stress Tests**: Identifies limitations in handling out-of-distribution scenarios and the need for improved 3D perception capabilities.

### 6. Takeaways
- **Pros**: High-fidelity 4D virtual try-on from a single garment image., Mitigates view-pose coupling inconsistencies., Facilitates adaptive garment deformation transfer.
- **Cons**: Dependence on single 2D garment images may limit realism., Complexity in ensuring coherent avatar geometry., Potential challenges in generalizing across diverse view-pose combinations.
- **Future Work**: Explore integration with more complex garment dynamics., Investigate scalability to larger datasets., Enhance the framework for real-time applications.

</details>

## video understanding

### [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](http://arxiv.org/pdf/2510.08568v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Zero-Shot Manipulation

### 2. Motivation & Gaps
- The paper addresses the need for versatile manipulation techniques that can handle various object types and manipulation scenarios.

- **Related work challenges:**
  - Vision-Language-Action (VLA) models: Require vast quantities of robot-specific vision-language-action data that is difficult and expensive to collect.
  - Modular systems for task understanding and robot control: Translating semantic understanding into physical actions remains an open problem.
  - Prior work relying on self-collected data: Reintroduces the data bottleneck and limits generalizability and scalability.
  - Concurrent work on 6D pose extraction for demonstration-free manipulation: Relies on a rigid-body assumption, limiting applicability to a broader class of objects.
  - Flow-based manipulation methods: Require robot data or task-specific training, hindering generalization for zero-shot manipulation.
  - Prior work on object manipulation: Limited ability to handle complex object dynamics and real-time execution.
  - AVDC: Struggles with precise, long-horizon placements due to lack of 3D awareness.
  - VidBot: Fails when tasks require objectâ€“object relations and precise relative pose placement.
  - Diffusion Policy: Shows poor generalization from a few examples, especially in randomly sampled evaluations.
  - Veo: Closed-source model that produces video clips but lacks flexibility in task execution.
  - Wan2.1: Open-source model that performs poorly without a goal image.
  - Open-World Object Manipulation using Pre-Trained Vision-Language Models: Limited adaptability to new environments without retraining.
  - Local Policies Enable Zero-shot Long-horizon Manipulation: Difficulty in generalizing across diverse tasks.
  - Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations: Dependence on high-quality video generation.
  - Wan2.1: Limited support for all required modes in newer versions.
  - Veo: Lack of goal-image conditioning in the model.
  - MegaSaM: Ambiguity in estimated depth maps even after postprocessing.
  - Grounded-SAM2: Need for accurate object grounding in video sequences.
  - Trajectory Optimization: Finding optimal, collision-free, and smooth sequences of joint configurations.
  - Previous manipulation methods: Limited to specific object types and require extensive training data.

### 3. Core Idea
- NovaFlow enables zero-shot manipulation by generating actionable flows from videos, allowing manipulation of diverse objects without prior training.

### 4. Method
- **Pipeline**: The method involves generating actionable flows from videos and applying them to manipulate objects in real-time.
- **Architecture / Loss / Training**: Utilizes a combination of smoothness weight, collision penalty weight, and regularization for training.
- **Complexity / Resources**: The method is designed to be viewpoint-agnostic and can be deployed on various platforms after hand-eye calibration.

### 5. Experiments
- **Datasets & Metrics**: Real-world manipulation experiments were conducted to evaluate the effectiveness of NovaFlow.
- **Baselines**: AVDC, Data-dependent methods, Diffusion Policy, Existing video generation models, Existing video-based manipulation methods, Existing zero-shot manipulation techniques, Existing zero-shot techniques, Grounded-SAM2, Inverse Dynamics Model, MegaSaM, Model-based approaches for 6D pose extraction, Model-based planning approaches, Previous demonstration-free methods, Traditional manipulation methods, Traditional robotic manipulation methods, Veo, VidBot, Wan2.1
- **Main Results**: NovaFlow successfully manipulated rigid, deformable, and articulated objects across different scenarios.
- **Ablations**: Ablation studies demonstrate the importance of video quality and flow extraction techniques in achieving high performance.
- **Limitations / Stress Tests**: The method's performance is contingent on the quality of generated video flows and the accuracy of depth estimation.

### 6. Takeaways
- **Pros**: No need for task-specific tuning., Generalizable across different embodiments., State-of-the-art zero-shot performance.
- **Cons**: May still be limited by the quality of the generated videos., Relies on the accuracy of monocular depth estimation., Potential for generative artifacts affecting the final output.
- **Future Work**: Explore further applications in unstructured environments., Investigate improvements in object tracking., Enhance the framework with additional perception modules.

</details>

### [Where Have All the Kaczmarz Iterates Gone?](http://arxiv.org/pdf/2510.08563v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Numerical analysis of the Randomized Kaczmarz algorithm

### 2. Motivation & Gaps
- The paper addresses the convergence behavior of the Randomized Kaczmarz algorithm in the context of noisy linear systems, providing theoretical bounds and empirical validation.

- **Related work challenges:**
  - Strohmer and Vershynin (2009): Demonstrated that RK converges linearly in expectation for consistent systems but did not address inconsistent systems.
  - Needell (2014): Showed that RK iterates approach a ball centering at the least squares solution for consistent systems, but did not consider noise in both the coefficient matrix and the right-hand side.
  - Bergou et al. (2021): Provided convergence analysis for RK in general noisy cases, but the implications for inconsistent systems remain unclear.
  - Needell [15]: All works on noisy systems have used the LS solution of the underlying consistent noiseless system as a reference point.
  - Theorems 1.2 and 1.3: Require specific initial points that may not be satisfied in practice.
  - N/A: N/A
  - Previous studies on convergence of iterative methods: Lack of precise bounds for limit points in noisy environments
  - LIBSVM: A library for support vector machines: Handling noisy and inconsistent systems in practical applications.
  - Efficient and robust solution strategies for saddle-point systems: Developing algorithms that maintain performance in the presence of noise.
  - Numerical approximation of some linear stochastic partial differential equations driven by special additive noises: N/A
  - Online and batch supervised background estimation via l1 regression: N/A
  - New variants of the POCS method using affine subspaces of finite codimension with applications to irregular sampling: N/A

### 3. Core Idea
- The paper presents theoretical bounds for the convergence of the Randomized Kaczmarz algorithm applied to noisy linear systems and validates these bounds through numerical experiments.

### 4. Method
- **Pipeline**: The method involves generating noisy linear systems and applying the Randomized Kaczmarz algorithm to analyze convergence behavior.
- **Architecture / Loss / Training**: Theoretical bounds are derived and compared against empirical results from various datasets.
- **Complexity / Resources**: The experiments utilize synthetic data and real-world datasets from LIBSVM, with computational resources detailed in the code repository.

### 5. Experiments
- **Datasets & Metrics**: Experiments are conducted on synthetic data and real-world datasets from LIBSVM, measuring approximation errors and convergence rates.
- **Baselines**: Cyclic Kaczmarz algorithm, Least squares solutions of consistent systems, N/A, Previous convergence bounds, Previous works on Kaczmarz iterates, Randomized Kaczmarz algorithm, Standard iterative methods, Theoretical bounds from Theorem 5 and Corollary 2
- **Main Results**: The results show that the bounds are valid and that Corollary 2 exhibits a faster convergence rate than Theorem 5.
- **Ablations**: The impact of different reference points on convergence behavior is analyzed.
- **Limitations / Stress Tests**: The experiments highlight the limitations of the algorithm in terms of convergence horizons and the effects of noise.

### 6. Takeaways
- **Pros**: Provides a deeper understanding of the Kaczmarz algorithm in noisy environments., Offers practical insights into the algorithmâ€™s performance under realistic conditions., Establishes bounds on convergence that can inform future applications.
- **Cons**: Limited applicability to inconsistent systems without further modifications., Theoretical results may not fully capture practical performance in all scenarios., Dependence on noise levels may complicate real-world applications.
- **Future Work**: Explore modifications to the Kaczmarz algorithm for better performance on inconsistent systems., Investigate the impact of different types of noise on convergence behavior., Develop practical guidelines for applying RK in real-world scientific and engineering problems.

</details>

### [MultiCOIN: Multi-Modal COntrollable Video INbetweening](http://arxiv.org/pdf/2510.08561v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the need for advanced techniques in video generation, particularly focusing on masked generative models.

- **Related work challenges:**
  - Framer: Focused on respecting motion trajectories but lacks versatility and fine-grained user controls.
  - Framer: Achieving impressive results in controllable inbetweening using motion trajectories.
  - Stable Video Diffusion: Generating temporally coherent content using latent diffusion models.
  - Tune-a-video: Facilitating few-shot video generation by fine-tuning pre-trained image diffusion models.
  - Stable Video Diffusion (SVD): Maintaining temporal consistency across frames.
  - Diffusion Transformer (DiT): Modeling long-range dependencies and global context for fine details.
  - Framer: Relies solely on trajectory control, limiting flexibility in content editing.
  - Framer: Motion is introduced as an external condition, which may not effectively integrate with video features.
  - N/A: N/A
  - Make pixels dance: High-dynamic video generation: High dynamic range video generation remains a complex task requiring innovative approaches.
  - The unreasonable effectiveness of deep features as a perceptual metric: Utilizing deep features effectively for perceptual metrics in video generation is still a challenge.
  - Audio-driven neural gesture reenactment with video motion graphs: Integrating audio cues with video generation poses significant challenges in maintaining realism.

### 3. Core Idea
- The core idea is to leverage masked generative models to enhance the quality and realism of video generation.

### 4. Method
- **Pipeline**: The method involves a generative pipeline that utilizes masked inputs to predict video frames.
- **Architecture / Loss / Training**: The architecture employs a transformer-based model with specific loss functions tailored for video generation tasks.
- **Complexity / Resources**: The model requires substantial computational resources for training due to its complexity.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various video datasets and employ metrics such as PSNR and SSIM for evaluation.
- **Baselines**: Diffusion Transformer, Existing video generation models, Framer, N/A, Recent transformer-based models, Stable Video Diffusion, Traditional GAN-based approaches, Tune-a-video
- **Main Results**: The results demonstrate significant improvements in video quality compared to baseline models.
- **Ablations**: Ablation studies indicate the importance of masked inputs in enhancing model performance.
- **Limitations / Stress Tests**: Limitations include the model's dependency on large datasets and potential overfitting.

### 6. Takeaways
- **Pros**: Allows for versatile and fine-grained user controls., Achieves high-quality and fine-grained video interpolation., Empowers users to create smooth and plausible transitions.
- **Cons**: Requires advanced computational resources., Complexity in ensuring compatibility between controls and the generative model.
- **Future Work**: Explore further enhancements in user control mechanisms., Investigate additional modalities for video inbetweening.

</details>
