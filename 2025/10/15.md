# Daily Paper Digest Â· 2025-10-15
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars](http://arxiv.org/pdf/2510.12785v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video generation and reenactment using multi-view techniques

### 2. Motivation & Gaps
- The paper addresses the challenges in generating high-quality, animatable 4D avatars from 2D images and driving videos.

- **Related work challenges:**
  - Recent methods for avatar creation from single images: Lack of multi-view information and explicit 3D representation leading to degraded image quality.
  - 3D representation methods: Struggle to capture fine-grained facial dynamics and expression-dependent appearance.
  - 2D generative techniques: Exhibit weaker multi-view consistency due to lack of explicit 3D constraints.
  - Human4DiT [Shao et al. 2024a]: Generates body-pose controlled multi-view video but is limited to generating only a very small number of frames at a time.
  - Animatable 3D avatars: Most methods fail to capture fine-grained motion of structures such as the tongue and lips.
  - Monocular portrait video generation: Exhibits artifacts when the rendered viewpoint deviates significantly from the input image.
  - CogVideoX: Limited ability to handle multi-view video generation without large datasets.
  - FLAME: Inadequate representation of head pose and expression in generated videos.
  - Taubner et al. 2024: Challenges in capturing high-frequency temporal details in video frames.
  - MMVDM: Generating a large set of multi-view videos from a single reference image.
  - 3D Gaussian Splatting: Fitting a 4D avatar representation to generated multi-view videos.
  - Classifier-free Guidance: Improving predictions in a multi-view setting without losing view-specific information.
  - GAGAvatar: Inconsistent temporal quality in generated videos.
  - CAP4D: Fails to generate views behind the head and struggles with geometry consistency.
  - FYE + PanoHead: Artifacts due to two-step generation process.
  - CAP4D: Lower overall preference compared to MVP4D in user studies.
  - Hallo3: Struggles with fine-grained temporal details and extreme lighting conditions.
  - Multi-view CFG: Conventional CFG reduces performance compared to the proposed multi-view strategy.
  - Denoising Diffusion Probabilistic Models: N/A
  - Classifier-Free Diffusion Guidance: N/A
  - GaussianAvatar: Towards realistic human avatar modeling from a single video via animatable 3D Gaussians: N/A
  - CAP4D-MMDM: Output flickers due to reliance on an image diffusion model, resulting in high-frequency artifacts.
  - MVP4D: Previous methods fail to capture fine details and structures of the face and hair.
  - Baseline methods: Inability to generate 360-degree views without noticeable artifacts.
  - Previous methods for 4D avatar generation: Struggled with reconstructing complex geometries and dynamic effects.
  - Conventional CFG methods: Caused extreme contrast in generated images due to poor-quality predictions.
  - Existing video animation methods: Limited in generating high-quality, temporally coherent sequences.
  - GaussianAvatars: Limited ability to model detailed temporal dynamics.
  - CAP4D: Inadequate handling of back-facing views and excessive motion artifacts.
  - MMDM: Struggles with generating high-quality multi-view outputs.
  - Follow-Your-Emoji (FYE): Limited to monocular settings and requires extension to multi-view.
  - PanoHead: Static multi-view generation model that needs integration with dynamic video generation.
  - DISK keypoint detection: Requires accurate matching and triangulation for 3D consistency.
  - Deng et al. 2024: Distilling the model into more efficient feed-forward methods.

### 3. Core Idea
- The proposed method combines multiple video generation techniques to create realistic 4D avatars that can express a range of emotions and movements.

### 4. Method
- **Pipeline**: The method involves generating a reference video, extending it to multi-view, and applying a multi-view face tracker for animation.
- **Architecture / Loss / Training**: Utilizes various metrics such as PSNR, SSIM, and JOD for training and evaluation.
- **Complexity / Resources**: 8x NVIDIA H100 with activation checkpointing to reduce VRAM.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on the FFHQ dataset and Nersemble evaluation dataset using metrics like PSNR, SSIM, and identity preservation.
- **Baselines**: Animatable 3D avatars, CAP4D, CAP4D-MMDM, CAT3D, CAT4D, CogVideoX, Conventional CFG methods, Existing multi-view video synthesis approaches, FLAME, GAGAvatar, Human4DiT, HunyuanPortrait, MVP4D-MMVDM, Monocular portrait video generation, N/A, Pippo, Portrait4D-v2, Previous 3D avatar reconstruction methods, Previous multi-view avatar generation techniques, Single-view video generation methods, Single-view video generation techniques, Stage 1, Stage 2, Stage 3, Taubner et al. 2024, VOODOO XP, VOODOO3D
- **Main Results**: Stage 3 achieves the best PSNR of 23.39.
- **Ablations**: Ablation studies show that limiting stage 1 training to mode 1 improves PSNR.
- **Limitations / Stress Tests**: Iterated generation leads to image quality deterioration.

### 6. Takeaways
- **Pros**: Generates high-fidelity, animatable avatars from a single image., Produces temporally consistent videos across multiple viewpoints., Significantly improves realism and 3D consistency compared to previous methods.
- **Cons**: Training requires substantial computational resources., Scarcity of multi-view portrait video data poses challenges.
- **Future Work**: Explore further enhancements in animation fidelity., Investigate additional applications in virtual environments., Develop methods to reduce training data requirements.

</details>

### [Wavefront Coding for Accommodation-Invariant Near-Eye Displays](http://arxiv.org/pdf/2510.12778v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Addressing Vergence-Accommodation Conflict in 3D Displays

### 2. Motivation & Gaps
- The paper addresses the inherent vergence-accommodation conflict (VAC) in conventional 3D displays, which can lead to visual discomfort and fatigue.

- **Related work challenges:**
  - Accommodation-enabling displays: Delivering close-to-natural viewing experience while recreating near-correct retinal blur.
  - Accommodation-invariant displays: Coupling vergence with accommodation by removing retinal defocus blur completely.
  - Holographic NEDs: Limited eyebox and presence of speckle noise.
  - Maxwellian view displays: Reduced eyebox size due to light being funneled through a single pinhole.
  - Adaptive optics in projectors: Trade-off between extended depth range and spatial resolution.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Conventional stereoscopic display: Limited accommodation depth range and image quality
  - AI-NED from Konrad et al. [5]: Trade-off between resolution and depth
  - N/A: N/A
  - Vergenceâ€“accommodation conflicts hinder visual performance and cause visual fatigue: Existing methods do not effectively eliminate VAC.
  - Visual discomfort and visual fatigue of stereoscopic displays: A review: Lack of comprehensive solutions for visual comfort in 3D displays.
  - Accommodation-invariant computational near-eye displays: Previous designs often rely on complex adaptive optics.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The proposed AI-NED design utilizes a DOE-based architecture to eliminate retinal defocus blur and couple accommodation with vergence, relying solely on binocular disparity.

### 4. Method
- **Pipeline**: The method involves wavefront coding to co-optimize a novel DOE design for accommodation invariance and a pre-processing module to enhance perceived image quality.
- **Architecture / Loss / Training**: The architecture includes a pre-processing CNN (P-CNN) that digitally encodes the AI image, and a physics-based differentiable simulation model to propagate the image through the display and viewer optics, comparing the output with a ground-truth retina image using pixel-to-pixel and structural similarity losses.
- **Complexity / Resources**: The method optimizes static optics, avoiding the need for complex adaptive optics or gaze tracking.

### 5. Experiments
- **Datasets & Metrics**: Simulations and optical measurements were used to evaluate the proposed architecture.
- **Baselines**: AI-NED from Konrad et al. [5], Accommodation-enabling displays, Accommodation-invariant displays, Conventional 3D display methods, Conventional NED setups, Conventional approach, Conventional stereoscopic display, Holographic NEDs, Maxwellian view displays, N/A
- **Main Results**: The proposed architecture can extend the depth of focus for up to four diopters.
- **Ablations**: The impact of the pre-processing network was analyzed, showing improved image sharpness.
- **Limitations / Stress Tests**: Future work will include user studies to measure accommodation responses and assess visual comfort.

### 6. Takeaways
- **Pros**: Eliminates retinal defocus blur, improving visual comfort., Joint optimization of optics and CNN enhances performance., Demonstrated effectiveness through optical measurements.
- **Cons**: Requires complex setup and fabrication of custom optical elements., Potential challenges in achieving compact form factor., Dependence on accurate gaze tracking for some methods.
- **Future Work**: Explore further optimizations in real-world conditions., Investigate integration with other display technologies., Develop more compact and user-friendly designs.

</details>

### [Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective](http://arxiv.org/pdf/2510.12763v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Brain age estimation from MRI scans

### 2. Motivation & Gaps
- The study aims to estimate the age of healthy subjects using T1-weighted MRI scans, addressing the need for accurate brain age prediction methods.

- **Related work challenges:**
  - Brain Age Gap Prediction Models: Methodological obscurities due to lack of deterministic relationships affecting clinical significance.
  - Data-driven ML algorithms using structural MRI: Identifying neurodegeneration biomarkers effectively.
  - Brain age gap prediction algorithms: Correcting for age biases in predictions.
  - Various studies on brain age gap prediction: Divergence in underlying ML principles and methodological obscurities in âˆ†-Age prediction.
  - [32]: Divergence in ML principles for âˆ†-Age prediction.
  - [34]: Moderate fit on chronological age leading to better clinical utility.
  - [35]: No significant correlation between chronological age prediction accuracy and clinical utility of âˆ†-Age.
  - PCA-regression model: Instability in addressing the requirements of qualitative evaluation of âˆ†-Age.
  - Deep learning models: Lack of explainability in predictions, failing to meet the requirements of qualitative evaluation.
  - GSP and GNNs: An overview: Existing theoretical properties of GNNs are agnostic to the spatial and spectral characteristics of data-driven graphs.
  - Morphometric similarity networks: Generalizing anatomical covariances to include multiple information modalities within structural MRI.
  - N/A: Refined mathematical understanding of GNNs instantiated on covariance matrices.
  - PCA-regression in brain age gap prediction: PCA-driven approaches are prone to unstable or irreproducible inference outcomes due to stochastic perturbations in the covariance eigenspectrum.
  - PCA-regression: Exhibits instability when principal components are re-evaluated using perturbed sample covariance matrices.
  - Sparse VNNs: Need for better quality covariance matrices while preserving stability in high-dimensional neuroimaging settings.
  - Transferability of VNNs: Ensuring VNNs retain performance across datasets of different dimensionalities without retraining.
  - Previous studies on brain age prediction: Lack of interpretability in the models used, leading to ambiguities in understanding the contributions of different brain regions.
  - N/A: N/A
  - N/A: Performance-driven methods are inadequate for the practical viability of brain age gap prediction.
  - N/A: N/A
  - The effect of the APOE genotype on individual BrainAGE in normal aging, mild cognitive impairment, and Alzheimerâ€™s disease: Understanding the genetic influences on brain aging.
  - Prediction of brain age suggests accelerated atrophy after traumatic brain injury: Identifying the impact of traumatic events on brain aging.
  - Explainable brain age gap prediction in neurodegenerative conditions using covariance neural networks: Developing interpretable models for brain age prediction.
  - Optuna: A next-generation hyperparameter optimization framework: N/A
  - Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI: N/A
  - Characteristics of subjective cognitive decline associated with amyloid positivity: N/A
  - Plasma and CSF neurofilament light: relation to longitudinal neuroimaging and cognitive measures: N/A
  - Neuropathological stageing of Alzheimer-related changes: N/A
  - fsbrain: an R package for the visualization of structural neuroimaging data: N/A

### 3. Core Idea
- Utilizing kernel methods to analyze T1-weighted MRI scans for estimating brain age.

### 4. Method
- **Pipeline**: Data preprocessing, feature extraction using kernel methods, and age estimation.
- **Architecture / Loss / Training**: The VNN model consists of 2 layers with 2 and 6 filter taps respectively, trained on the OASIS-3 dataset with 22,570 learnable parameters.
- **Complexity / Resources**: The model complexity is managed through hyperparameter optimization using the Optuna package.

### 5. Experiments
- **Datasets & Metrics**: Utilized various datasets of T1-weighted MRI scans to validate the model's performance.
- **Baselines**: Convolutional Neural Networks, Convolutional neural networks, Deep learning models, Deep learning-based approaches, Existing brain age gap prediction models, N/A, PCA-regression, Principal Component Analysis-based Regression, Sparse VNNs, Standard age prediction models without anatomical interpretability, Standard regression models, Support Vector Regression, Support vector regression, Traditional ML methods, Traditional ML-driven methods, Traditional MRI analysis methods, Traditional age estimation methods
- **Main Results**: The proposed method shows improved accuracy in estimating brain age compared to existing methods.
- **Ablations**: Further analysis on the impact of different features extracted from MRI scans on the prediction accuracy.
- **Limitations / Stress Tests**: Robustness of age prediction models to factors such as distribution shifts is key to achieve reproducible outcomes.

### 6. Takeaways
- **Pros**: Improved interpretability of brain age gap predictions using GSP., Potential for better patient outcomes through automated MRI analysis., Enhanced understanding of neurodegeneration through interdisciplinary approaches.
- **Cons**: Methodological challenges in clinical deployment., Dependence on the quality of neuroimaging data., Complexity of GSP methods may hinder widespread adoption.
- **Future Work**: Exploration of domain-specific foundation models in neuroimaging., Further studies on neurodegenerative disease subtyping., Integration of GSP with other machine learning techniques for improved outcomes.

</details>

## Gaussian Splatting

### [Performance of Gaussian Boson Sampling on Planted Bipartite Clique Detection](http://arxiv.org/pdf/2510.12774v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the effectiveness of Gaussian Boson Sampling (GBS) in detecting planted bicliques.

### 2. Motivation & Gaps
- The study aims to determine if GBS can provide a quantum computational advantage in the planted biclique detection problem.

- **Related work challenges:**
  - Previous studies on GBS and its applications in graph theory.: Lack of rigorous analysis on GBS's effectiveness for planted biclique detection.
  - Heuristic and experimental approaches to GBS.: Limited understanding of the statistical properties of GBS outputs in the context of planted biclique detection.
  - Planted Clique Problem: The planted biclique problem is at least as hard as the planted clique problem, which complicates detection.
  - Degree-based Heuristics: Degree-based heuristics fail to reliably detect planted biclique nodes when K is much smaller than âˆšn.
  - Brute-force Search: The best-known algorithm for the planted biclique problem remains a brute-force search, which is inefficient.
  - N/A: N/A
  - Previous studies on GBS sampling: Lack of precise probabilistic reasoning about edge and matching structures.
  - Prior studies on GBS and its applications in combinatorial problems.: Limited understanding of the statistical properties of vertex weights in GBS.
  - N/A: Detecting planted bicliques in the conjectured hard regime remains infeasible under simple weight-based strategies.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The analysis focuses on the vertex weight statistic derived from GBS outputs to assess its ability to distinguish planted biclique vertices from non-planted ones.

### 4. Method
- **Pipeline**: Theoretical analysis of vertex weights using probabilistic and combinatorial tools.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The approach operates in quasi-polynomial time, and no polynomial-time algorithm has been proven for detecting planted cliques in the specified regime.

### 5. Experiments
- **Datasets & Metrics**: The experiments involve bipartite ErdÅ‘sâ€“RÃ©nyi graphs and Gaussian states.
- **Baselines**: Brute-force search, Classical algorithms for biclique detection, Classical algorithms for planted biclique detection, Degree-based heuristics, N/A, Previous GBS sampling methods
- **Main Results**: GBS does not provide a significant advantage for planted biclique detection in the regime where K is small compared to âˆšn.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The analysis assumes ideal GBS devices, which may not reflect practical implementations.

### 6. Takeaways
- **Pros**: Provides a rigorous statistical foundation for understanding GBS sampling frequencies., Quantifies the bias introduced by planted structures in GBS outputs., Highlights the potential need for advanced GBS-based algorithms.
- **Cons**: Detection of planted biclique nodes remains unreliable in certain regimes., The computational complexity of the planted biclique problem is not resolved., Current methods may not leverage the full potential of GBS.
- **Future Work**: Further investigation into advanced GBS-based algorithms., Exploration of other quantum approaches for planted biclique detection., Study of the implications of GBS on broader computational problems.

</details>

### [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](http://arxiv.org/pdf/2510.12768v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dynamic 3D scene reconstruction and novel view synthesis

### 2. Motivation & Gaps
- Advances in dynamic 3D scene reconstruction and novel view synthesis from monocular videos, addressing challenges in AR/VR and physical scene understanding.

- **Related work challenges:**
  - Dynamic Gaussian Splatting: Existing methods optimize all Gaussian primitives uniformly, leading to motion drifts under occlusion and degraded synthesis at unseen views.
  - SoM (Shape-of-Motion): Struggles on extreme novel views far from the input trajectory.
  - MoSca: Lacks structured propagation for motion guidance.
  - SE-GS (Zhao et al., 2025): Overfitting on reconstructing static scenes.
  - Vanilla Dynamic Gaussian Splatting (Luiten et al., 2024): Fragility under occlusion and extreme novel viewpoints.
  - Dynamic Gaussian Splatting variants (Lei et al., 2025; Wang et al., 2025a): Reliance on unstable 2D priors such as depth and optical flow.
  - Huang et al., 2024: Existing methods fail under occlusion or extreme viewpoints.
  - Lei et al., 2025: Distance-based heuristics cannot robustly handle long-range geometric and motion dependencies.
  - MoSca (Lei et al., 2025): Attempts to capture correlation but does not address uncertainty effectively.
  - SoM (Wang et al., 2025a): Widely adopted but lacks robustness under extreme viewpoint shifts.
  - MoSca (Lei et al., 2025): Represents the current state of the art but still exhibits distortion and artifacts under challenging conditions.
  - MoSca (Lei et al., 2025): Inadequate handling of uncertainty in dynamic scenes.
  - SoM (Wang et al., 2025a): Limited performance in extreme viewpoint shifts.
  - N/A: N/A
  - Luiten et al. (2024): Dynamic scene representation using 3D Gaussians with time-varying motion.
  - Huang et al. (2024): Incorporating motion locality in Gaussian splatting.
  - Stearns et al. (2024): Ensuring geometry preservation in dynamic scenes.
  - SoM (Wang et al., 2025a): Limited ability to handle dynamic scenes effectively.
  - MoSca (Lei et al., 2025): Inconsistencies in Gaussian scale and distribution.
  - DyCheck (Gao et al., 2022): Limited view shifts in the dataset.
  - DA VIS (Perazzi et al., 2016): Insufficient realism in dynamic object representation.
  - MoSca: Performance drops compared to full model when using under-trained base model.

### 3. Core Idea
- Modeling uncertainty to improve synthesis under extreme viewpoints for robust 4D modeling.

### 4. Method
- **Pipeline**: Key node selection strategy and uncertainty estimation for dynamic scene reconstruction.
- **Architecture / Loss / Training**: Utilizes a graph model with 4D Gaussians for tracking and geometry information.
- **Complexity / Resources**: Runtime analysis shows efficiency with fast rendering speed (> 60 FPS) and reasonable training times (~4 sec/image).

### 5. Experiments
- **Datasets & Metrics**: Evaluated on DyCheck dataset with metrics PSNR, SSIM, and LPIPS.
- **Baselines**: 4D-Rotor, 4DGS (Wu et al., 2024), DynIBaR (Li et al., 2023), Dynamic Gaussians (Luiten et al., 2024), HyperNeRF (Park et al., 2021), Marbles, MoDec-GS (Kwak et al., 2025), MoSca, MoSca (Lei et al., 2025), N/A, NeRF-based methods, SC-GS (Huang et al., 2024), SE-GS, SoM, SoM (Wang et al., 2025a), T-NeRF (Gao et al., 2022), USPLAT4D, Vanilla Dynamic Gaussian Splatting
- **Main Results**: Ours (full time) achieves PSNR of 19.63, SSIM of 0.715, and LPIPS of 0.249.
- **Ablations**: Ablation studies show impact of key node selection strategy on performance metrics.
- **Limitations / Stress Tests**: Model struggles with incorrect motion in the 4D prior and can fail under extreme conditions.

### 6. Takeaways
- **Pros**: Improved stability in motion estimates under occlusion., High-quality synthesis at extreme viewpoints., Model-agnostic framework that can enhance existing methods.
- **Cons**: Still fragile under extreme novel viewpoints., Potential overfitting in certain scenarios.
- **Future Work**: Explore further integration with other dynamic reconstruction methods., Investigate real-time applications in augmented reality., Enhance uncertainty estimation techniques.

</details>

### [A High-Level Feature Model to Predict the Encoding Energy of a Hardware Video Encoder](http://arxiv.org/pdf/2510.12754v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Energy demand prediction

### 2. Motivation & Gaps
- The paper addresses the need for accurate energy demand predictions in hardware video decoders to optimize performance and reduce energy consumption.

- **Related work challenges:**
  - Various models for predicting energy demand of software video encoders: Limited research on energy consumption prediction of hardware video encoders.
  - EichermÃ¼ller et al. provide an encoding time and energy model for the SVT-A V1 video codec.: Their model is robust for software implementations but does not address hardware encoders.
  - Herglotz et al. introduced a High-Level feature model for H.265 hardware decoders.: The model cannot be directly applied to hardware encoders due to different available features.
  - [17], [21]: Previous works focused on decoding energy modeling, leaving a gap in encoding energy predictions.
  - [20]: Linear Regression struggles with measurement noise, which affects prediction accuracy.
  - N/A: N/A
  - Encoding time and energy model for SVT-A V1 based on video complexity: Lack of comprehensive models that account for video complexity in energy predictions.
  - VEEP: Video encoding energy and CO2 emission prediction: Challenges in accurately predicting CO2 emissions alongside energy consumption.
  - Memory energy consumption analyzer for video encoder hardware architectures: Insufficient analysis of memory energy consumption in existing architectures.

### 3. Core Idea
- The core idea is to utilize software profiling techniques to estimate the energy demand of hardware video decoders, improving prediction accuracy.

### 4. Method
- **Pipeline**: The method involves profiling software decoders to gather energy consumption data, which is then used to model hardware decoder energy demands.
- **Architecture / Loss / Training**: The model uses a Gaussian process with a linear basis function and an exponential kernel function, trained with 10-fold cross-validation to prevent overfitting.
- **Complexity / Resources**: The approach requires access to profiling tools and hardware specifications for accurate modeling.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various datasets to validate the energy prediction models, focusing on accuracy and efficiency metrics.
- **Baselines**: Existing energy prediction models, Existing models for software video encoders, GPR, LR, Linear Regression, Software-based energy estimations
- **Main Results**: The proposed method shows significant improvements in prediction accuracy compared to baseline models.
- **Ablations**: The impact of each feature on accuracy was tested by removing features and setting them to constant values.
- **Limitations / Stress Tests**: The study acknowledges limitations in the generalizability of the results across different hardware architectures.

### 6. Takeaways
- **Pros**: Accurate prediction of encoding energy for hardware video encoders., Can be used for prior estimation of energy required for various spatial resolutions., Addresses gaps in existing models for hardware encoders.
- **Cons**: Limited to specific encoding configurations (P-frames and single keyframe)., Does not account for all possible video encoding scenarios., Requires further validation across different hardware platforms.
- **Future Work**: Expand the model to include more encoding standards and presets., Investigate the impact of additional high-level features on energy prediction., Explore integration with real-time encoding systems for practical applications.

</details>

## avatar

### [InfiniHuman: Infinite 3D Human Creation with Precise Control](http://arxiv.org/pdf/2510.11650v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D human generation and reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of generating high-quality 3D human models using multi-modal inputs, including text, SMPL normal maps, and clothing images.

- **Related work challenges:**
  - Score Distillation Sampling (SDS): Long optimization times, limited visual fidelity, and lack of precise control over attributes.
  - Liao et al. 2025: Limited controllability in generating avatars from user-defined conditions.
  - Cao et al. 2023: Inability to condition on detailed clothing items.
  - Zhuang et al. 2025: Existing datasets lack fine-grained annotations essential for precise control.
  - FLUX: Produces images with dramatic perspective and complex lighting, which are suboptimal for 3D reconstruction tasks.
  - OminiControl: Requires paired image-scan training data for garment extraction.
  - NLF: Aligning SMPL parameters accurately with both overall pose and pixel-level features.
  - MVDream: Low-resolution constraints leading to blurry detailed features.
  - Human-3Diffusion: Inconsistencies across views in generated images.
  - OminiControl2: Need for precise control over fine-grained attributes in avatar generation.
  - MVDream: Limited generation speed and quality compared to optimization-based methods.
  - SPAD: Requires significant time for generation despite achieving higher quality.
  - DreamAvatar: Suffers from unnatural saturation and poor text-following ability.
  - Gen-Schnell: Cannot generate faithful details such as face due to low resolution.
  - Existing avatar generation methods: Limited in visual quality and speed compared to the proposed method.
  - Multi-view mesh carving: Can cause texture artifacts in self-occluded parts of the avatar.
  - HumanNorm, CVPR2024: wrong color, unnatural limb, wrong geometry
  - HumanGaussian, CVPR2024: wrong color, degenerating geometry
  - AvatarVerse, AAAI2024: Janus Problem, incorrect color, geometry artifacts
  - InfiniHuman-GenHRes: unnatural saturation, wrong color
  - DreamAvatar, CVPR2024: failed to generate avatar
  - FLUX: Complex lighting degrading multi-view generation
  - OpenPose: Real-time multi-person 2D pose estimation limitations
  - DreamAvatar: Text-and-shape guided generation complexities
  - HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion: Achieving high fidelity in dynamic human representations.
  - 3D Gaussian Splatting for Real-Time Radiance Field Rendering: Real-time rendering of complex 3D scenes.
  - CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes: Integrating text-driven methods with 3D human animation.
  - DreamFusion and its variants: SDS-based methods often suffer from slow convergence and visual artifacts such as over-smoothed textures or structural inaccuracies.
  - Chupa: Fails to generalize to complex text prompts and does not support specific clothing images as conditioning input.
  - IDOL: Generated results often exhibit noticeable view inconsistencies and temporal artifacts.
  - IDOL [Zhuang et al. 2025]: Generated results often exhibit noticeable view inconsistencies and temporal artifacts due to the neighbor-only attention mechanism.
  - IDOL Dataset: Achieving better visual realism and multi-view consistency.
  - MVDream: Handling wrong geometry in 3D rendering.
  - MVDream: Originally designed to condition only on text, lacking support for additional modalities like clothing images.
  - PSHuman: Generates single blurry head-view images instead of high-resolution multi-view images.
  - OminiControl2: Difficulty in fusing spatially aligned and non-aligned conditions for image generation.
  - PSHuman: Limited to generating low-resolution single head views.
  - Human-3Diffusion: Inconsistency between orthographic and perspective image generation.

### 3. Core Idea
- The proposed method enhances 3D human generation by fine-tuning a multi-view diffusion model to utilize additional modalities, improving realism and detail in generated images.

### 4. Method
- **Pipeline**: The pipeline involves fine-tuning a multi-view diffusion model on a dataset of human scans and using a Splat Decoder for 3D Gaussian Splat representations.
- **Architecture / Loss / Training**: The model employs spectral normalization and a combination of perceptual loss (LPIPS) and â„“2 reconstruction loss during training.
- **Complexity / Resources**: Training is conducted on 8 NVIDIA A100 GPUs with a batch size of 256 and an initial learning rate of 5 Ã— 10âˆ’4.

### 5. Experiments
- **Datasets & Metrics**: The model is trained on InfiniHumanData and evaluated through user studies for realism assessment.
- **Baselines**: 3D Gaussian Splatting, AvatarVerse, AvatarVerse, AAAI2024, CLIP-Actor, Cao et al. 2023, Chupa, DreamAvatar, DreamAvatar, CVPR2024, DreamFusion, Existing state-of-the-art methods, FLUX, Gen-HRes, Gen-Schnell, Human-3Diffusion, HumanGaussian, HumanGaussian, CVPR2024, HumanNorm, HumanNorm, CVPR2024, HumanRF, IDOL, InfiniHuman-GenHRes, Liao et al. 2025, MVDream, NLF, OminiControl, OminiControl2, OpenPose, PSHuman, SPAD, TADA, Zhuang et al. 2025
- **Main Results**: The fine-tuned model demonstrates superior realism in generated images, receiving 765 votes compared to 746 for scan subjects in user studies.
- **Ablations**: Ablation studies demonstrate the importance of incorporating 2D joint refinement and multi-view generation.
- **Limitations / Stress Tests**: The model may struggle with extreme poses or occlusions that affect clothing visibility.

### 6. Takeaways
- **Pros**: Democratizes high-quality avatar generation., Provides fine-grained control at infinite scale., Offers a practical and affordable solution.
- **Cons**: Dependence on existing datasets may restrict the diversity of generated outputs., Complexity of the model may require significant computational resources., Potential challenges in aligning generated images with real-world variations.
- **Future Work**: Public release of the automatic data generation pipeline., Comprehensive dataset InfiniHumanData., Generative models InfiniHumanGen.

</details>

### [Building and Evaluating a Realistic Virtual World for Large Scale Urban Exploration from 360Â° Videos](http://arxiv.org/pdf/2510.11447v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Emulating 6-DoF perception in 360Â° panoramas

### 2. Motivation & Gaps
- The paper addresses the challenge of navigating 360Â° panoramas with a focus on enhancing user experience through avatar-assisted navigation.

- **Related work challenges:**
  - Deep learning techniques for automatic 3D model generation: Heavily rely on the accuracy of training datasets and face significant challenges in terms of generalization and computational efficiency.
  - Google Street View and MovieMap: Limits interactivity and makes it difficult to create social experiences as users can only passively watch videos or images.
  - GSV and MovieMap: Limited user interaction and exploration capabilities.
  - Geollery: Reduction of realism due to texture changes based on user position.
  - Tourgether360: Users can only move along the camera trajectory, limiting exploration.
  - MovieMap: Limited interaction in video-based environments.
  - Existing deep video completion models: Inability to handle distortion in ERP format of 360Â° videos.
  - GSV: Limited interaction and navigation capabilities.
  - MovieMap: Lacks immersive experience compared to 360RVW.
  - Previous studies on spatial cognition in virtual worlds: Understanding how different interfaces impact memory and navigation.
  - GSV: Users had difficulty navigating due to the need to search for locations of placed 360Â° images.
  - MovieMap: Participants criticized the cumbersome adjustment of video playback speed, which hindered exploration.
  - 360RVW: Operating an avatar with a gamepad was seen as laborious and detracted from the experience.
  - Multi-view 3D reconstruction based on deep learning: A survey and comparison of methods: Lack of effective methods for real-time navigation in immersive environments.
  - Building Movie Map - A Tool for Exploring Areas in a City: Limited interactivity and user engagement in existing navigation tools.
  - N/A: N/A

### 3. Core Idea
- The core idea is to utilize avatars to assist users in navigating 360Â° environments, providing a more immersive and interactive experience.

### 4. Method
- **Pipeline**: The method involves integrating avatar navigation with existing 360Â° panorama technologies.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The implementation requires moderate computational resources for real-time rendering.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize a dataset of 360Â° panoramas and measure user engagement and navigation efficiency.
- **Baselines**: 360RVW, Avatar360, Existing avatar-based navigation systems, Existing video completion models, GSV, Google Street View, MovieMap, N/A, Traditional 360Â° navigation methods
- **Main Results**: The results indicate a significant improvement in user navigation experience and engagement when using avatar-assisted navigation.
- **Ablations**: Evaluation of virtual collision detection and video completion techniques comparing with and without rotation.
- **Limitations / Stress Tests**: The study acknowledges limitations in scalability and the need for further testing in diverse environments.

### 6. Takeaways
- **Pros**: Allows for interactive exploration of large urban environments., Reduces the need for manual creation of complex 3D models., Provides a highly immersive experience using 360Â° videos.
- **Cons**: Still relies on the quality of the 360Â° videos., Potential limitations in areas without sufficient video coverage.
- **Future Work**: Explore further improvements in video completion techniques., Investigate the integration of more interactive elements., Expand the system to cover more diverse urban environments.

</details>

## video understanding

### [Prethermal gauge structure and surface growth in $\mathbb{Z}_2$ lattice gauge theories](http://arxiv.org/pdf/2510.12800v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate long-time mean-field dynamics of large-scale spin systems

### 2. Motivation & Gaps
- The study focuses on the long-time dynamics in small systems of L=2Ã—2 plaquettes with periodic boundary conditions, comparing mean-field dynamics, DTWA, and ED.

- **Related work challenges:**
  - Lattice gauge theories (LGTs): Lack of a universal description of thermalization in LGTs, especially for kinetically constrained models.
  - Quantum simulators: Characterizing the systemâ€™s dynamical response to gauge defects is essential for scalability.
  - Thermalization pathways: Understanding how local symmetries influence thermalization in LGTs.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Small-scale ED predictions: Limited scalability to large-scale quantum simulators.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Rydberg tweezer arrays: Notorious numerical challenges of quantum many-body systems
  - DTWA: Fails to describe the quantum case and is outperformed by mean-field dynamics
  - Gauge theories: Prethermalization strongly depends on the interplay between scrambling of fluctuations and locally constrained dynamics
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The paper discusses the thermalization dynamics in a Rydberg model without dynamical matter, highlighting the independence of certain parameters from disorder strength and the emergent Z2 gauge structure.

### 4. Method
- **Pipeline**: Comparison of classical mean-field dynamics, DTWA, and ED for models with and without dynamical matter.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Study of small systems with L=2Ã—2 plaquettes

### 5. Experiments
- **Datasets & Metrics**: Experimental relevance in state-of-the-art Rydberg tweezer experiments with hundreds or thousands of qubits.
- **Baselines**: Classical mean-field dynamics, DTWA, ED, Exact diagonalization (ED), N/A, Semi-classical discrete time Wigner approximation (DTWA), Small-scale ED predictions
- **Main Results**: The height of the plateau and the critical time after which the plateau decays are independent of the disorder strength.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The system sizes are too small to analyze surface growth.

### 6. Takeaways
- **Pros**: Provides a testbed for quantum simulators., Reveals universal features of thermalization in LGTs., Demonstrates the experimental feasibility of the model.
- **Cons**: DTWA does not capture the prethermal plateau., Thermalization pathways are complex and not fully understood., Limited to specific configurations and conditions.
- **Future Work**: Explore thermalization in more complex gauge theories., Investigate the role of local symmetries in other many-body systems., Develop methods to better capture prethermal dynamics in simulations.

</details>

### [Detect Anything via Next Point Prediction](http://arxiv.org/pdf/2510.12798v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- chat detection and segmentation

### 2. Motivation & Gaps
- The paper addresses the need for improved chat detection and segmentation in various applications.

- **Related work challenges:**
  - Grounding DINO: Low recall rate and duplicate predictions.
  - Qwen2.5-VL: Struggles with precise object localization.
  - Existing MLLM approaches: Inability to effectively manage coordinate prediction and spatial representation.
  - Traditional object detection models: Limited language understanding capabilities.
  - T-Rex2: Certain objects are inherently difficult to express through text alone.
  - RexSeek: Manual annotation is labor-intensive and unscalable.
  - Supervised Fine-Tuning (SFT): SFT introduces geometric discretization issues and behavioral regulation deficiencies.
  - Reinforcement Learning with GRPO: The model struggles with output quantity regulation during inference.
  - Faster RCNN: Limited ability to handle diverse object categories and complex scenes.
  - DETR: Struggles with spatial alignment and redundancy in predictions.
  - Grounding DINO: Not trained on COCO, limiting its effectiveness in common object detection tasks.
  - Faster RCNN: Limited to closed-set detection.
  - Grounding DINO: Not trained on COCO, affecting performance.
  - MLLMs like SEED1.5-VL: Performance drop when handling multiple categories simultaneously.
  - MLLMs: Struggle with dense and tiny object detection, leading to large-box predictions and structured duplicate predictions.
  - T-Rex2: Traditional expert models still outperform Rex-Omni in overall performance.
  - MLLMs: Struggle with dense or small-scale instances in object pointing tasks.
  - ScreenSpot-Pro: Requires high precision in localizing UI elements under challenging visual conditions.
  - Rex-Omni: Performance gap compared to closed-set models in layout grounding.
  - PaddleOCR: Handling diverse OCR challenges across different datasets.
  - X-Pose: Generalizes poorly to AP10K despite strong performance on COCO.
  - Rex-Omni-SFT: Limited performance compared to the full Rex-Omni model.
  - GRPO: Needs to effectively mitigate repetitive predictions learned during SFT.
  - SFT model: Exhibits substantial repeated predictions and large box predictions.
  - GRPO model: Shows minimal gains in performance but effectively reduces duplicate outputs.
  - Traditional regression-based object detection methods: Limited performance in complex scenarios and reliance on predefined categories.
  - Open-set object detection methods: Struggles with complex descriptions and limited language understanding.
  - MLLM-based object detection methods: Shallow language understanding leading to difficulties in context-rich scenarios.
  - Pix2Seq: Existing methods frequently suffer from limitations such as low recall rates, coordinate drift, and spurious duplicate predictions.
  - Kosmos-2, Shikra, Ferret, CogVLM: MLLMs often struggle with the fine-grained spatial precision required for object detection.
  - Segment Anything: Limited generalization across diverse object categories.
  - MDETR: Inefficiency in multi-modal understanding.
  - Grounded Language-Image Pre-training: Inadequate performance in open-set scenarios.
  - SSD: Single Shot Multibox Detector: Limited to predefined categories.
  - DeepFashion: Powering Robust Clothes Recognition: Requires rich annotations for effective performance.
  - Few-shot Object Counting and Detection: Struggles with generalization to unseen categories.
  - Icdar2017 competition on reading chinese text in the wild: Limited performance in complex environments.
  - Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text: Difficulty in handling arbitrary shapes and sizes of text.
  - Efficientdet: Scalable and efficient object detection: Balancing efficiency and accuracy in detection tasks.
  - Dino: Detr with improved denoising anchor boxes for end-to-end object detection: Challenges in object detection accuracy and efficiency.
  - Ferret-v2: An improved baseline for referring and grounding with large language models: Limitations in grounding and referring tasks.
  - mixup: Beyond empirical risk minimization: Issues with empirical risk minimization in model training.

### 3. Core Idea
- The core idea is to enhance chat detection and segmentation using a large language model.

### 4. Method
- **Pipeline**: The method involves a multi-stage pipeline for processing chat data.
- **Architecture / Loss / Training**: Utilizes a novel architecture with specific loss functions tailored for chat tasks.
- **Complexity / Resources**: The model requires significant computational resources for training and inference.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on various datasets with standard metrics for evaluation.
- **Baselines**: DAB-DETR-R50, DETR, DETR-R50, DINO, DINO-R50, DINO-ResNet50, DeepSeek-VL2-Tiny, Deformable-DETR-R50, Dino, DyHead-R50, EfficientDet, Faster R-CNN, Faster RCNN, Faster RCNN-R50, Ferret-v2, GRPO, Grounding DINO, MiMo-VL-7B, N/A, OVIS2.5-9B, PaddleOCR, Qwen2.5-VL, Rex-Omni, Rex-Omni-SFT, RexSeek, SEED1.5-VL, SFT, SFT-Sampling-Best, SFT-Sampling-Vote, SSD, Standard object detection models, T-Rex2, X-Pose, YOLOv3, YOLOv5, mixup
- **Main Results**: The results demonstrate significant improvements in chat detection and segmentation accuracy.
- **Ablations**: Ablation studies indicate the importance of specific components in the model.
- **Limitations / Stress Tests**: Tests reveal limitations in handling certain edge cases in chat data.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art performance in object detection., Versatile capabilities including object referring and visual prompting., Improves token efficiency for coordinate prediction.
- **Cons**: Struggles with localization compared to traditional detectors., May produce duplicate predictions., Learning complexity in mapping discrete tokens to continuous pixel space.
- **Future Work**: Explore further enhancements in language understanding for object detection., Investigate additional data engines for improved training., Develop strategies to mitigate duplicate predictions.

</details>
