# Daily Paper Digest Â· 2025-10-17
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [Terra: Explorable Native 3D World Model with Point Latents](http://arxiv.org/pdf/2510.14977v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D generation

### 2. Motivation & Gaps
- The paper addresses the need for effective methods in generating 3D point clouds using latent diffusion techniques.

- **Related work challenges:**
  - DriveDreamer: Focuses on image-based representations but lacks 3D consistency.
  - Sora: Video-based world models that do not consider explicit 3D priors.
  - Genie-3: While successful, it still overlooks the inherent 3D nature of environments.
  - ViewCrafter (Yu et al., 2024b): Bypasses multi-view inconsistency but compromises exploration efficiency.
  - Prometheus (Yang et al., 2025a): Trains a dual-modal diffusion network but does not fully address the integration of texture and depth.
  - Luo & Hu (2021): Focuses on point cloud generation but lacks the ability to synthesize textured results.
  - Kingma & Welling, 2014: Conventional VAEs struggle to generalize to unstructured point latents where 3D coordinates contain crucial geometry information.
  - Zhao et al., 2021: Existing architectures do not effectively handle the locality of point latents and associated local structures.
  - Prometheus: Limited ability to synthesize plausible images while maintaining accurate 3D structures.
  - Trellis: Lacks consistent geometry and texture in generated scenes.
  - PixelSplat: Does not utilize 3D geometry information, leading to inferior performance.
  - Cosmos world foundation model platform for physical AI: Limited exploration capabilities in existing models.
  - Self-supervised learning from images with a joint-embedding predictive architecture: Challenges in generating coherent 3D representations.
  - Stable video diffusion: Scaling latent video diffusion models to large datasets: Difficulty in maintaining consistency across generated scenes.
  - Flow matching for generative modeling: Challenges in generative modeling techniques.
  - Diffusion probabilistic models for 3d point cloud generation: Limitations in existing diffusion models for 3D generation.
  - High-resolution image synthesis with latent diffusion models: Need for high-resolution outputs in 3D generation.

### 3. Core Idea
- The core idea is to utilize interactive point cloud latent diffusion to enhance the generation of 3D models.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates latent diffusion processes with interactive point cloud generation.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for optimizing point cloud generation.
- **Complexity / Resources**: The method is designed to be resource-efficient while maintaining high-quality outputs.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various datasets to evaluate the performance of the proposed method against standard metrics.
- **Baselines**: Can3Tok, Can3Tok*, DriveDreamer, Existing 3D world models, Existing diffusion models, Genie-3, Luo & Hu (2021), MVSplat, PixelSplat, Prometheus, Sora, Terra, Traditional 3D generation techniques, Traditional generative models, Trellis, ViewCrafter
- **Main Results**: The results demonstrate significant improvements in the quality and efficiency of 3D point cloud generation.
- **Ablations**: Ablation studies indicate the effectiveness of different components of the proposed method.
- **Limitations / Stress Tests**: Tests reveal some limitations in handling complex geometries.

### 6. Takeaways
- **Pros**: Achieves high 3D consistency., Supports flexible rendering from any viewpoint., Reduces redundancy in input 3D data.
- **Cons**: Existing methods are limited to either object-level or coarse scene-level generation., Challenges in learning reprojection constraints in an implicit manner., Potential inefficiencies in exploration due to smaller generation steps.
- **Future Work**: Explore further enhancements in 3D consistency., Investigate integration with other modalities., Develop applications for real-time interaction.

</details>

### [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](http://arxiv.org/pdf/2510.14970v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Gene selection and phenotype prediction using BINN models

### 2. Motivation & Gaps
- The study aims to improve the identification of significant genes related to flowering time in maize using a novel BINN architecture.

- **Related work challenges:**
  - Conventional G2P models: Dependence on genotype data alone, leading to underutilization of functional knowledge.
  - Models incorporating intermediate molecular phenotypes: Remain impractical for genomic selection since such data are unavailable at deployment or design time.
  - Linear mixed models: State-of-the-art but limited in integrating multi-omics information.
  - Deep learning methods: Have not shown consistent improvements over traditional models and lack tailored architectures.
  - Existing BINNs: Enforce rigid mappings and depend on omics measurements at deployment, limiting practicality.
  - Conventional G2P models: Limited predictive performance due to noise and incomplete pathway knowledge.
  - Existing omics-based models: Struggles with capturing subtle genetic variations in diverse populations.
  - Traditional BINNs: Limited flexibility in genetically distant populations
  - TWAS: May fail to capture nonlinear genetic contributors
  - Ridge regression: Underperformance in sparse-data regimes
  - Ridge Regression (RR): Limited ability to capture nonlinear relationships in sparse data.
  - Fully Connected Networks (FCN): Struggles with interpretability and performance in sparse data regimes.
  - Genomic Best Linear Unbiased Prediction (GBLUP): Inability to incorporate biological mechanisms effectively.
  - Existing BINNs: Assigning each hidden neuron to a specific biological entity limits flexibility and interpretability.
  - Traditional predictive models: Struggle with overfitting and sensitivity to noise in sparse data settings.
  - Current omics data applications: Often face issues with noise and heterogeneity, complicating effective model architecture design.
  - Genomic language models (gLMs): Inferring function for genes lacking annotation.
  - Functional genomics predictors: Converting provisional signals into strong inductive biases.
  - Integration of environmental signals: Heterogeneity of available omics data from different sources.
  - Previous studies on genomic selection and phenotype prediction: Lack of interpretability in how individual biological entities influence phenotype predictions.
  - Genomic selection for crop improvement: Existing methods lack interpretability and integration of multi-omics data.
  - A review of deep learning applications for genomic selection: Challenges in effectively predicting phenotypes from genotypes.
  - Methods of integrating data to uncover genotypeâ€“phenotype interactions: Difficulty in linking genetic data to phenotypic outcomes.
  - Bertheloot et al.: Capturing the switch-like behavior of bud outgrowth across species.
  - Powell et al.: Parameterizing interactions in the ODE framework with genetic values.
  - N/A: Recovering hidden relationships in real breeding populations.
  - Torres-Rodriguez et al. [10]: Identifying genes controlling flowering time using a comprehensive TWAS dataset.
  - Torres-Rodriguez et al.: Existing models may not effectively identify significant genes due to data sparsity.
  - G2P baseline models: Performance comparison with BINN models
  - Ridge Regression using every 100th marker: Evaluating the impact of biologically informed feature selection
  - Ablation study on network sparsity: Understanding the effect of network architecture on predictive performance
  - N/A: N/A

### 3. Core Idea
- The paper presents various architectures of Biological Interaction Neural Networks (BINN) designed for processing different omics types in parallel, allowing for tailored models based on specific data structures and biological assumptions.

### 4. Method
- **Pipeline**: Independent subnetworks process each omics type (e.g. transcriptomics, proteomics, methylation) in parallel, with their latent outputs concatenated only at the final integrator.
- **Architecture / Loss / Training**: The architectures include Single-layer BINN, Staggered-layer BINN, Stacked-layer BINN, and Parallel-layer BINN, each suited for different types of data integration.
- **Complexity / Resources**: The model requires a validation dataset during training and utilizes 5-fold cross-validation for performance evaluation.

### 5. Experiments
- **Datasets & Metrics**: The performance is evaluated on datasets related to flowering time traits using Spearman correlation.
- **Baselines**: Conventional neural networks, Fully Connected Network (FCN), Fully-connected network (FCN), G2P baselines, GBLUP, Genomic Best Linear Unbiased Prediction (GBLUP), Kernel approaches, Linear mixed models, N/A, Other deep learning frameworks, Other machine learning approaches, Random forests, Ridge Regression (RR), Ridge Regression on bio-informed markers, Ridge Regression on every 100th marker, Ridge Regression using ElasticNet-selected genes, Ridge Regression using all genes, Ridge Regression using eQTL-selected markers, Ridge Regression using every 100th marker, Ridge regression, Standard genomic selection models, Traditional genomic prediction models, Trained BINN ensembles, Unconstrained fully-connected network (FCN)
- **Main Results**: The BINN model significantly outperforms the baseline models with a p-value of 2.23 Ã—10âˆ’6.
- **Ablations**: An ablation study showed that biologically informed network sparsity improves model performance.
- **Limitations / Stress Tests**: The correlation metric may underestimate true model variance due to fixed data splits.

### 6. Takeaways
- **Pros**: Improved genomic prediction accuracy., Reveals nonlinear biological relationships., Guides genomic selection and candidate gene selection.
- **Cons**: Requires extensive multi-omics data for training., May not generalize well to all crop species., Complex architecture may complicate implementation.
- **Future Work**: Explore integration with additional omics data types., Investigate application to other crops and traits., Develop user-friendly tools for breeders.

</details>

### [RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks](http://arxiv.org/pdf/2510.14968v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Demonstration Decomposition

### 2. Motivation & Gaps
- The paper addresses the challenge of demonstration decomposition in robotic learning systems, proposing a novel approach that enhances planner-policy coordination.

- **Related work challenges:**
  - UVD: Uses heuristic decomposition rules that may generate unfamiliar sub-tasks for the visuomotor policy.
  - Hi Robot: Relies on human annotation for sub-task decomposition.
  - UVD: Can sub-optimally decompose sub-tasks, deviating from training data.
  - R3M: Requires task-specific knowledge for segmentation.
  - N/A: N/A
  - Expert heuristic decomposer: Serves as a performance upper bound but may not generalize well across tasks.
  - UVD: Detects change points of learning-based visual features but does not align well with the samples in training data.
  - Uniform decomposition: Divides demonstrations into equal partitions, which may not capture the nuances of task execution.
  - UVD: Fails to locate keyframes precisely and generates sub-tasks that deviate from expert sub-task divisions.
  - Gemini-2.5-pro: Despite powerful general video understanding abilities, it underperforms compared to RDD in specific tasks.
  - Rt-2: Vision-language-action models transfer web knowledge to robotic control: Integration of vision-language-action models in robotic control.
  - Openvla: An open-source vision-language-action model: Scalability and accessibility of vision-language-action models.
  - Fast: Efficient action tokenization for vision-language-action models: Efficiency in action tokenization for improved performance.
  - N/A: N/A
  - N/A: N/A
  - Open X-Embodiment (OXE) Dataset: Removing proprietary data barriers for training generalist models.
  - SmolVLA: Outperforming closed-source datasets with open-sourced community datasets.
  - AgiBot World: Providing open-source toolchains and standardized data collection pipelines.

### 3. Core Idea
- RDD formulates the demonstration decomposition task as an optimal partitioning problem, solved efficiently using dynamic programming and a novel sub-task interval scoring function.

### 4. Method
- **Pipeline**: Dynamic programming approach for optimal partitioning of demonstrations.
- **Architecture / Loss / Training**: The method utilizes a novel sub-task interval scoring function and dynamic programming for optimal partitioning.
- **Complexity / Resources**: The complexity of the algorithm grows linearly with the number of frames, and it supports parallel evaluation to reduce latency.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on real-world manipulation benchmarks like AgiBotWorld-Alpha and OOD datasets from Robo-Cerebra, using metrics like mean intersection over union (mIoU).
- **Baselines**: Expert, Gemini-2.5-pro, Hi Robot, N/A, R3M, State-of-the-art demonstration decomposers, State-of-the-art heuristic decomposer, UVD, Uniform, Vanilla Planner, w/o Finetune
- **Main Results**: RDD outperforms existing demonstration decomposers.
- **Ablations**: The necessity of fine-tuning on target tasks was established through comparative evaluations.
- **Limitations / Stress Tests**: RDD's performance was tested against noisy samples and the quality of the source dataset was discussed.

### 6. Takeaways
- **Pros**: Automatic decomposition without human intervention., Alignment with training data improves task performance., Robustness across diverse settings.
- **Cons**: May not generalize well to all task types., Performance can be affected by the quality of training data., Complexity assumptions may not hold in all scenarios.
- **Future Work**: Explore further optimization techniques for decomposition., Investigate applications in more complex environments., Enhance the framework to handle a wider variety of tasks.

</details>

## Gaussian Splatting

### [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](http://arxiv.org/pdf/2510.14926v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Simulation of open quantum systems

### 2. Motivation & Gaps
- The paper addresses the need for efficient simulation methods for general open quantum systems.

- **Related work challenges:**
  - Numerous case studies on quantum resources reducing current fluctuations.: Complexity of techniques required for describing current fluctuations in strongly coupled, nonlinear open quantum systems.
  - Shubrook et al. study on fluctuations of heat transferred during equilibration.: Need for sophisticated techniques like perturbative nonequilibrium Green functions and tensor-network simulations.
  - N/A: N/A
  - N/A: N/A
  - Thermodynamic Uncertainty Relations in Quantum Systems: Understanding the trade-off between current fluctuations and entropy production.
  - Quantum Jump Unraveling of Full Counting Statistics: Implementing ideal detectors for monitoring current statistics.
  - N/A: N/A
  - Previous studies on TUR violations: Limited consideration of nonclassicality in environmental degrees of freedom.
  - N/A: N/A
  - N/A: N/A
  - Tensor-network method to simulate strongly interacting quantum thermal machines: Complexity in simulating strongly interacting systems.
  - Performance of reservoir discretizations in quantum transport simulations: Limitations in accuracy and efficiency of existing discretization methods.
  - Markovian master equations for quantum thermal machines: Local versus global approach: Trade-offs between local and global approaches in modeling quantum thermal machines.

### 3. Core Idea
- The introduction of optimized auxiliary oscillators to enhance the simulation of open quantum systems.

### 4. Method
- **Pipeline**: The method involves the use of auxiliary oscillators to represent the dynamics of open quantum systems efficiently.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The method is designed to reduce computational complexity while maintaining accuracy.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize synthetic datasets to evaluate the performance of the proposed method against existing techniques.
- **Baselines**: Classical TUR, Full counting statistics, Markovian master equations, N/A, Standard Lindblad master equation, Standard weak-coupling master equation, Tensor-network method, Weak coupling regime behaviors
- **Main Results**: The proposed method shows significant improvements in simulation accuracy and efficiency compared to baseline methods.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The method's performance is tested under various conditions to identify its limitations.

### 6. Takeaways
- **Pros**: Provides new insights for controlling current fluctuations in quantum devices., Facilitates a physically transparent analysis of current fluctuations beyond weak-coupling., Demonstrates the potential of exploiting quantum resources for reliable thermal machines.
- **Cons**: Complexity of techniques may obscure physical intuition., Limited to moderate coupling strengths due to the rotating-wave approximation., No clear relationship between nonclassicality and quantum TUR violations.
- **Future Work**: Application of the RC mapping to more complex environmental structures., Development of more sophisticated embeddings for better predictive power., Further exploration of nonclassical properties in different quantum systems.

</details>

### [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](http://arxiv.org/pdf/2510.14925v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluating the impact of critique mechanisms on the calibration of large language models

### 2. Motivation & Gaps
- This work illustrates how philosophical frameworks can inform quantitative analyses of model calibration and epistemic stability.

- **Related work challenges:**
  - Prior work on Kantian themes and cybernetics: Lack of a unified structural framework linking Kantâ€™s philosophy to modern generative models.
  - Studies on instability and hallucination in AI systems: Insufficient mathematical explicitness in linking philosophical concepts to empirical frameworks.
  - N/A: N/A
  - Existing hallucination metrics: These metrics are fundamentally output-centric and do not address the inferential dynamics of reasoning systems.
  - Bayesian predictive coding in cognitive neuroscience: The relationship between uncertainty and confidence is complex and not fully captured by existing models.
  - Kantâ€™s Critique of Pure Reason: The philosophical grounding of reason's self-limiting function is often overlooked in empirical analyses.
  - Kant's critical philosophy: Maintaining coherence of form and proportionality between concept and intuition.
  - Empirical estimation of H-Risk: Operationalizing Kant's principles in practical systems.
  - Jacobian-based stability analysis in control and neural networks: Translating traditional stability measures to the context of LLMs and their internal reasoning dynamics.
  - Kantian feedback mechanisms: Implementing a critique step in LLMs to enhance their inferential stability.
  - N/A: N/A
  - Kant, cybernetics, and cybersecurity: Integration and implications: Integration of philosophical concepts with practical applications in AI.
  - A cybernetic theory of persons: how Sellars naturalized Kant: Translating transcendental categories into mathematical form.
  - Philosophy and cybernetics: Questions and issues: Fostering interdisciplinary dialogue between philosophy of mind and AI safety.
  - N/A: N/A

### 3. Core Idea
- Linking Kantian epistemology with state-space inference to enhance model calibration and reasoning stability.

### 4. Method
- **Pipeline**: Prompt-Critique-Revision Loop where the model generates an answer, critiques it, and revises based on the critique.
- **Architecture / Loss / Training**: Utilizes a linear minimum-mean-square-error correction to adjust latent representations before output.
- **Complexity / Resources**: The method involves a three-step feedback cycle and requires computational resources for training the linear probe and conducting stability analysis.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on factual question-answering datasets like FEVER and NQ, measuring factual accuracy, self-consistency, hallucination rate, and structural instability proxies.
- **Baselines**: Baseline (B1): Stability-margin sweep., Baseline (B2): Observability/conditioning sweep., Existing models of cognitive architecture, Existing output-centric hallucination metrics, Linear-Gaussian simulations, N/A, Standard LLM outputs without critique mechanism
- **Main Results**: The critique mechanism significantly improves calibration metrics, with strong correlations between the instability index and miscalibration.
- **Ablations**: Tests showed that removing components of the H-Risk index reduced correlation with miscalibration, supporting the necessity of the composite form.
- **Limitations / Stress Tests**: The critique mechanism was not fully instantiated in all experiments, limiting the generalizability of results.

### 6. Takeaways
- **Pros**: Provides a novel framework linking Kantian philosophy to modern AI systems., Offers insights into diagnosing and reducing overconfidence in reasoning systems., Establishes a quantitative approach to understanding hallucination in AI.
- **Cons**: The framework may be too abstract for practical implementation., Limited empirical validation in the current version of the paper., Potential challenges in generalizing findings across different AI models.
- **Future Work**: Further empirical testing of the H-Risk metric across diverse AI systems., Exploration of additional philosophical implications of the findings., Development of more robust models that incorporate feedback stability principles.

</details>

## avatar

### [Sales Skills Training in Virtual Reality: An evaluation utilizing CAVE and Virtual Avatars](http://arxiv.org/pdf/2510.14603v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigating the effectiveness of virtual reality in sales training

### 2. Motivation & Gaps
- The paper explores the potential of virtual reality (VR) as a tool for enhancing sales skills training.

- **Related work challenges:**
  - Traditional sales training methods: High cost and dependence on participant performance and adaptability
  - Existing VR training programs: Lack of empirical studies focusing on immersive training tools tailored specifically for sales and negotiation contexts
  - CAVE systems in education: Effectiveness varies depending on application and individual learner abilities
  - CAVE systems for fire safety training: Effectiveness varies based on individual learner abilities.
  - CAVE in emotional intelligence training: Training success depends more on spatial abilities than emotional intelligence.
  - CAVE in sports training: Need for further research on long-term benefits.
  - CAVE in engineering education: Comparison with other VR setups needed to validate superior outcomes.
  - Previous studies on VR training effectiveness: Lack of significant statistical effects in user experience across different conditions.
  - User Experience Questionnaire: No statistically significant differences between conditions were found.
  - Qualitative feedback analysis: Mixed perceptions of the simulated environment and avatar interactions.
  - Previous studies on user experience in virtual environments: Lack of statistically significant findings despite observed trends.
  - Research on social presence in virtual reality: Inconsistent results regarding the impact of unfriendly conditions on perceived presence.
  - Technical challenges in virtual reality training: Technical disruptions affecting user immersion and focus.
  - Howard, M., Gutworth, M., & Jacobs, R. (2021). A meta-analysis of virtual reality training programs.: Limited understanding of the specific benefits of VR in sales training contexts.
  - Stephens, R., Awasthi, A., Crowley, K., Boyle, F., & Walsh, J. (2021). A Literature Review of Virtual Reality Interpersonal Training for Salespeople.: Need for empirical evidence on the effectiveness of VR training compared to traditional methods.

### 3. Core Idea
- Utilizing immersive virtual reality environments to simulate real-world sales scenarios for training purposes.

### 4. Method
- **Pipeline**: The training program involves a series of VR simulations designed to mimic sales interactions.
- **Architecture / Loss / Training**: Participants engaged in role-playing exercises with varying customer personalities and environments.
- **Complexity / Resources**: The implementation requires VR headsets and software capable of creating realistic sales environments.

### 5. Experiments
- **Datasets & Metrics**: The effectiveness of the training was measured using pre- and post-training assessments of sales skills.
- **Baselines**: Condition 1 (FUxFE), Condition 2 (FUxUE), Condition 3 (UUxFE), Condition 4 (UUxUE), Igroup Presence Questionnaire (IPQ), Online sales training modules, Other VR training systems, Paper-based training materials, Previous user experience studies, Social Presence Questionnaire (SPQ), Standard training methods, Traditional face-to-face training, Traditional sales training methods, User Experience Questionnaire - Short Version (UEQ-S)
- **Main Results**: Participants trained in VR showed significant improvement in sales techniques compared to those trained using traditional methods.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The study acknowledges limitations such as sample size and the need for long-term follow-up.

### 6. Takeaways
- **Pros**: Enhanced engagement through immersive training environments, Risk-free learning opportunities for trainees, Standardized learning experiences ensuring consistent quality
- **Cons**: High cost of implementation, Dependence on technology and infrastructure, Limited empirical studies on effectiveness in sales training
- **Future Work**: Further research on VR systems dedicated to sales training, Development of more interactive and engaging learning experiences, Exploration of CAVE systems in various training contexts

</details>

### [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](http://arxiv.org/pdf/2510.14241v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Deepfake detection

### 2. Motivation & Gaps
- The paper presents PIA, a multi-modal technique for detecting deepfakes by analyzing phoneme articulation, visual features, geometric consistency of lips, and identity indicators.

- **Related work challenges:**
  - Agarwal et al.: Prior work demonstrated that phoneme-viseme mismatches can serve as reliable indicators of manipulation, particularly in the context of deepfakes generated by automated lip-syncing.
  - Various existing methods: Most existing deepfake detection methods use only one modality, predominantly relying on analysis focused solely on audio or visual signals.
  - Various existing deepfake detection methods: Often rely on either audio, video, or multimodal signals with distinct feature extraction and classification techniques.
  - Multimodal methods utilizing phonemeâ€“viseme alignment: Detecting deepfake videos remains difficult due to the complexity of mismatches.
  - Temporal audio-visual inconsistency detection: Existing models may not fully capture subtle discrepancies in manipulated videos.
  - N/A: Existing methods may not effectively capture the subtle identity shifts in deepfake videos.
  - LipForensics: Achieved 80.1% ACC and 82.4% AUC, but does not capture temporal inconsistencies effectively.
  - A VFF: Achieved 98.6% ACC and 99.1% AUC, but lacks the integration of phoneme articulation.
  - Prior work on Deepspeak v1.0: Did not evaluate on Deepspeak v2.0, which includes avatar-based deepfakes.
  - Detecting deep-fake videos from phoneme-viseme mismatches: Existing methods may not effectively capture subtle temporal and cross-modal inconsistencies.
  - Whisperx: Time-accurate speech transcription of long-form audio: Current models are limited to English-language inputs and may not generalize across different video resolutions.
  - Detecting deep-fake videos from phoneme-viseme mismatches: N/A
  - Whisperx: Time-accurate speech transcription of long-form audio: N/A
  - Simswap: An efficient framework for high fidelity face swapping: N/A
  - NPVForensics: Jointing non-critical phonemes and visemes for deepfake detection: N/A
  - Voice-face homogeneity tells deep-fake: N/A
  - Videoretalking: Audio-based lip synchronization for talking head video editing in the wild: N/A
  - Not made for each other-audio-visual dissonance-based deepfake detection and localization: N/A
  - Exposing lip-syncing deepfakes from mouth inconsistencies: N/A
  - Implicit identity driven deepfake face swapping detection: N/A
  - A unified end-to-end dense swin transformer deep learning model for audio-visual deepfakes detection: N/A
  - Emotions donâ€™t lie: An audio-visual deepfake detection method using affective cues: N/A
  - Diff2lip: Audio conditioned diffusion models for lip-synchronization: N/A
  - Fsgan: Subject agnostic face swapping and reenactment: N/A
  - Deepfake detection based on discrepancies between faces and their context: N/A
  - Audio-visual feature fusion for video deepfake detection: N/A
  - A lip sync expert is all you need for speech to lip generation in the wild: N/A
  - Learning transferable visual models from natural language supervision: N/A
  - Learning to detect manipulated facial images: N/A
  - Face swapping model based on insightface: N/A
  - Deepfake video detection using convolutional vision transformer: N/A
  - Simple and effective zero-shot cross-lingual phoneme recognition: N/A
  - Audio-visual joint learning for detecting deepfake: N/A
  - Unlocking the hidden potential of clip in generalizable deepfake detection: N/A
  - Explicit correlation learning for generalizable cross-modal deepfake detection: N/A
  - Integrating spatial knitting attentions to embed high-level and fidelity-rich conditions in diffusion models: N/A
  - Memory-guided diffusion for expressive talking video generation: N/A
  - Exploring temporal coherence for more general video face forgery detection: N/A
  - Joint audio-visual deepfake detection: N/A

### 3. Core Idea
- PIA integrates visual, geometric, and identity cues with phoneme analysis to enhance deepfake detection performance.

### 4. Method
- **Pipeline**: The model processes phoneme data alongside visual and geometric features to identify inconsistencies in deepfake videos.
- **Architecture / Loss / Training**: Utilizes an EfficientNet-B0 backbone and incorporates various input streams for training.
- **Complexity / Resources**: Requires significant computational resources for training and fine-tuning on high-resolution datasets.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on the DeepSpeak v2.0 dataset using AUC scores across various manipulation types.
- **Baselines**: A V oID-DF, A VFF, A VFakeNet, AV-DFD, AVAD (LRS2), AVAD (LRS3), AVFF, CV iT, Emotions Donâ€™t Lie, Existing deepfake detection methods, FTCN, LipForensics, MDS, Multimodal detection techniques, N/A, RealForensics, VFD, Xception
- **Main Results**: PIA achieved state-of-the-art performance with AUC scores significantly higher than baseline models.
- **Ablations**: Ablation studies showed the impact of excluding various input streams on model performance.
- **Limitations / Stress Tests**: The model's generalization is limited to the specific resolutions used for training and is not applicable to RealVideo-FakeAudio scenarios.

### 6. Takeaways
- **Pros**: Significantly improves detection of subtle deepfake alterations., Integrates multiple modalities for enhanced accuracy., Achieves high performance metrics on benchmark datasets.
- **Cons**: Complexity of the model may require significant computational resources., Potential overfitting on specific datasets., Challenges in real-time application due to processing time.
- **Future Work**: Explore further enhancements in multimodal integration., Investigate real-time detection capabilities., Expand the dataset to include more diverse deepfake types.

</details>

### [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](http://arxiv.org/pdf/2510.14081v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar reconstruction

### 2. Motivation & Gaps
- Existing methods rely on synthetic datasets that lack fine details, leading to a realism gap in 3D reconstructions.

- **Related work challenges:**
  - Single-image 3D human reconstruction: Methods struggle with ambiguity and often produce incorrect geometry for occluded regions.
  - Multi-view 3D human reconstruction: Traditional methods require precise camera calibration and a large number of images.
  - Generative models for 3D avatars: Most rely on synthetic datasets that lack fine details, limiting realism.
  - RenderPeople: Fails to capture realism and identity preservation in 3D reconstructions.
  - Objaverse: Synthetic datasets do not provide the necessary detail for realistic human appearance.
  - Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image: N/A
  - Objaverse: A universe of annotated 3D objects: N/A
  - Denoising diffusion probabilistic models: N/A
  - AvatarCLIP: Zero-shot text-driven generation and animation of 3D avatars: N/A
  - EV A3D: Compositional 3D human generation from 2D image collections: N/A
  - LRM: Large reconstruction model for single image to 3D: N/A
  - End-to-end recovery of human shape and pose: N/A
  - 3D Gaussian splatting for real-time radiance field rendering: N/A
  - URAvatar: Universal relightable Gaussian codec avatars: N/A
  - Neural volumes: Learning dynamic renderable volumes from images: N/A
  - SMPL: A skinned multi-person linear model: N/A
  - FaceLift: Learning generalizable single image 3D face reconstruction from synthetic heads: N/A
  - NeRF: Representing scenes as neural radiance fields for view synthesis: N/A
  - Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans: N/A
  - Structure-from-motion revisited: N/A
  - Pixelwise view selection for unstructured multi-view stereo: N/A
  - Attention is all you need: N/A
  - GRM: Large Gaussian reconstruction model for efficient 3D reconstruction and generation: N/A
  - GS-LRM: Large reconstruction model for 3D Gaussian splatting: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A

### 3. Core Idea
- To create a high-fidelity, identity-preserving 3D avatar using a novel dataset of high-fidelity 3D scans of real people.

### 4. Method
- **Pipeline**: Capture, Canonicalize, Splat
- **Architecture / Loss / Training**: Transformer-based Large Reconstruction Model with a weighted loss function combining L1 photometric loss, perceptual loss, alpha loss, and scale regularization loss.
- **Complexity / Resources**: Requires high-quality, multi-view dome captures of real individuals for training.

### 5. Experiments
- **Datasets & Metrics**: Utilizes a novel dataset of person-specific Gaussian splatting avatars derived from high-quality multi-view dome captures.
- **Baselines**: Generative models using synthetic datasets, Multi-view reconstruction methods, N/A, RenderPeople, Single-image reconstruction methods
- **Main Results**: Our full pipeline, trained on our high-fidelity Human Avatar Dataset with multi-view inputs, significantly outperforms other configurations on our internal test set.
- **Ablations**: This demonstrates the critical importance of both high-quality training data and the use of multiple input views for high reconstruction quality.
- **Limitations / Stress Tests**: Models trained solely on synthetic data exhibit identity shifts and lack realism.

### 6. Takeaways
- **Pros**: Generates hyper-realistic 3D avatars from minimal input., Robust identity preservation through generative canonicalization., Utilizes high-fidelity training data for enhanced realism.
- **Cons**: Requires multiple unstructured images for best results., May struggle with extreme poses or occlusions., Dependent on the quality of input images.
- **Future Work**: Explore further improvements in realism and detail., Investigate applications in virtual reality and telepresence., Develop methods for real-time avatar generation.

</details>

## video understanding

### [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](http://arxiv.org/pdf/2510.14972v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Code Translation

### 2. Motivation & Gaps
- The paper addresses the challenges of translating code between programming languages while preserving functionality and correctness.

- **Related work challenges:**
  - Large language models (LLMs) for code rely on subword tokenizers: Tokenization based on statistics rather than grammar leads to misaligned representations.
  - Previous studies on LLMs: Lack of focus on the impact of tokenization on model behavior.
  - HumanEvalPack (Muennighoff et al., 2023): Inconsistent tokenization leading to fragmented representations.
  - Code summarization (Hu et al., 2018; Panthaplackel et al., 2020): Evaluating correctness of generated code from natural language descriptions.
  - Code translation (Ahmad et al., 2023; Puri et al., 2021): Translating code snippets while maintaining semantic integrity.
  - N/A: N/A
  - Liu et al. (2025): Token merges across whitespace boundaries produce more meaningful units.
  - Chirkova and Troshin (2023): Tokenizer designed to better align with PL syntax, achieving lower token counts.
  - Chirkova and Troshin (2023): Tokenizers designed to better align with PL syntax still face challenges in preserving model performance.
  - Zheng et al. (2025): Instruction-tuned models show performance drops with unconventional tokenization.
  - Wang et al. (2025): Adversarial changes to token boundaries degrade model predictions.
  - N/A: N/A
  - HumanEval: Ensuring consistent outputs and handling edge cases in code translation.
  - Avatar and CodeNet benchmarks: Fixing compatibility issues and ensuring accurate test results.
  - N/A: N/A

### 3. Core Idea
- The core idea is to mutatively rewrite code contexts by parsing and renaming identifiers while maintaining the semantics of the original code.

### 4. Method
- **Pipeline**: The method involves parsing code, identifying immutable and renameable identifiers, and applying consistent edits across code and tests.
- **Architecture / Loss / Training**: Layer-wise analysis shows that the issue originates in early embeddings where subword segmentation fails to capture grammar token boundaries.
- **Complexity / Resources**: The experiments were conducted on an NVIDIA H100 GPU cluster, consuming approximately 1840 GPU-hours.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilized the HumanEval, Avatar, and CodeNet benchmarks, focusing on accuracy and sensitivity metrics.
- **Baselines**: Avatar, Avatar-java2py, Avatar-py2java, BPE tokenizers, Character-level tokenizers, CodeNet, DeepSeek-Coder, Grammar-driven tokenizers, HumanEval, HumanEval-Explain-java, HumanEval-Explain-py, HumanEval-Fix-java, HumanEval-Fix-py, Llama-3, N/A, Qwen2.5-Coder
- **Main Results**: The results indicate improvements in code translation accuracy while maintaining test pass behavior.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The study acknowledges limitations in handling pathological samples and certain edge cases.

### 6. Takeaways
- **Pros**: Identifies and formalizes the misaligned tokenization problem in code LLMs., Introduces a framework for quantifying model sensitivity to semantic-preserving code rewrites., Open-sources framework and data for future research.
- **Cons**: Tokenization design remains under-explored., Minor formatting changes can lead to significant shifts in model behavior., Current tokenizers may not align with programming language grammar.
- **Future Work**: Develop grammar-aware tokenization strategies., Explore the impact of tokenization on other LLM tasks., Facilitate further research on domain-adaptive tokenization.

</details>

### [C4D: 4D Made from 3D through Dual Correspondences](http://arxiv.org/pdf/2510.14960v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Point Transformation using Linear Blend Displacement

### 2. Motivation & Gaps
- The method aims to achieve smooth and locally influenced transformations of 3D points based on the displacements of control points.

- **Related work challenges:**
  - DUSt3R: Inaccurate results when applied to dynamic scenes due to reliance on multi-view geometric constraints.
  - DUSt3R: Sequential pipeline complexity and vulnerability to errors in sub-tasks.
  - MonST3R: Optimization-based methods relying on off-the-shelf priors for supervision.
  - Optical Flow Estimation: Requires known camera parameters and is sensitive to outliers.
  - Camera Pose Estimation: Global alignment can misalign moving objects, affecting accuracy.
  - Point Tracking: Maintaining accuracy in dynamic environments with occlusions.
  - DUSt3R: Requires ground-truth camera intrinsics for accurate pose estimation.
  - MASt3R: Limited to static scenes and does not generalize well to dynamic environments.
  - MonSt3R: Fine-tuned on dynamic datasets but lacks joint estimation capabilities.
  - RAFT: Only predicts position and occlusion of tracking points.
  - TAP-Net: Limited to predicting static point positions.
  - CoTracker: Does not account for mobility in tracking points.
  - N/A: N/A
  - DUSt3R: Exhibits flickering artifacts in video depth estimation.
  - MonST3R: Struggles with recognizing dynamic regions under challenging conditions.
  - MonST3R: Struggles with multi-tasking when using a sole CNN or 3D-aware encoder.
  - FlowP-SAM: State-of-the-art supervised method that our approach outperforms.
  - N/A: N/A

### 3. Core Idea
- Utilizing Linear Blend Displacement (LBD) to transform 3D points based on the displacements of smoothed tracking points treated as control points.

### 4. Method
- **Pipeline**: The method involves obtaining smoothed 3D trajectories, performing nearest neighbor search for control points, computing weights, aggregating displacements, and transforming query points.
- **Architecture / Loss / Training**: The architecture includes a CNN encoder and a 3D-aware encoder, with a focus on trajectory smoothing and point transformation.
- **Complexity / Resources**: The method requires significant computational resources for dense tracking and smoothing processes.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize TUM-Dynamics and ScanNet datasets for training, and MOVi-E, Panning MOVi-E, and MOVi-F datasets for evaluation, measuring accuracy.
- **Baselines**: 3D formulations, CoTracker, DUSt3R, FlowP-SAM, MASt3R, MFT, MonST3R, MonSt3R, N/A, PIPs, RAFT, Specialized methods for camera pose estimation, TAP-Net, TAPIR
- **Main Results**: Our approach achieved an IoU of 47.89, outperforming MonST3R and FlowP-SAM.
- **Ablations**: Ablation studies showed that combining CNN and 3D-aware encoders improves performance.
- **Limitations / Stress Tests**: The method's performance is limited by the lack of available labels for mobility.

### 6. Takeaways
- **Pros**: C4D upgrades 3D reconstruction to 4D using temporal correspondences., Dynamic-aware point tracker improves accuracy in tracking moving points., Motion masks enhance the 3D reconstruction process.
- **Cons**: Existing methods struggle with dynamic scenes., Complexity in ensuring smooth trajectories for 3D points.
- **Future Work**: Explore further optimization techniques for dynamic scenes., Investigate additional applications of C4D in real-time systems.

</details>

### [RealDPO: Real or Not Real, that is the Preference](http://arxiv.org/pdf/2510.14955v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Quality Assessment

### 2. Motivation & Gaps
- The paper aims to improve the evaluation of video generation quality by leveraging large language models (LLMs) for a more structured and scalable assessment.

- **Related work challenges:**
  - Reward-weighted regression (RWR): Susceptible to reward hacking, leading to a decline in video quality despite high scores.
  - Direct preference optimization (DPO): Limited scalability for high-resolution video generation.
  - Gradient feedback (GF): Bias propagation may lead to loss of ability to assess specific key metrics.
  - Diffusion-Based Video Generation: Limitations in motion dynamics and content richness.
  - Reinforce Learning in Image/Video Generation: Training reward models may suffer from hacking issues and reduced evaluation ability in specific domains.
  - Direct Preference Optimization (DPO): Avoiding complexities and biases introduced by learned reward models.
  - Yang et al., 2024b: Baseline model struggles with aligning generated content with human preferences.
  - Wang et al., 2024c: LiFT's reliance on reward models limits its effectiveness in real-world scenarios.
  - Liu et al., 2025: VideoAlign's naive DPO variant using synthetic data fails to capture real-world complexities.
  - VBench (Huang et al., 2024a;b): Limited performance in generating high-quality video outputs.
  - LiFT (Wang et al., 2024c): Fails to complete actions accurately, leading to poor alignment with textual descriptions.
  - VideoAlign (Liu et al., 2025): Generated actions are not highly aligned with the text.
  - Pre-trained base models: Underperform on specific tasks requiring fine-grained alignment.
  - Supervised fine-tuning methods: May not capture intricate details in video generation.
  - Supervised Fine-Tuning: Effectiveness is limited by the quality and quantity of available annotations.
  - LiFT and VideoAlign: Reward models often fail to provide effective feedback in complex scenarios.
  - VBench-I2V: Providing objective and fine-grained assessments of video quality.
  - N/A: N/A

### 3. Core Idea
- Utilizing LLMs to evaluate generated videos based on structured templates that assess various quality dimensions.

### 4. Method
- **Pipeline**: The evaluation process involves using LLMs to score generated videos based on predefined criteria across multiple dimensions.
- **Architecture / Loss / Training**: RealDPO loss for aligning model predictions with preferred and non-preferred samples.
- **Complexity / Resources**: Conducted experiments on 8 Nvidia H100 GPUs with a total batch size of 8.

### 5. Experiments
- **Datasets & Metrics**: The evaluation metrics include I2V Subject, Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic Quality, and Imaging Quality.
- **Baselines**: CogVideoX-5B, DPO, Existing preference optimization techniques, Human user study, LiFT, Liu et al., 2025, N/A, Pre-trained models, Pretrained models, Previous video quality assessment methods, SFT, State-of-the-art DiT-based model CogVideoX-5B, Supervised fine-tuning methods, Traditional supervised fine-tuning methods, VideoAlign, VideoLLM, Wang et al., 2024c, Yang et al., 2024b
- **Main Results**: LLM-based evaluations align closely with human user study findings, validating the effectiveness of the proposed approach.
- **Ablations**: Ablation studies highlight the importance of human feedback in improving model performance.
- **Limitations / Stress Tests**: The limitations include potential overfitting to specific datasets and the need for extensive computational resources.

### 6. Takeaways
- **Pros**: Improved motion realism in video generation., Utilizes real-world data for better preference learning., Enables iterative self-correction of model outputs.
- **Cons**: Potential overfitting due to reliance on real-world data., Challenges in assessing complex motion accurately.
- **Future Work**: Explore further enhancements in motion synthesis techniques., Investigate scalability for high-resolution video generation., Develop more robust evaluation metrics for complex motion.

</details>
