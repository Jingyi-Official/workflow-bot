# Daily Paper Digest Â· 2025-10-22
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [From Volume Rendering to 3D Gaussian Splatting: Theory and Applications](http://arxiv.org/pdf/2510.18101v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction and view synthesis

### 2. Motivation & Gaps
- Gaussian Splatting introduced a novel approach to 3D reconstruction, enabling the creation of high-quality representations.

### 3. Core Idea
- The paper discusses various methods for 3D reconstruction using Gaussian Splatting, focusing on the integration of different input types and architectures to enhance the quality of generated 3D representations.

### 4. Method
- **Pipeline**: The pipeline involves using multi-view images, depth information, and various neural network architectures to generate 3D Gaussians.
- **Architecture / Loss / Training**: Different architectures like U-Net, transformers, and FFNs are employed, with a focus on optimizing the loss functions for better reconstruction quality.
- **Complexity / Resources**: The methods vary in complexity, with some requiring significant computational resources for training and inference.

</details>

### [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](http://arxiv.org/pdf/2510.17617v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Co-Speech Gesture Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating co-speech gestures that are semantically aligned with spoken language and visual inputs.

### 3. Core Idea
- The proposed method leverages language and image inputs to generate gestures in a zero-shot manner, enhancing the realism and relevance of the generated gestures.

### 4. Method
- **Pipeline**: The method involves processing language and image inputs to generate corresponding gestures without prior training on specific gesture datasets.
- **Architecture / Loss / Training**: Utilizes a novel architecture that combines language processing with visual context, optimizing for gesture generation through a custom loss function.
- **Complexity / Resources**: The approach is designed to be computationally efficient, requiring moderate resources for training and inference.

</details>

### [Capturing Head Avatar with Hand Contacts from a Monocular Video](http://arxiv.org/pdf/2510.17181v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Reconstructing realistic head avatars with hand contact from monocular videos

### 2. Motivation & Gaps
- The method aims to improve the accuracy of hand and facial reconstruction from monocular video input, addressing limitations in existing methods.

### 3. Core Idea
- The proposed method utilizes contact loss and depth order loss to align hand and face meshes, enabling accurate reconstruction of hand-face interactions.

### 4. Method
- **Pipeline**: The method includes preprocessing with contact-aware alignment and depth-aware collision prevention, followed by a non-rigid deformation network for reconstruction.
- **Architecture / Loss / Training**: Utilizes PCA-based deformation prior and contact loss to ensure physically plausible deformations.
- **Complexity / Resources**: The method requires a synthetic dataset and real-world video sequences for training and evaluation.

</details>

## video understanding

### [Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](http://arxiv.org/pdf/2510.18876v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Generate multiple-choice questions based on visual scenes and relationships

### 2. Motivation & Gaps
- The paper addresses the need for high-quality, diverse question generation in Visual Question Answering (VQA) using ground truth data.

### 3. Core Idea
- To create a structured approach for generating multiple-choice questions that test various relationships in visual scenes using ground truth data.

### 4. Method
- **Pipeline**: Input ground truth data, analyze relationships, and generate questions in a structured JSON format.
- **Architecture / Loss / Training**: RoI-aligned feature replay architecture with ablation studies demonstrating its effectiveness.
- **Complexity / Resources**: Utilizes a global batch size of 64 and a learning rate of 1e-5 with cosine decay.

</details>

### [DSI-Bench: A Benchmark for Dynamic Spatial Intelligence](http://arxiv.org/pdf/2510.18873v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluating vision-language models on dynamic spatial reasoning tasks

### 2. Motivation & Gaps
- The paper addresses the need for a benchmark to evaluate dynamic spatial intelligence in vision-language models.

### 3. Core Idea
- Introduce DSI-Bench as a comprehensive benchmark for assessing the performance of vision-language models in dynamic spatial reasoning.

### 4. Method
- **Pipeline**: The evaluation pipeline includes sample-wise and group-wise assessments of model performance on dynamic spatial tasks.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Evaluation includes proprietary and open-source models, with a focus on accuracy under different reasoning settings.

</details>

### [How Do LLMs Use Their Depth?](http://arxiv.org/pdf/2510.18871v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyzing the impact of token frequency on model predictions using TunedLens probes

### 2. Motivation & Gaps
- The study investigates how the frequency of tokens affects the predictions made by early layers of language models, particularly focusing on high-frequency tokens.

### 3. Core Idea
- The prominence of high-frequency tokens in early layer predictions is due to the information content of the early layer representations rather than an artifact of the KL-divergence frequency bias.

### 4. Method
- **Pipeline**: Training two versions of TunedLens probes: a baseline version and a modified version that adjusts the update frequency of high-frequency tokens.
- **Architecture / Loss / Training**: The modified TunedLens adjusts the KL-loss contribution of high-frequency tokens to match that of lower frequency tokens.
- **Complexity / Resources**: Utilizes the original codebase from Belrose et al. (2023) for training the TunedLens probes.

</details>

## model collapse

### [Instabilities of a Generalized Gross-Neveu Quantum Criticality](http://arxiv.org/pdf/2510.18875v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyze instabilities toward ferromagnetism and superconductivity

### 2. Motivation & Gaps
- The study investigates the instabilities in a theoretical model at the low temperature quantum critical point, focusing on the scaling dimensions of fermion bilinears.

### 3. Core Idea
- The analysis focuses on the scaling dimensions of fermion bilinears to diagnose instabilities toward superconductivity and ferromagnetism.

### 4. Method
- **Pipeline**: Compute the scaling dimensions of fermion bilinears using Bethe-Salpeter equations and analyze the conditions for instabilities.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

</details>

### [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](http://arxiv.org/pdf/2510.18874v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Continual fine-tuning of large language models

### 2. Motivation & Gaps
- The study investigates the phenomenon of catastrophic forgetting in large language models when subjected to continual fine-tuning.

### 3. Core Idea
- The paper proposes a systematic approach to analyze and mitigate catastrophic forgetting in large language models during continual fine-tuning.

### 4. Method
- **Pipeline**: Utilizes a univariate mixture-of-Gaussians synthetic task to compare forward KL and reverse KL updates.
- **Architecture / Loss / Training**: Focuses on gradient updates and learning rates for different settings.
- **Complexity / Resources**: Involves computational resources for running simulations and evaluations.

</details>
