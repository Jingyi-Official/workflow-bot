# Daily Paper Digest Â· 2025-10-18
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Sales Skills Training in Virtual Reality: An evaluation utilizing CAVE and Virtual Avatars](http://arxiv.org/pdf/2510.14603v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigating the effectiveness of virtual reality in training sales skills

### 2. Motivation & Gaps
- The paper explores the potential of virtual reality (VR) as a tool for enhancing sales training, addressing the need for innovative training methods in a rapidly evolving market.

- **Related work challenges:**
  - Research on VR training programs: Lack of empirical studies focusing on the development, testing, and evaluation of immersive training tools tailored specifically for sales and negotiation contexts.
  - CAVE systems in education and training: Effectiveness varies depending on application and individual learner abilities, necessitating ongoing research.
  - CAVE systems in education: Effectiveness varies based on application and learner abilities.
  - Emotional intelligence training using CAVE: Training success depends more on spatial abilities than emotional intelligence.
  - CAVE in sports training: Need for further research on long-term benefits.
  - Previous studies on VR training effectiveness: Lack of significant statistical effects in user experience across different conditions.
  - User Experience Questionnaire: No statistically significant differences between conditions were found.
  - Qualitative feedback analysis: Participants reported mixed experiences regarding the immersive quality and realism of the virtual environment.
  - Previous studies on user experience in virtual environments: Lack of statistically significant effects of contextual variations on user experience.
  - Research on presence and social presence in virtual reality: Inconsistent findings regarding the impact of unfriendly conditions on perceived presence.
  - Qualitative insights from user feedback: Limited understanding of subjective experiences due to reliance on quantitative measures.
  - Howard, M., Gutworth, M., & Jacobs, R. (2021). A meta-analysis of virtual reality training programs.: Limited understanding of the specific benefits of VR in sales training contexts.
  - Stephens, R., Awasthi, A., Crowley, K., Boyle, F., & Walsh, J. (2021). A Literature Review of Virtual Reality Interpersonal Training for Salespeople.: Challenges in measuring the effectiveness of VR training compared to traditional methods.

### 3. Core Idea
- Utilizing immersive virtual reality environments to simulate real-world sales scenarios for effective training.

### 4. Method
- **Pipeline**: The training program involves a series of VR simulations that replicate sales interactions, allowing trainees to practice and receive feedback.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The implementation requires VR headsets, software for simulation development, and a controlled environment for training.

</details>

### [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](http://arxiv.org/pdf/2510.14241v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Deepfake detection

### 2. Motivation & Gaps
- The paper presents PIA, a multi-modal technique for detecting deepfakes by analyzing phoneme articulation, visual features, geometric consistency of lips, and identity indicators.

- **Related work challenges:**
  - Agarwal et al.: Prior work demonstrated that phoneme-viseme mismatches can serve as reliable indicators of manipulation, particularly in the context of deepfakes generated by automated lip-syncing.
  - Various existing methods: Most existing deepfake detection methods use only one modality, predominantly relying on analysis focused solely on audio or visual signals.
  - Various deepfake generation techniques: Detection techniques must keep pace with the advancements in generation methods.
  - Visual-only detectors: They analyze spatial artifacts and temporal inconsistencies but may miss multimodal discrepancies.
  - Audio-visual detectors: They exploit lip-speech mismatches but may not capture all nuances of manipulation.
  - N/A: Existing methods may not effectively capture the subtle identity shifts in deepfake videos.
  - LipForensics: Achieved 80.1% ACC and 82.4% AUC, but does not leverage temporal consistency.
  - A VFF: Top model with 98.6% ACC and 99.1% AUC, but lacks comprehensive multimodal integration.
  - Prior work on Deepspeak v1.0: Only evaluated on lower quality deepfakes, missing avatar-based manipulations.
  - Detecting deep-fake videos from phoneme-viseme mismatches: Existing methods may not effectively capture subtle temporal inconsistencies.
  - Whisperx: Time-accurate speech transcription of long-form audio: Limited to English-language inputs and may not generalize across different video resolutions.
  - Detecting deep-fake videos from phoneme-viseme mismatches: N/A
  - Whisperx: Time-accurate speech transcription of long-form audio: N/A
  - Simswap: An efficient framework for high fidelity face swapping: N/A
  - NPVForensics: Jointing non-critical phonemes and visemes for deepfake detection: N/A
  - Voice-face homogeneity tells deep-fake: N/A
  - Videoretalking: Audio-based lip synchronization for talking head video editing in the wild: N/A
  - Not made for each other-audio-visual dissonance-based deepfake detection and localization: N/A
  - Exposing lip-syncing deepfakes from mouth inconsistencies: N/A
  - Implicit identity driven deepfake face swapping detection: N/A
  - A v-fakenet: A unified end-to-end dense swin transformer deep learning model for audio-visual deepfakes detection: N/A
  - Emotions donâ€™t lie: An audio-visual deepfake detection method using affective cues: N/A
  - Diff2lip: Audio conditioned diffusion models for lip-synchronization: N/A
  - Fsgan: Subject agnostic face swapping and reenactment: N/A
  - Deepfake detection based on discrepancies between faces and their context: N/A
  - Audio-visual feature fusion for video deepfake detection: N/A
  - A lip sync expert is all you need for speech to lip generation in the wild: N/A
  - Learning transferable visual models from natural language supervision: N/A
  - Learning to detect manipulated facial images: N/A
  - Facefusion: N/A
  - Deepfake video detection using convolutional vision transformer: N/A
  - Simple and effective zero-shot cross-lingual phoneme recognition: N/A
  - Audio-visual joint learning for detecting deepfake: N/A
  - Unlocking the hidden potential of clip in generalizable deepfake detection: N/A
  - Explicit correlation learning for generalizable cross-modal deepfake detection: N/A
  - Integrating spatial knitting attentions to embed high-level and fidelity-rich conditions in diffusion models: N/A
  - Memory-guided diffusion for expressive talking video generation: N/A
  - Exploring temporal coherence for more general video face forgery detection: N/A
  - Joint audio-visual deepfake detection: N/A

### 3. Core Idea
- PIA integrates phoneme articulation with visual and geometric features to robustly detect deepfakes.

### 4. Method
- **Pipeline**: The model uses a combination of phoneme data, visual cues, and geometric features to assess deepfake authenticity.
- **Architecture / Loss / Training**: Utilizes an EfficientNet-B0 backbone for feature extraction and training.
- **Complexity / Resources**: The model requires significant computational resources for training and evaluation.

</details>

### [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](http://arxiv.org/pdf/2510.14081v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar reconstruction

### 2. Motivation & Gaps
- Existing methods rely on synthetic datasets that lack fine details, leading to lower realism in 3D reconstructions.

- **Related work challenges:**
  - Single-image 3D human reconstruction: Limited by available information, leading to plausible but incorrect geometry for occluded regions.
  - Multi-view 3D human reconstruction: Requires a large number of images with precise camera calibration, which is impractical for casual users.
  - Generative models for 3D avatars: Most methods rely on synthetic datasets that lack fine details, resulting in generic appearances.
  - RenderPeople: Fails to capture realism and intricate details in 3D representations.
  - Objaverse: Synthetic datasets do not provide the necessary realism for high-fidelity reconstructions.
  - Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image: N/A
  - Objaverse: A universe of annotated 3D objects: N/A
  - Denoising diffusion probabilistic models: N/A
  - AvatarCLIP: Zero-shot text-driven generation and animation of 3D avatars: N/A
  - EV A3D: Compositional 3D human generation from 2D image collections: N/A
  - Large reconstruction model for single image to 3D: N/A
  - End-to-end recovery of human shape and pose: N/A
  - 3D Gaussian splatting for real-time radiance field rendering: N/A
  - URAvatar: Universal relightable Gaussian codec avatars: N/A
  - Neural volumes: Learning dynamic renderable volumes from images: N/A
  - SMPL: A skinned multi-person linear model: N/A
  - FaceLift: Learning generalizable single image 3D face reconstruction from synthetic heads: N/A
  - NeRF: Representing scenes as neural radiance fields for view synthesis: N/A
  - Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans: N/A
  - Structure-from-motion revisited: N/A
  - Pixelwise view selection for unstructured multi-view stereo: N/A
  - Attention is all you need: N/A
  - Large Gaussian reconstruction model for efficient 3D reconstruction and generation: N/A
  - Large reconstruction model for 3D Gaussian splatting: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A

### 3. Core Idea
- The proposed method utilizes a novel dataset of high-fidelity 3D scans of real people to train a reconstruction model that achieves hyper-realism in 3D avatars.

### 4. Method
- **Pipeline**: Capture, Canonicalize, Splat
- **Architecture / Loss / Training**: The model employs a transformer architecture with a loss function combining L1 photometric loss, perceptual loss, alpha loss, and scale regularization loss.
- **Complexity / Resources**: The model is trained on a dataset of 3.2K avatars and generates 5M renders for training.

</details>

## video understanding

### [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](http://arxiv.org/pdf/2510.14972v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Code Translation

### 2. Motivation & Gaps
- The paper addresses the challenges in translating code between programming languages while preserving functionality and semantics.

- **Related work challenges:**
  - Large language models (LLMs) for code: Reliance on subword tokenizers that do not align with programming language grammar.
  - Tokenization in programming languages: Deterministic tokenization in compilers contrasts with the statistical nature of LLM tokenization.
  - HumanEvalPack (Muennighoff et al., 2023): Inconsistent or fragmented representations of code due to tokenizer differences.
  - Code summarization (Hu et al., 2018; Panthaplackel et al., 2020): Evaluating the correctness of outputs generated from code summarization.
  - Code translation (Ahmad et al., 2023; Puri et al., 2021): Translating code snippets while maintaining semantic integrity.
  - N/A: N/A
  - Liu et al. (2025): Token merges across whitespace boundaries produce more meaningful units.
  - Chirkova and Troshin (2023): Tokenizer designed to better align with PL syntax, achieving lower token counts.
  - Chirkova and Troshin (2023): Tokenizers designed to better align with PL syntax still face challenges in preserving model performance.
  - Zheng et al. (2025): Instruction-tuned models show performance drops with unconventional tokenization.
  - Wang et al. (2025): Adversarial changes to token boundaries can significantly degrade model predictions.
  - N/A: N/A
  - HumanEval benchmark: Ensuring consistent translation of tests, entry points, and declarations.
  - Avatar and CodeNet benchmarks: Fixing harness-compatibility issues and handling pathological samples.
  - N/A: N/A

### 3. Core Idea
- The core idea is to mutatively rewrite code contexts by parsing and renaming identifiers while ensuring that the underlying problem semantics remain unchanged.

### 4. Method
- **Pipeline**: The method involves parsing code, identifying immutable and renameable identifiers, and applying consistent edits across code samples.
- **Architecture / Loss / Training**: Layer-wise analysis shows that the issue originates in early embeddings where subword segmentation fails to capture grammar token boundaries.
- **Complexity / Resources**: The experiments were conducted on an NVIDIA H100 GPU cluster, consuming approximately 1840 GPU-hours.

</details>

### [C4D: 4D Made from 3D through Dual Correspondences](http://arxiv.org/pdf/2510.14960v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Point Transformation using Linear Blend Displacement

### 2. Motivation & Gaps
- The method aims to achieve smooth and locally influenced transformations of 3D points based on the displacements of control points derived from smoothed tracking points.

- **Related work challenges:**
  - DUSt3R: Inaccurate results when applied to dynamic scenes due to reliance on multi-view geometric constraints.
  - DUSt3R: Sequential pipeline complexity and vulnerability to errors in sub-tasks.
  - MonST3R: Limited performance in dynamic scenes due to moving objects violating multi-view geometric constraints.
  - Optical Flow Estimation: Requires known camera parameters and is sensitive to outliers.
  - Camera Pose Estimation: Global alignment can misalign moving objects, affecting accuracy.
  - Point Tracking: Maintaining accuracy in dynamic scenes with occlusions.
  - DUSt3R: Requires ground-truth camera intrinsics for accurate pose estimation.
  - MASt3R: Limited to static scenes and does not generalize well to dynamic environments.
  - MonSt3R: Fine-tuned on dynamic datasets but lacks the ability to estimate camera intrinsics from monocular video.
  - RAFT: Only predicts position and occlusion of tracking points.
  - TAP-Net: Limited to predicting position and occlusion.
  - CoTracker: Does not predict mobility of tracking points.
  - N/A: N/A
  - DUSt3R: Exhibits zigzag artifacts in depth estimation.
  - MonST3R: Struggles to recognize dynamic regions under challenging conditions.
  - MASt3R: Lacks temporal smoothness in depth results.
  - MonST3R: Struggles with multi-tasking when using a sole CNN or 3D-aware encoder.
  - FlowP-SAM: State-of-the-art supervised method that our approach outperforms.
  - N/A: N/A

### 3. Core Idea
- Utilizing Linear Blend Displacement (LBD) to transform 3D points based on proximity-weighted displacements from control points.

### 4. Method
- **Pipeline**: The method involves obtaining smoothed 3D trajectories, identifying nearest control points, computing weights, aggregating displacements, and transforming query points.
- **Architecture / Loss / Training**: The architecture includes a CNN encoder and a 3D-aware encoder, with a focus on adaptive weighting for trajectory smoothing.
- **Complexity / Resources**: The training involves 50,000 steps with a batch size of 32, using a learning rate of 5e-4 and AdamW optimizer.

</details>

### [RealDPO: Real or Not Real, that is the Preference](http://arxiv.org/pdf/2510.14955v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Quality Assessment

### 2. Motivation & Gaps
- The paper aims to improve the evaluation of video generation quality by leveraging large language models (LLMs) for a more structured and scalable assessment.

- **Related work challenges:**
  - Reward-weighted regression (RWR): Susceptible to reward hacking, leading to a decline in video quality despite high scores.
  - Direct preference optimization (DPO): Limited scalability for high-resolution video generation.
  - Gradient feedback (GF): Multi-dimensional reward models may lose the ability to assess specific key metrics.
  - Diffusion-Based Video Generation: Limitations in motion dynamics and content richness.
  - Reinforce Learning in Image/Video Generation: Training reward models may suffer from hacking issues and reduced evaluation ability in specific domains.
  - Direct Preference Optimization (DPO): Avoiding complexities and biases introduced by learned reward models.
  - Yang et al., 2024b: Baseline model struggles with aligning generated content with human expectations.
  - Wang et al., 2024c: LiFT's reliance on reward models limits its effectiveness in real-world scenarios.
  - Liu et al., 2025: VideoAlign's naive DPO variant using synthetic data fails to capture real-world complexities.
  - VBench (Huang et al., 2024a;b): Lack of effective evaluation metrics for video generation.
  - LiFT (Wang et al., 2024c): Inability to complete complex actions like handshakes.
  - VideoAlign (Liu et al., 2025): Poor consistency in character appearance and actions.
  - Pre-trained base models: Underperform on specific tasks requiring fine-grained alignment.
  - Supervised fine-tuning methods: May not capture intricate details in video generation.
  - Existing alignment methods: Lack effectiveness in achieving superior performance across diverse samples.
  - Supervised Fine-Tuning: Effectiveness constrained by the quality and quantity of available annotations.
  - LiFT and VideoAlign: Reward models often fail to provide effective feedback in complex scenarios.
  - VBench-I2V: Providing objective and fine-grained assessments of video quality.
  - N/A: N/A

### 3. Core Idea
- Utilizing LLMs to evaluate generated videos based on structured templates that assess various quality dimensions.

### 4. Method
- **Pipeline**: The evaluation process involves using LLMs to score generated videos based on predefined criteria across multiple dimensions.
- **Architecture / Loss / Training**: RealDPO loss for aligning model predictions with preferred and non-preferred samples.
- **Complexity / Resources**: Conducted experiments on 8 Nvidia H100 GPUs with a total batch size of 8.

</details>

## model collapse

### [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](http://arxiv.org/pdf/2510.14981v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-view consistent image editing

### 2. Motivation & Gaps
- The proposed method offers a simple and efficient framework for multi-view consistent image editing.

- **Related work challenges:**
  - Haque et al., 2023: Relying on explicit 3D representations limits real-time applicability.
  - Jin et al., 2024: Time-consuming optimization processes hinder efficiency.
  - Liu et al., 2022: Inconsistent results when sampling from diffusion models independently.
  - Kawar et al., 2022: Requires additional assumptions in the forward process.
  - Haque et al., 2023: Prone to visual artifacts.
  - Litman et al., 2025: Training a multiview diffusion model for each task is computationally expensive.
  - Liu et al. (2022): Suffers from inconsistencies across frames.
  - Du et al. (2023): Suffers from inconsistencies across frames.
  - Richardson et al. (2023): Suffers from severe artifacts.
  - Hunyuan 3D: Results follow the prompt loosely when doing retexturing.
  - Hunyuan3D: Produces simple edits that often do not align with the text prompt.
  - Liu et al. (2022): Can introduce flickering artifacts in generated outputs.
  - Du et al. (2023): May incorrectly attribute lighting variance to view-dependent effects.
  - Generative multiview relighting for 3d reconstruction under extreme illumination variation: N/A
  - Magic fixup: Streamlining photo editing by watching dynamic videos: N/A
  - Met3r: Measuring multi-view consistency in generated images: N/A
  - Universal guidance for diffusion models: N/A
  - Multidiffusion: fusing diffusion paths for controlled image generation: N/A
  - Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing: N/A
  - 3D-Fixup: Advancing Photo Editing with 3D Priors: N/A
  - Improving diffusion models for inverse problems using manifold constraints: N/A
  - Denoising diffusion probabilistic models: N/A
  - Reduce, reuse, recycle: compositional generation with energy-based diffusion models and mcmc: N/A
  - Cat3d: Create anything in 3d with multi-view diffusion models: N/A
  - Instruct-nerf2nerf: Editing 3d scenes with instructions: N/A
  - Denoising diffusion restoration models: N/A
  - 3d gaussian splatting for real-time radiance field rendering: N/A
  - Synctweedies: A general generative framework based on synchronized diffusions: N/A
  - Inversion-free text-based editing using pre-trained flow models: N/A
  - Multi-view relighting with material-guided diffusion: N/A
  - Compositional visual generation with composable diffusion models: N/A
  - Flow straight and fast: Learning to generate and transfer data with rectified flow: N/A
  - Inference-time scaling for diffusion models beyond scaling denoising steps: N/A
  - Lightlab: Controlling light sources in images with diffusion models: N/A
  - Rethinking score distillation as a bridge between image distributions: N/A
  - Guided image synthesis and editing with stochastic differential equations: N/A
  - Representing scenes as neural radiance fields for view synthesis: N/A
  - Null-text inversion for editing real images using guided diffusion models: N/A
  - Editable image elements for controllable synthesis: N/A
  - Text-guided texturing of 3d shapes: N/A
  - High-resolution image synthesis with latent diffusion models: N/A
  - Multi-view diffusion for 3d generation: N/A
  - Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation: N/A
  - Simulating world inconsistency: N/A
  - Haque et al. (2023): Residual inconsistency in edited outputs compared to test-time optimization-based methods.
  - Weber et al. (2024): Increased memory and computational requirements when running both the 2D editing model and the multi-view diffusion model in parallel.
  - MV-Adapter: Can only generate a fixed set of camera views, limiting its utility for editing.
  - InstructNeRF2NeRF: Inability to gradually handle inconsistency with less dense camera coverage.

### 3. Core Idea
- The coupling term in stochastic sampling allows the model to correct for noise and maintain outputs within the training distribution.

### 4. Method
- **Pipeline**: Use Stable-Virtual-Camera (SVC) for processing multiple frames and edit a reference image as the conditioning view.
- **Architecture / Loss / Training**: SVC was trained with a shifted noise schedule compared to SD2.1 image models.
- **Complexity / Resources**: Experiments conducted using NVIDIA A6000 GPUs with a memory requirement equivalent to the combined memory of the two models.

</details>

### [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](http://arxiv.org/pdf/2510.14979v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual-language generation and long video understanding

### 2. Motivation & Gaps
- The need for an end-to-end training approach that eliminates biases and scales complexities in visual-language generation and video understanding.

- **Related work challenges:**
  - Fuyu Bavishi et al. (2023): Pioneering a route towards monolithic VLMs.
  - EVE Diao et al. (2024): Learning vision perception from scratch and mitigating vision-language conflicts.
  - HoVLE Tao et al. (2025): Addressing the limitations of modular designs in VLMs.
  - Li et al. (2024a;b): Rigid inductive biases in pretrained Vision Encoders limit resolution flexibility.
  - Wang et al. (2024a): Post-training interventions are often required to mitigate semantic biases.
  - Diao et al. (2024): Modular architectures impede design simplicity and deployment efficiency.
  - Prior methods in multimodal learning: Collapse visual tokens into 1D representations or reallocate pre-trained LLM head dimensions, limiting the modeling of spatial relationships.
  - 3D-RoPE methods: Mismatched channel and frequency allocation between visual and textual modalities, hindering effective multimodal interactions.
  - Existing LLM architectures: Inability to fully leverage multimodal capabilities due to rigid structural designs.
  - InternVL2.5: Limited performance in complex visual reasoning tasks.
  - Mono-InternVL: Struggles with high-resolution image understanding.
  - Qwen2.5-VL: Inadequate alignment between visual and linguistic data.
  - Qwen2-VL Wang et al. (2024a): Requires extensive pre-training data.
  - InternVL3 Zhu et al. (2025): Performance heavily reliant on large datasets.
  - Mono-InterVL Luo et al. (2024; 2025): Struggles with knowledge-/OCR-heavy tasks.
  - RoPE models: Existing models struggle with disentangling height, width, and temporal components, which impairs local semantics perception.
  - Pre-Buffer and Vision Encoders: Previous models have high training costs and do not efficiently utilize billion-scale training data.
  - N/A: N/A
  - N/A: N/A
  - Moma: efficient early-fusion pre-training with mixture of modality-aware experts: Efficiency in pre-training for multi-modal tasks.
  - Clevr-math: A dataset for compositional language, visual and mathematical reasoning: Complexity in reasoning across different modalities.
  - Aligning large multi-modal model with robust instruction tuning: Alignment of multi-modal models with instructions.
  - N/A: N/A
  - Qwen3VL: Highlighting concepts that resonate with design choices, including dense linking of visual-language features.
  - DeepStack: Establishing strong pixel-word associations from the earliest stages.
  - Existing visual-language models: Dependence on manually imposed biases and limitations in training from scratch.
  - Current language models: Inability to mitigate biases due to dominance of the language modality.

### 3. Core Idea
- NEO's architecture integrates video generation demands and related tasks systematically, allowing data and models to dictate the learning process.

### 4. Method
- **Pipeline**: End-to-end training within a unified architecture.
- **Architecture / Loss / Training**: Incorporates attention mechanisms and rotary positional encodings.
- **Complexity / Resources**: Constrained by current text corpus and computational resources.

</details>

### [Agentic Design of Compositional Machines](http://arxiv.org/pdf/2510.14980v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Construct a machine that can throw boulders in a parabolic trajectory, designed to maximize throw distance and height by utilizing the principle of leverage.

### 2. Motivation & Gaps
- The machine is failing due to mechanical design defects, specifically structural weaknesses and improper component placement.

- **Related work challenges:**
  - State-of-the-art LLMs: Current models fall short in capabilities required for compositional machine design.
  - Reinforcement learning (RL): Exploring RL as a path to improve LLM performance in machine design.
  - Automatic Theorem Proving: Compositional and exponentially large action space.
  - Electronic Design Automation (EDA): Requires spatial reasoning under constraints.
  - Fly-by-wire systems in aircraft: Dependence on control policies for stability.
  - Previous machine design methodologies: Inadequate encoding of machine structures and spatial reasoning.
  - Xiao et al., 2025: N/A
  - Teng et al., 2025: N/A
  - Zhang et al., 2025: N/A
  - Qiu et al., 2025b: Existing methods often struggle with precise spatial placement of machine parts.
  - Zhu et al., 2025b: Recent methods aim to prevent models from collapsing into a narrow set of strategies.
  - Schwarz et al., 2018: Finetuned LLMs must avoid catastrophic forgetting and maintain reasoning ability.
  - Cui et al., 2025: Loss of diversity in output entropy during reinforcement learning.
  - Ouyang et al., 2022: Need for methods that fully enable LLMs to explore novel solutions.
  - Tang et al., 2025: Mitigation methods like Pass@k training are not exhaustively explored.
  - Toolformer: Language models can teach themselves to use tools: Integrating language models with practical tools for CAD generation.
  - Self-refine: Iterative refinement with self-feedback: Improving the iterative process of model refinement in CAD applications.
  - 3d-gpt: Procedural 3d modeling with large language models: Enhancing procedural modeling capabilities in CAD systems.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Qwen3-Coder-480B-A35B-Instruct: Generated machines often fail in systematic ways, including flawed reasoning and structural attachment errors.
  - Gemini 2.5 Pro: A machineâ€™s appearance does not necessarily reflect its actual performance, highlighting the need for reward functions that account for both task performance and human perception.
  - Meta-Designer: Compounding error introduced by additional design stages.
  - Inspector Agent: Performance degradation in models with weaker 3D understanding.
  - Active Feedback Queries: Inconsistent performance across models without environment feedback.
  - N/A: N/A
  - Previous catapult designs: Balancing simplicity with functionality under constraints.
  - N/A: N/A
  - Rotating Block [10]: Causes excessive torque leading to structural failure.
  - Wooden Block support structure [6, 7, 8]: Inadequate to support the weight and forces generated during operation.
  - N/A: Structural integrity under high torque and instability due to asymmetric forces.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- Modify the mechanical design to ensure structural integrity and stability while launching the boulder.

### 4. Method
- **Pipeline**: Analyze the failure, identify root causes, and propose modifications to improve stability and reduce destructive forces.
- **Architecture / Loss / Training**: The LLM is fine-tuned from Qwen2.5-14B-Instruct with LoRA on all linear layers.
- **Complexity / Resources**: The machine's design involves multiple components including wooden blocks, a rotating block, rubber bands, and a boulder.

</details>
