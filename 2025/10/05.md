# Daily Paper Digest · 2025-10-05
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](http://arxiv.org/pdf/2510.02314v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Multi-view Attack in 3D Generative Systems

### 2. Motivation & Gaps
- The paper addresses vulnerabilities in 3D representation models by proposing a density-guided poisoning method that strategically injects illusory objects while maintaining scene consistency.

- **Related work challenges:**
  - IPA-NeRF: Limited applicability to explicit 3D scene representations like 3DGS.
  - Poison-Splat: Focuses on computational cost attacks rather than visible illusion embedding.
  - IPA-NeRF: Pioneered poisoning attacks against NeRF by inserting crafted samples at specific viewing angles.
  - Poison-Splat: Targeted 3DGS efficiency by generating samples that increase memory consumption.
  - Geometry Cloak: Prevents unauthorized 3D reconstruction from copyrighted images.
  - 3D Gaussian Splatting (3DGS): Maintaining multi-view consistency while embedding illusions.
  - Kernel Density Estimation (KDE): Identifying optimal low-density locations for embedding objects.
  - View Consistency Disruption: Weakening multi-view consistency without affecting the poisoned view.
  - IPA-NeRF: Accelerated training and rendering
  - IPA-Splat: Adapting IPA-NeRF for 3D Gaussian Splatting
  - IPA-NeRF: Existing methods struggle with maintaining quality in innocent views while embedding convincing illusions.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - IPA-NeRF (Nerfacto): Poor convergence on complex scenes
  - IPA-NeRF (Instant-NGP): Produces heavily distorted illusory objects
  - IPA-Splat: Limited effectiveness in high view overlap environments

### 3. Core Idea
- Our method demonstrates superior robustness and computational efficiency in generating illusory objects for backdoor attacks.

### 4. Method
- **Pipeline**: The method involves direct image poisoning, density-guided point cloud poisoning, and noise scheduling to achieve optimal attack effectiveness.
- **Architecture / Loss / Training**: The architecture focuses on maximizing V-ILLUSORY effectiveness while minimizing impact on V-TEST quality.
- **Complexity / Resources**: Reduces GPU memory usage by 41% and Gaussian points by 88%.

### 5. Experiments
- **Datasets & Metrics**: COCO 2017 dataset for illusory objects; Mip-NeRF 360 dataset for computational efficiency.
- **Baselines**: IPA-NeRF, IPA-NeRF (Instant-NGP), IPA-NeRF (Nerfacto), IPA-NeRF with Instant-NGP, IPA-NeRF with Nerfacto, IPA-Splat, N/A, Naive 3DGS (w/o attack), Poison-Splat
- **Main Results**: Our method achieves success rates of 64% to 83% across different threshold combinations, significantly outperforming existing approaches.
- **Ablations**: Combining direct replacement, density-guided poisoning, and multi-view consistency disruption achieves superior illusion embedding.
- **Limitations / Stress Tests**: Our method shows limitations in complex environments with high view overlap.

### 6. Takeaways
- **Pros**: First work addressing data poisoning attacks on 3D Gaussian Splatting., Identified and analyzed the robustness of 3DGS against prior poisoning techniques., Proposed a novel density-guided poisoning method tailored for 3DGS.
- **Cons**: The method may not generalize to all neural rendering architectures., Potential for high computational costs in certain scenarios., Limited exploration of countermeasures against the proposed attack.
- **Future Work**: Explore further enhancements to the density-guided poisoning method., Investigate additional vulnerabilities in 3D scene representation methods., Develop more robust defenses against such poisoning attacks.

</details>

### [Interactive Training: Feedback-Driven Neural Network Optimization](http://arxiv.org/pdf/2510.02297v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Neural Network Training Optimization

### 2. Motivation & Gaps
- The paper presents a framework that allows for real-time interventions in neural network training, enabling both humans and AI agents to adjust training parameters dynamically.

- **Related work challenges:**
  - Bergstra and Bengio, 2012: Static training paradigms do not allow for real-time adjustments.
  - Takase et al., 2023: Unstable loss dynamics and underperformance on specific tasks require human intervention.
  - Zhang et al., 2022: Prematurely terminating training jobs leads to inefficiencies and wasted computational resources.
  - Traditional static training methods: Lack of adaptability to changing training dynamics.
  - Existing training frameworks: Limited interactivity and real-time feedback mechanisms.
  - Human-in-the-loop training: Difficulty in integrating human interventions seamlessly into the training process.
  - Human-in-the-Loop Machine Learning: Existing methods often rely on predefined schedules or specific forms of input rather than real-time interventions.
  - Automated ML and Adaptive Optimization: Traditional methods for hyperparameter tuning do not incorporate real-time feedback from training dynamics.
  - Population-Based Training (PBT): PBT learns an automatic dynamic schedule of hyperparameters but does not allow real-time human intervention.
  - Reinforcement Learning for Scheduling: Existing methods typically remain advisory and do not integrate directly into the training loop.
  - Interactive Debugging Tools: Current tools allow analysis but lack the capability for immediate modifications during training.
  - Transformers: State-of-the-art natural language processing: N/A
  - Learning an adaptive learning rate schedule: N/A
  - Opt: Open pre-trained transformer language models: N/A

### 3. Core Idea
- The Interactive Training framework reimagines neural network training as an interactive process where both humans and AI agents can modify training strategies in real-time.

### 4. Method
- **Pipeline**: The framework allows for continuous monitoring and intervention based on live feedback during the training process.
- **Architecture / Loss / Training**: The architecture includes a learning rate scheduler that can be adjusted dynamically based on training performance.
- **Complexity / Resources**: The method requires expertise from human trainers or sophisticated AI agents to effectively intervene during training.

### 5. Experiments
- **Datasets & Metrics**: WikiText-2 train data
- **Baselines**: Fixed learning rate schedule, Human intervention baseline, N/A, Static baseline with fixed learning rate schedule, Traditional static optimization methods, Traditional static training paradigms
- **Main Results**: The framework shows improved accuracy, reduced sensitivity to initial hyperparameters, and real-time adaptation to application needs.
- **Ablations**: The study included comparisons between human interventions and LLM-based automated interventions.
- **Limitations / Stress Tests**: Reproducibility issues were noted, as different interventions could lead to varying outcomes.

### 6. Takeaways
- **Pros**: Real-time adjustments improve training outcomes., Dynamic intervention reduces the need for manual restarts., Supports both human and AI-driven optimization.
- **Cons**: Requires initial setup and understanding of the framework., Potential complexity in managing real-time interventions.
- **Future Work**: Further development of automated AI agents for training., Exploration of fully interactive training paradigms., Integration with more diverse training tasks and datasets.

</details>

### [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](http://arxiv.org/pdf/2510.02278v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Traffic Forecasting

### 2. Motivation & Gaps
- The proposed datasets contain important road attributes that strongly affect traffic speed and volume, which are necessary for precise traffic forecasting.

- **Related work challenges:**
  - Jagadish et al., 2014: Existing datasets have a small number of locations (road segments) with available measurements.
  - Li et al., 2018: No graph structure based on road connectivity is available between sensors.
  - Yu et al., 2018: Measurements fail to capture complex urban traffic within cities.
  - Attention-based Spatial-Temporal Graph Convolutional Networks (ASTGCN): Complex implementation and computational overhead.
  - Graph WaveNet (GWN): Difficulty in learning latent spatial structure and long-range temporal dependencies efficiently.
  - Adaptive Graph Convolutional Recurrent Network (AGCRN): Decoupling model performance from predefined graph structures is challenging.
  - Previous Traffic Forecasting Datasets: Heuristic edge construction based on travel distance rather than actual road connectivity.
  - Existing Neural Spatiotemporal Models: Inability to handle large and complex datasets effectively.
  - Torch Spatiotemporal (Cini & Marisca, 2022): Limited models can be trained on large datasets due to high resource demands.
  - LargeST (Liu et al., 2023): Existing models require long training times and are not scalable.
  - DCRNN: Exhibits significantly longer training times and fails to complete within 250 hours for larger lookback windows.
  - STGCN: Scalability issues complicate its application in real-world systems.
  - GWN: Long training times as lookback window increases.
  - Graph neural network for traffic forecasting: A survey: Limited ability to capture dynamic spatial-temporal dependencies.
  - Dynamic spatial-temporal aware graph neural network for traffic flow forecasting: Challenges in real-time data processing and prediction accuracy.
  - Diffusion convolutional recurrent neural network: Data-driven traffic forecasting: Inadequate handling of long-term dependencies in time series data.
  - city-traffic-M: Higher density and branching structure with a uniform road network.
  - city-traffic-L: Complex structure with bottlenecks and high-traffic corridors due to geographical features.
  - Previous datasets: Do not include important road attributes affecting traffic forecasting.

### 3. Core Idea
- Utilizing learnable node embeddings and additional temporal features for improved traffic forecasting.

### 4. Method
- **Pipeline**: Use of learnable node embeddings and static features along with temporal features encoded with one-hot and periodic trigonometric functions.
- **Architecture / Loss / Training**: Trained using the AdamW optimizer with a fixed learning rate of 0.0003 for 5 epochs, repeated 3 times for mean and standard deviation.
- **Complexity / Resources**: All experiments conducted on a single NVIDIA A100 GPU with 80GB of VRAM.

### 5. Experiments
- **Datasets & Metrics**: The datasets include various static attributes and traffic dynamics metrics, analyzed for different road subsets based on features like speed limit and road conditions.
- **Baselines**: ASTGCN, DCRNN, GNN-Mean, GNN-TrfAttn, GRUGCN, GWN, Global mean/median, METR-LA, N/A, Node-wise mean/median, Other machine learning models, PEMS-BAY, Previous 1 day/week ago, Previous strategy, STGCN, Traditional time series forecasting methods, linear model, naive baselines
- **Main Results**: Traffic volume and speed vary significantly based on road features such as speed limits and conditions.
- **Ablations**: Conducted ablation studies to assess the impact of different model components on performance.
- **Limitations / Stress Tests**: If the model exceeds memory limits, attempts are made to decrease model parameters; if it still fails, OOM is reported.

### 6. Takeaways
- **Pros**: Provides a realistic benchmark for urban traffic forecasting., Datasets contain rich features for better model training., Demonstrates improved scalability of the proposed GNN approach.
- **Cons**: Existing models struggle with the proposed dataset size., Limited testing on other urban environments.
- **Future Work**: Encourage further advancements in traffic forecasting., Support progress in urban computing and smart city development., Explore additional modeling techniques for improved performance.

</details>

## Gaussian Splatting

### [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](http://arxiv.org/pdf/2510.02308v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Tangent space estimation and embedding of noisy data

### 2. Motivation & Gaps
- The paper addresses the challenges of estimating tangent spaces and embedding noisy data while preserving intrinsic geometry.

- **Related work challenges:**
  - Local Principal Component Analysis (LPCA): Struggles in high-noise settings and requires prior knowledge of geometric and noise characteristics.
  - Adaptive neighborhood size selection: Hindered by unknown geometric quantities such as curvature, reach, and noise level.
  - LPCA: LPCA is not robust to noise, resulting in poor tangent space estimates.
  - Previous methods for tangent space estimation: Struggled with noise and spurious edges in nearest neighbor graphs.
  - Graph Laplacian approaches: May not effectively capture the intrinsic geometry of the data.
  - Eigenvector-based methods: Require careful selection of hyperparameters to avoid noise interference.
  - Previous methods for tangent space estimation: Often fail to account for the influence of vertical energy on eigenvalues.
  - Davis-Kahan theorem: Establishing the conditions under which Laplacian eigenvectors remain stable under noise perturbations.
  - Previous studies on Laplacian eigenvectors: Limited understanding of the effects of noise on low-frequency eigengaps.
  - LPCA: Highly sensitive to noise, leading to inaccurate tangent space estimates.
  - LEGO: Requires robust performance across varying noise levels and hyperparameters.
  - LPCA: Sensitivity to noise and inability to accurately recover local intrinsic geometry.
  - LEGO: Requires careful handling of noise to maintain the integrity of the embedding.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - Mikhail Belkin and Partha Niyogi. “Towards a theoretical foundation for Laplacian-based manifold methods”: N/A
  - Amit Singer. “From graph to manifold Laplacian: The convergence rate”: N/A
  - Nicolás García Trillos et al. “Error estimates for spectral convergence of the graph Laplacian on random geometric graphs toward the Laplace–Beltrami operator”: N/A
  - Xiuyuan Cheng and Boris Landa. “Bi-stochastically normalized graph Laplacian: convergence to manifold Laplacian and robustness to outlier noise”: N/A
  - Yariv Aizenbud and Barak Sober. “Non-parametric estimation of manifolds from noisy data”: N/A
  - Yariv Aizenbud and Barak Sober. “Estimation of Local Geometric Structure on Manifolds from Noisy Data”: N/A
  - Anna V Little. “Estimating the intrinsic dimension of high-dimensional data sets: a multiscale, geometric approach”: N/A
  - Anna V Little, Mauro Maggioni, and Lorenzo Rosasco. “Multiscale geometric methods for data sets I: Multiscale SVD, noise and curvature”: N/A
  - Christopher R Genovese et al. “Minimax manifold estimation”: N/A
  - Stefan Haag, Jonas Lampart, and Stefan Teufel. “Generalised quantum waveguides”: N/A
  - Daniel Hsu, Sham Kakade, and Tong Zhang. “A tail inequality for quadratic forms of subgaussian random vectors”: N/A
  - Andrew V Knyazev and Merico E Argentati. “Principal angles between subspaces in an A-based scalar product: algorithms and perturbation estimates”: N/A
  - Roy R Lederman and Ronen Talmon. “Learning the geometry of common latent variables using alternating-diffusion”: N/A
  - Shuyang Ling. “Generalized power method for generalized orthogonal Procrustes problem: global convergence and optimization landscape analysis”: N/A
  - Dhruv Kohli, Gal Mishne, and Alexander Cloninger. “Non-degenerate rigid alignment in a patch framework”: N/A
  - Shankar Krishnan et al. “Global registration of multiple 3D point sets via optimization-on-a-manifold.”: N/A
  - Boris Landa and Xiuyuan Cheng. “Robust inference of manifold density and geometry by doubly stochastic scaling”: N/A

### 3. Core Idea
- The proposed method utilizes a tear-enabled alignment framework to produce injective embeddings of data lying on closed manifolds.

### 4. Method
- **Pipeline**: Data is first denoised and dimensionality reduced, followed by tangent space estimation using LPCA and LEGO, and finally embedding using a tear-enabled alignment framework.
- **Architecture / Loss / Training**: The method focuses on preserving the intrinsic geometry of the data while embedding it into a lower-dimensional space.
- **Complexity / Resources**: The method requires computational resources for dimensionality reduction and tangent space estimation, particularly for larger datasets.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize synthetic noisy datasets and evaluate the embeddings based on visual clarity and variance explained.
- **Baselines**: LEGO, LPCA, Local Principal Component Analysis (LPCA), N/A, Noise-robust methods, Other Laplacian-based approaches, Previous eigenfunction-based approaches, Previous methods for Laplacian eigenvector stability, Standard tangent space estimation methods, Traditional PCA methods
- **Main Results**: LEGO outperforms LPCA in capturing the underlying 2D structure and functional variance.
- **Ablations**: Ablation studies demonstrate the impact of different parameters on the quality of the embeddings.
- **Limitations / Stress Tests**: The method's performance is limited by the noise level in the data and the choice of parameters.

### 6. Takeaways
- **Pros**: LEGO provides more robust tangent space estimates in noisy environments., Utilizes global structure of data for local tangent space estimation., Theoretical justifications support the effectiveness of the method.
- **Cons**: Requires understanding of graph Laplacian eigenvectors., May still be affected by noise in certain conditions.
- **Future Work**: Explore further applications in manifold learning and data denoising., Investigate adaptive methods for neighborhood size selection., Develop enhancements to improve robustness in extreme noise conditions.

</details>

### [ALMA Deep Field in SSA22: Reconstructed [CII] Luminosity Function at z = 6](http://arxiv.org/pdf/2510.02303v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Examine completeness and contamination for ADF22 data and discuss non-detection explanations.

### 2. Motivation & Gaps
- The study aims to understand the discrepancies in the detection of line emitters in ALMA data, particularly focusing on the [Cii] luminosity function and the implications for star formation activity in the universe.

- **Related work challenges:**
  - ADF22 line survey: Reported spurious detections of high-z line-emitting source candidates.
  - Previous source-finding methods: Underestimation of contamination rates due to limited data.
  - Hayatsu et al. (2017): Detection of faint emission-line sources and the high contamination rate in the observed data.
  - González-López et al. (2017): Statistical fluctuations leading to false detections in large ALMA survey datasets.
  - N/A: N/A
  - Hayatsu et al. (2017): Detection reliability and contamination rate evaluation.
  - Carilli and Walter (2013): Calculating luminosity from flux.
  - Williams, de Geus, and Blitz (1994): Source extraction methods.
  - Hatsukade et al. (2016): N/A
  - Aravena et al. (2016): N/A
  - Williams, de Geus, and Blitz (1994): N/A
  - Díaz-Santos et al. 2013: N/A
  - Swinbank et al. 2012: N/A
  - Hemmati et al. 2017: N/A
  - Hayatsu et al. 2019: N/A
  - Capak et al. 2015: N/A
  - Aravena et al. 2016: N/A
  - N/A: N/A
  - Hemmati et al. (2017): The original function is a double power law.
  - De Looze et al. (2014): SFR–L[CII] relations may not be accurate for normal [Cii] emitters.
  - Farrah et al. (2013): Weak SFRD constraints using other FIR lines.
  - N/A: False detection of high-SN ratio is unavoidable due to existing clump-like structures.
  - N/A: N/A

### 3. Core Idea
- The reconstruction method for line LF can be applied to other blind line surveys using ALMA deep field datacubes.

### 4. Method
- **Pipeline**: Utilization of mock observational data and Monte Carlo simulations to analyze detection rates and contamination.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Analysis conducted using CASA software version 4.5.3.

### 5. Experiments
- **Datasets & Metrics**: ALMA data from various cycles including Cycle-2 and Cycle-5.
- **Baselines**: FDM data, N/A, Previous source-finding methods, TDM data
- **Main Results**: Confirmed that previously detected emitters/candidates are classified as unreliable.
- **Ablations**: Comparison of clump-finding results between TDM and FDM correlator datacubes.
- **Limitations / Stress Tests**: Discussion on the limitations of the CASA simulator and the impact of statistical fluctuations on detection.

### 6. Takeaways
- **Pros**: Improved understanding of false detection rates in high-SN observations., Method can be applied to future blind line surveys., Confirmed negligible non-Gaussian noise effects.
- **Cons**: Spurious detections complicate the analysis., Limited number of real ADF22 datacubes may skew results., Technical issues in data processing can affect outcomes.
- **Future Work**: Further refinement of the luminosity function estimation method., Exploration of additional observational techniques., Investigation of other high-redshift line emissions.

</details>

## avatar

### [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](http://arxiv.org/pdf/2510.01619v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Realistic garment dynamics simulation

### 2. Motivation & Gaps
- Existing physics-based avatar methods struggle with accurate garment dynamics and rendering quality.

- **Related work challenges:**
  - PhysAvatar: Fails when animation inputs have a small degree of self-penetration, causing simulation failures.
  - Existing methods using piecewise linear transformations: Limited in accurately capturing complex deformations and tend to overfit to motions observed during training.
  - Xiang et al.: Relies on a time-consuming manual parameter search to approximate reasonable cloth behavior.
  - C-IPC: Fails to resolve collisions for noisy colliders during Continuous Collision Detection.
  - Learning-based simulation methods: Limited generalizability beyond training dynamics and cannot guarantee physically plausible deformations.
  - DiffAvatar: Omitted appearance modeling and tailored for scan-based asset preparation rather than dynamic avatar reconstruction.
  - Material Point Method (MPM): Mainly used for modeling the dynamics of general objects, not specifically tailored for garment dynamics.
  - Anisotropic constitutive model: Requires a Lagrangian mesh to track material directions, complicating the modeling of garments.
  - Existing collision handling algorithms: Designed for external objects, not effective for colliders represented as meshes.
  - PhysAvatar [78]: Limited applicability in real-world scenarios due to ideal conditions assumed in existing approaches.
  - PhysAvatar [78]: Fails when driving body mesh colliders have self-penetrations.
  - C-IPC [31]: Takes a long time to converge for resolving complex collisions.
  - Learning-based simulators [8, 7]: Less effective in modeling unseen dynamics.
  - N/A: N/A
  - PhysAvatar: Achieving accurate garment dynamics and high rendering quality.
  - PhysAvatar [78]: Inaccurate garment dynamics and rendering quality.
  - Gaussian Garments [55]: Struggles to produce physically accurate deformations.
  - MMLPHuman [74]: Exhibits unnatural surface artifacts or discontinuities under challenging poses.
  - Gaussian Garments [55]: Struggles to capture physical laws under settings where physical parameters must be estimated from only one second of motion, leading to high geometric error.
  - MMLPHuman [74]: Lacks explicit surface modeling and physical understanding, producing unrealistic surface artifacts or broken geometry when encountering unseen poses.
  - Finite-Difference Optimization: Scalability with increasing parameters
  - Relighting-aware extensions for Gaussian avatars: Current framework does not support relightable rendering
  - Generative priors for inpainting unobserved regions: Rendering quality degradation for occluded or unseen parts

### 3. Core Idea
- The paper presents an anisotropic constitutive model for simulating realistic garment behavior, capturing the strain-energy density function to compute stress tensors for dynamic human avatar reconstruction.

### 4. Method
- **Pipeline**: The pipeline includes garment dynamics simulation using MPM and mesh-based collision handling.
- **Architecture / Loss / Training**: Utilizes a constitutive model for anisotropic elastoplasticity and physical parameter learning.
- **Complexity / Resources**: Simulation runs at approximately 1.1 seconds per frame on a single NVIDIA GeForce RTX 4090.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on the ActorsHQ [14] dataset using metrics like penetration depth and physical plausibility scores.
- **Baselines**: ARAH [65], C-IPC, Existing MPM collision handling algorithms, GS-Avatar [12], Gaussian Garments [55], MMLPHuman [74], N/A, PhysAvatar, PhysAvatar [78], TA V A [32], XPBD
- **Main Results**: Our method outperforms both baselines across all geometry and appearance metrics.
- **Ablations**: Ablation studies validate the importance of key components like the constitutive model and physical parameter learning.
- **Limitations / Stress Tests**: Quantitative evaluation of physical plausibility is challenging due to the lack of ground-truth physical annotations.

### 6. Takeaways
- **Pros**: Supports physically realistic and robust animations for loose garments., Achieves high-fidelity rendering from free viewpoints., Demonstrates zero-shot generalizability to novel scene interactions.
- **Cons**: Requires complex tailoring of the MPM simulator for effective garment dynamics., High computational resources needed for simulation and rendering., Limited by the quality of multi-view video input.
- **Future Work**: Explore further generalizability to more complex interactions., Investigate improvements in rendering techniques., Enhance the efficiency of the simulation process.

</details>

### [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](http://arxiv.org/pdf/2510.01182v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Empirical analysis of bug reports in multi-user XR systems

### 2. Motivation & Gaps
- The study aims to understand the unique challenges and characteristics of bugs in multi-user XR systems, which differ from traditional software systems.

- **Related work challenges:**
  - Research on software bugs in various domains: Significant gap in understanding bugs specifically arising from multi-user interactions in XR environments.
  - Existing studies on distributed systems: Do not account for the unique complexities of multi-user XR environments.
  - Previous studies on XR systems: Lack of systematic analysis of bug reports to identify patterns and trends.
  - Asymmetric Interaction Capabilities: Different abilities to interact with the shared environment based on platform.
  - Loss/Hijack of Control: Users losing control or having their camera view taken over by others.
  - Communication & Awareness Impairment: Impairments in social and awareness mechanisms affecting collaborative experiences.
  - Previous studies on software bugs: Traditional software bugs are often tolerable, whereas XR bugs can break immersion, raising the stakes for quality assurance.
  - Existing QA methodologies: Current methodologies may not effectively address the unique challenges posed by multi-user XR environments.
  - Hubs-Foundation/hubs issue #5586: Orphaned state of objects when both creator and owner leave the session.
  - Hubs-Foundation/hubs issue #1000: Implicit ownership leading to invisibility of objects for new users.
  - Hubs-Foundation/hubs issue #4892: Pinned objects disappearing when their creator leaves.
  - Layer demonstrated how game-specific logic failed to account for multiple players: Developers often retrofit single-player logic for multiplayer scenarios without fully considering the implications of multiple actors.
  - Configuration Sensitivity: Many bugs may represent inadequate default configurations or poor configuration documentation.
  - Layered Complexity: Effective debugging requires understanding the full stack from application logic through networking to platform-specific implementations.
  - Traditional software analysis techniques: Fail to capture the unique characteristics of multi-user XR bugs.
  - Current debugging tools: Ill-equipped to handle the distributed, real-time nature of multi-user XR applications.
  - Existing SDKs and APIs: May not provide sufficiently intuitive or well-documented APIs for multi-user scenarios.
  - Rodriguez and Wang [70]: Analyzed trends and challenges in VR software projects.
  - Adams et al. [45]: Revealed privacy and security threats in VR applications.
  - Miller et al. [65]: Leveraged machine learning to prove personal identifiability with user tracking data.
  - Extended Reality: Its Challenges, Usage and Future Ahead: N/A
  - Software Testing for AR/VR: Ensuring Bug-Free Experiences: N/A
  - Evaluation of XR Applications: A Tertiary Review: N/A
  - N/A: N/A

### 3. Core Idea
- The paper develops comprehensive taxonomies categorizing symptoms, root causes, and consequences of XR defects based on an analysis of 2,649 real-world bug reports.

### 4. Method
- **Pipeline**: Empirical analysis of bug reports to identify and categorize defects.
- **Architecture / Loss / Training**: Rigorous qualitative analysis using iterative open coding to develop taxonomies characterizing multi-user XR bugs.
- **Complexity / Resources**: The analysis required multiple iterations and a hierarchical taxonomy for categorizing bugs.

### 5. Experiments
- **Datasets & Metrics**: Analysis of 2,649 real-world bug reports from various XR platforms.
- **Baselines**: Existing debugging tools, Hubs-Foundation/hubs, Multi-user systems, N/A, Single-user XR applications, Single-user systems, Traditional distributed systems bug studies, Traditional multi-user applications, Traditional online collaboration tools, Traditional software analysis techniques, Traditional software bug analysis methods, Unity SDK, Unreal Engine
- **Main Results**: Over 34% of defects cause severe disruptions like crashes and interaction breakdowns.
- **Ablations**: N/A
- **Limitations / Stress Tests**: Findings may not fully reflect emerging technologies or practices due to the rapidly evolving XR landscape.

### 6. Takeaways
- **Pros**: Provides actionable recommendations for developers and platform vendors., Develops a comprehensive taxonomy for multi-user XR bugs., Highlights unique challenges faced by multi-user XR systems.
- **Cons**: Limited understanding of multi-user XR bugs compared to other domains., Fragmented nature of bug reporting complicates knowledge aggregation., Potential privacy and health implications remain underexplored.
- **Future Work**: Further research on automated quality assurance tools for multi-user XR., Exploration of privacy and health implications in multi-user XR contexts., Development of specialized approaches to testing and debugging in multi-user XR.

</details>

### [Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/pdf/2510.01176v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Facial Animation Generation

### 2. Motivation & Gaps
- Existing methods for facial animation often produce spatiotemporally inconsistent results, leading to unnatural avatars.

- **Related work challenges:**
  - Fan et al. 2022; Richard et al. 2021; Xing et al. 2023: Existing methods lack sufficient detail for conveying subtle facial cues and often operate offline, limiting real-time performance.
  - Li et al. 2024b; Ng et al. 2024: Recent approaches synthesize high-fidelity avatars but primarily require entire audio sequences as input, not processing audio streams in real-time.
  - GSTalker: Limited generalization to multiple identities.
  - GaussianTalker: Requires offline computation for audio-based deformations.
  - EmoTalk3D: Computationally expensive and lacks real-time performance.
  - Universal Relightable Prior Model [Li et al. 2024a]: Existing models may not effectively handle real-time performance and the integration of gaze direction.
  - Diffusion models for facial expression generation: Inherent slowness of diffusion models during inference.
  - Graph-based gaze synthesis: Ensuring smooth and consistent gaze transitions.
  - Audio encoders for real-time applications: Maintaining causality in audio processing without future information.
  - GaussianTalker: Generates high-fidelity 3D face animation but is not designed for real-time applications.
  - TalkingGaussian: Focuses on person-specific 3D deformation and rendering, lacking universality.
  - GaussianTalker [Cho et al. 2024]: Generates high-fidelity 3DGS face animation but learns person-specific deformations.
  - TalkingGaussian [Li et al. 2024b]: Similar to GaussianTalker, it does not provide a direct online-based solution.
  - DiffPoseTalk [Sun et al. 2024]: Requires style conditioning which is not available in real-world scenarios.
  - wav2vec 1.0 [Schneider et al. 2019]: Non-causal encoding limitations in real-time applications.
  - wav2vec 2.0 [Baevski et al. 2020]: Maintaining accuracy while achieving temporal consistency.
  - N/A: N/A
  - FaceFormer: High latency and unsuitability for real-time applications.
  - CodeTalker: High computational overhead and autoregressive nature.
  - AniPortrait: Blurry and distorted artifacts in generated images.
  - Wav2vec 1.0: Preserves causality but limits representation quality.
  - Wav2vec 2.0: Utilizes non-causal CNN layers, which may disrupt temporal coherence.
  - HuBERT: Relies on non-causal CNN layers, affecting causality.
  - N/A: N/A

### 3. Core Idea
- The proposed method generates high-fidelity avatars with synchronized lip movements by utilizing a gaze graph and a transformer architecture.

### 4. Method
- **Pipeline**: The method involves capturing audio and gaze data, processing it through a transformer architecture, and synthesizing facial animations based on the processed data.
- **Architecture / Loss / Training**: Utilizes a self-attention mechanism with a windowed mask to maintain temporal coherence and reduce boundary issues.
- **Complexity / Resources**: distributed data-parallel (DDP) using two A100 GPUs; Distillation training and emotion-conditioning training are done with a single A100 GPU.

### 5. Experiments
- **Datasets & Metrics**: Out of 265 capture subjects, we use 237 for training and 28 for testing. Data are segmented into sequences of frame length 100 (in 30FPS).
- **Baselines**: AniPortrait, Audio2Photoreal, Audio2Photoreal-Face, CodeTalker, DiffPoseTalk, EmoTalk3D, Existing diffusion models, Existing offline state-of-the-art methods, FaceFormer, GSTalker, GaussianTalker, HuBERT, N/A, TalkShow, TalkShow-Face, Traditional regression models for facial expression generation, Wav2vec 1.0, Wav2vec 2.0, wav2vec 1.0, wav2vec 2.0
- **Main Results**: Quantitative comparison experiments on freeform speech and sentence reading data.
- **Ablations**: Ablation studies demonstrate the importance of the gaze graph and transformer architecture in achieving high fidelity.
- **Limitations / Stress Tests**: Out of 28 test subjects, two were excluded for excessive frame drops and less than 70 segments can be used for freeform speech; two were excluded for excessive frame drops and less than 30 segments can be used for sentence reading.

### 6. Takeaways
- **Pros**: High fidelity and universal 3D facial avatars in real-time., Significant improvements in animation accuracy., Versatile framework for multimodal applications.
- **Cons**: Limited generalization across diverse identities., Potential computational overhead in complex scenarios., Dependence on audio quality for accurate expression generation.
- **Future Work**: Explore further multimodal applications., Enhance real-time performance in diverse scenarios., Investigate integration with additional sensory inputs.

</details>

## video understanding

### [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](http://arxiv.org/pdf/2510.02313v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- sounding object detection

### 2. Motivation & Gaps
- The paper addresses the challenge of aligning audio and visual modalities for sound source localization.

- **Related work challenges:**
  - Existing multimodal techniques: They often rely on global features and predefined sound categories, making it difficult to differentiate subtle sound impacts caused by different materials.
  - Current learning methods: They face challenges in understanding localized, object-centric interactions.
  - Multimodal object-centric representation learning: Previous works have used synthetic or laboratory-collected data, which do not scale well to real-world object interactions.
  - Slot Attention Models: Compressing image features into slot vectors for object segmentation.
  - Multimodal Datasets: Data is often synthetically generated or collected in controlled environments.
  - Audiovisual Localization: Requires precise boundary predictions which may not align with the task of identifying involved objects.
  - Previous object-centric learning methods: Directly applying masks to input to remove redundant visual tokens
  - SoundingActions: Limited to global representations without object awareness.
  - DenseA V: Focuses on audiovisual segmentation but lacks specific object interaction context.
  - SLA VC: Uses a localization framework that may not fully capture the nuances of object interactions.
  - SoundingActions: Previous state of the art but lacks object-awareness.
  - ImageBind: Trained on large-scale data but does not focus on object interactions.
  - LanguageBind: Similar to ImageBind, lacks specific focus on audio-visual correspondences.
  - N/A: N/A
  - Learning object permanence from video: N/A
  - Understanding human hands in contact at internet scale: N/A
  - Semantic object prediction and spatial sound super-resolution with binaural sounds: N/A
  - N/A: N/A

### 3. Core Idea
- extract ground truth object masks.

### 4. Method
- **Pipeline**: The method involves a multi-modal approach that integrates audio and visual embeddings for improved localization.
- **Architecture / Loss / Training**: Utilizes a training loss similar to SoundingActions but incorporates object-aware training.
- **Complexity / Resources**: The model uses a combination of pretrained encoders for audio, visual, and language modalities.

### 5. Experiments
- **Datasets & Metrics**: Epic Kitchens, Ego4D
- **Baselines**: Audiovisual Localization Models, Contrastive learning approaches, DenseA V, Existing multimodal action understanding tasks, ImageBind, LanguageBind, N/A, Previous object-centric learning methods, SLA VC, SSLAlign, Slot Attention Models, SoundingActions
- **Main Results**: Additional qualitative results for sounding object detection.
- **Ablations**: Two ablation studies were conducted: one without object-aware masking and another without slot attention.
- **Limitations / Stress Tests**: The model's performance is limited by the complexity of localizing sounds in diverse scenarios.

### 6. Takeaways
- **Pros**: Achieves state-of-the-art performance on new tasks., Utilizes a novel multimodal object-aware approach., Scales well to large datasets with diverse object interactions.
- **Cons**: Requires significant computational resources., May struggle with very subtle sound distinctions., Limited by the quality of the training data annotations.
- **Future Work**: Explore additional modalities for richer interactions., Investigate real-time applications of the model., Enhance the model's ability to generalize to unseen objects.

</details>

### [Inferring Dynamic Physical Properties from Video Foundation Models](http://arxiv.org/pdf/2510.02311v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Comparative analysis of elasticity in video frames

### 2. Motivation & Gaps
- The study aims to analyze and compare the elasticity characteristics of different objects as observed in video frames.

- **Related work challenges:**
  - Wu et al., 2015; Ding et al., 2021; Jatavallabhula et al., 2021: Early methods rely heavily on simulation supervision and handcrafted heuristics.
  - Voleti et al., 2022; Lu et al., 2023: Unsupervised learning captures latent dynamics but lacks interpretability in terms of concrete physical quantities.
  - Kawabe et al., 2014; Paulun et al., 2015: Later works infer attributes from task-specific visual cues, which may not generalize well.
  - Bordes et al., 2018: Focus on qualitative and categorical questions in video-image-text QA tasks.
  - Tung et al., 2023: Limited to qualitative assessments without quantitative measures.
  - Bear et al., 2021: Does not address the estimation of physical properties from videos.
  - Classical computer vision techniques: Limited accuracy in estimating physical properties due to reliance on heuristics.
  - Video generative models: Need for effective feature extraction and representation learning for physical property estimation.
  - Multimodal large language models (MLLMs): Integrating visual and textual information for accurate predictions.
  - DynamiCrafter (Xing et al., 2024): Effective feature extraction for 3D physics.
  - V-JEPA-2 (Assran et al., 2025): Utilizing self-supervised models for video feature extraction.
  - Shtedritski et al. (2023): Mitigating the sim-to-real gap in model training.
  - Video Generative Model: Struggles with generalization to real-world data for friction due to reliance on visual references.
  - Video Self-Supervised Model: Similar struggles with friction estimation and performance degradation on out-of-distribution data.
  - Multimodal Large Language Models (MLLMs): Performance drops significantly on synthetic splits, indicating a reliance on semantic rather than visual cues.
  - Learning to poke by poking: Experiential learning of intuitive physics: Models fall short of the oracle, particularly in absolute value prediction.
  - V-jepa: Latent video prediction for visual representation learning: Generative and self-supervised models have similar performance but need improvement in physical reasoning.
  - Physion: Evaluating physical prediction from vision in humans and machines: MLLMs perform worse overall but improve with more informative prompting.
  - Genesis: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: Accurate estimation of restitution coefficient from visual cues
  - Previous studies on elasticity analysis: Limited accuracy in estimating restitution coefficients from visual data.
  - Comparative analysis of elasticity in materials: Difficulty in determining which material exhibits higher elasticity based solely on visual input.
  - Previous studies on material elasticity: Lack of standardized methods for visual analysis of elasticity in videos.
  - Previous studies on elasticity analysis: Limited interpretability and stability in few-shot examples.
  - DynamiCrafter: N/A
  - V-JEPA-2: N/A
  - Qwen2.5VL-max: Limitation of resources, using a random subset of 100 samples.
  - GPT-4o: Limitation of resources, using a random subset of 100 samples.
  - Gemini-2.5-pro: Limitation of resources, using a random subset of 100 samples.

### 3. Core Idea
- To determine which of two instances has a greater elasticity value based on visual analysis of video frames.

### 4. Method
- **Pipeline**: Analysis of video frames to assess elasticity characteristics.
- **Architecture / Loss / Training**: Simple architecture to extract physical cues; models trained for generative tasks perform comparably to self-supervised models.
- **Complexity / Resources**: Requires video analysis tools and computational resources for frame extraction and analysis.

### 5. Experiments
- **Datasets & Metrics**: Videos showcasing different objects with varying elasticity properties.
- **Baselines**: Baseline, Baseline model for elasticity analysis, Baseline model for elasticity estimation, Classical estimation methods, Existing machine learning approaches, Existing vision-language models, Few-Shot Examples, Frame Index Provided, GPT-4o, GPT-4o (Hurst et al., 2024), Gemini 2.5 Pro (Comanici et al., 2025), Gemini-2.5-pro, Generative models, Multi-modal large language models (MLLMs), N/A, Oracle Estimation Teaching, Oracle Estimator, Oracle estimation teaching method, Previous elasticity analysis methods, Qwen2.5-VL-Max (Hui et al., 2024), Qwen2.5VL-max, Self-supervised models, Video Generative Model, Video Self-Supervised Model, With Red circle, Without red circle
- **Main Results**: The performance on the real test split test-3 improved from 0.47 to 0.84 with the red circle.
- **Ablations**: An ablation study was conducted using DynamiCrafter for the relative formulation of the elasticity property.
- **Limitations / Stress Tests**: Challenges in accurately estimating coefficients due to visual ambiguities in frame analysis.

### 6. Takeaways
- **Pros**: New dataset PhysVid facilitates the study of dynamic physical properties., Oracle methods provide an upper bound for inferable properties., Video foundation models show potential in capturing dynamic attributes.
- **Cons**: MLLMs currently underperform compared to other models., Dependence on visual cues may limit generalization., Existing datasets lack ground-truth annotations for dynamic properties.
- **Future Work**: Further exploration of prompting strategies for MLLMs., Improving interpretability of learned representations., Expanding the dataset to include more diverse physical properties.

</details>

### [Astrophysical Consequences of an Electroweak $\etaw$ Pseudo-Scalar](http://arxiv.org/pdf/2510.02310v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>📄 Paper Summary </summary>

### 1. Task / Problem
- Investigate the astrophysical implications of the ηw boson and its potential as a dark matter candidate.

### 2. Motivation & Gaps
- The paper discusses the potential of the ηw boson to emerge from the Standard Model without new physics, and its implications for dark matter and cosmological observations.

- **Related work challenges:**
  - Ref. [1]: Theoretical arguments supporting the presence of ηw within the Standard Model.
  - Ref. [2]: Astrophysical constraints on the couplings of ultralight bosons.
  - Ref. [6]: The idea of ηw being a superposition of hydrogen and anti-hydrogen does not conform with expected mass scaling.
  - Dvali et al. (2025): The need for a robust theoretical framework to support the existence of ηw.
  - CAST experiment (2017): Constraints on axion-photon coupling that challenge the viability of ηw as a dark matter candidate.
  - Supernova 1987A observations: The strong astrophysical bounds on the decay constant of ηw.
  - N/A: N/A

### 3. Core Idea
- The ηw boson, if it exists, could be a viable dark matter candidate, but current astrophysical constraints suggest its properties are more complex than naive expectations.

### 4. Method
- **Pipeline**: Theoretical analysis of the coupling of ηw to standard model particles and its implications for dark matter.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

### 5. Experiments
- **Datasets & Metrics**: Astrophysical observations and theoretical estimates of coupling constants.
- **Baselines**: Astrophysical constraints, N/A, Previous dark matter candidates, Standard Model predictions
- **Main Results**: The strongest bounds on ηw's decay constant are derived from astrophysical observations, indicating fηw must be significantly larger than previously expected.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The non-perturbative nature of ηw dynamics complicates straightforward interpretations of its properties.

### 6. Takeaways
- **Pros**: Potentially significant implications for understanding electroweak interactions., Challenges existing astrophysical constraints on ultralight bosons., Encourages further theoretical and phenomenological investigations.
- **Cons**: Status of ηw as a firm prediction of the Standard Model is not well-established., Identifying ηw with atomic states does not fit expected features., Astrophysical constraints may limit the viability of ηw as a dark matter candidate.
- **Future Work**: Further theoretical investigations into the properties of ηw., Exploration of ηw's role in dark matter and dark energy., Assessment of new astrophysical bounds on ultralight bosons.

</details>
