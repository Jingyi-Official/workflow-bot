# Daily Paper Digest Â· 2025-10-06
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](http://arxiv.org/pdf/2510.03228v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Texture Recognition

### 2. Motivation & Gaps
- The paper addresses the need for effective texture representation in neural networks.

- **Related work challenges:**
  - Junior & Backes (2016): Proposed the first texture representation learning technique using a Randomized Neural Network (RNN) but did not explore architectural improvements.
  - Cimpoi et al. (2015): Learning-based methods offer greater descriptive power but are less interpretable and require substantial data.
  - Su et al. (2023): Hybrid techniques combine traditional descriptors with neural networks but still face limitations in representation learning.
  - Haralick et al. (1973): Proposed a feature extraction technique but lacked a formal definition of texture.
  - Ojala et al. (1996): Introduced Local Binary Pattern but faced limitations in robustness against various image conditions.
  - Cimpoi et al. (2015): Used Fisher Vector for feature extraction but ignored spatial information.
  - Randomized Vector Functional Link (RVFL): Output nodes are connected with both hidden and input nodes, complicating the architecture.
  - Extreme Learning Machine (ELM): Output nodes are only connected with hidden nodes, limiting flexibility in architecture.
  - Zhang et al., 2023: Addressing computational and training stability in neural networks.
  - Wang & Isola, 2020: Enhancing conditioning of matrices during inversion steps.
  - Junior & Backes, 2016: Previous RNN-based techniques struggled with texture recognition performance.
  - Ribas et al., 2020: Existing methods did not effectively utilize statistical measures for texture representation.
  - Previous texture recognition methods: Limited performance due to inadequate feature extraction techniques.
  - Junior & Backes, 2016: Previous studies did not explore the impact of varying embedding sizes on texture representation accuracy.
  - Ribas et al., 2020: Limited analysis on the performance of combining features from lower-dimensional embeddings.
  - Self-supervised material and texture representation learning for remote sensing tasks: Existing methods may not effectively capture complex texture representations.
  - Deep texture recognition via exploiting cross-layer statistical self-similarity: Challenges in achieving robust texture descriptors across various benchmarks.
  - Deep filter banks for texture recognition and segmentation: Limitations in current texture analysis methods that do not leverage multi-objective optimization.
  - Ojala et al. (2002b): Limited effectiveness of traditional texture classification methods.
  - Liu et al. (2019): Challenges in representing complex textures accurately.
  - Scabini et al. (2023): Need for more efficient encoding of texture features.
  - Junior & Backes (2016): Generating pseudorandom numbers for random weight matrix creation.
  - DIRECT and MIXED branches: Summarizing the weights of learned decoders to create robust texture representations.

### 3. Core Idea
- Utilizing statistical measures to compress and summarize texture representations from learned decoders.

### 4. Method
- **Pipeline**: The method involves generating random weight matrices and applying compression functions to summarize learned weights.
- **Architecture / Loss / Training**: The architecture employs a combination of loss functions tailored for texture classification, optimizing for both accuracy and computational efficiency.
- **Complexity / Resources**: The method requires computational resources for handling high-dimensional data and performing random projections.

### 5. Experiments
- **Datasets & Metrics**: The experiments are conducted on standard texture datasets, evaluating performance using metrics such as accuracy and F1 score.
- **Baselines**: CLBP, CNTD, DSRNet, DeepTEN, Existing texture representation learning techniques, Extreme Learning Machine (ELM), FV-CNN, LPQ, Linear Discriminant Analysis (LDA), MCND, N/A, Opponent-Gabor, Previous embedding techniques, Previous texture recognition methods, Randomized Vector Functional Link (RVFL), Recent deep learning approaches, SSN, SSR, SST, Standard texture recognition methods, Traditional texture classification methods, VCTex
- **Main Results**: The MIXER model outperforms existing methods in terms of accuracy and processing speed.
- **Ablations**: Ablation studies demonstrate the importance of the Local Pattern Extractor in improving recognition performance.
- **Limitations / Stress Tests**: The model's performance is limited in scenarios with highly similar textures.

### 6. Takeaways
- **Pros**: Introduces a novel hyperspherical multi-head random projector., Captures both intra- and inter-channel relationships effectively., Achieves state-of-the-art performance on texture recognition benchmarks.
- **Cons**: May struggle with highly complex textures., Dependence on the quality of training data., Limited interpretability of the learned features.
- **Future Work**: Explore further architectural improvements., Investigate the application of MIXER in other domains., Enhance the optimization problem for better performance.

</details>

### [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](http://arxiv.org/pdf/2510.03215v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Enhancing communication between large language models (LLMs) through cache sharing

### 2. Motivation & Gaps
- The paper introduces Cache-to-Cache (C2C), a paradigm that allows LLMs to communicate beyond text by transforming and fusing key-value (KV) caches across models.

- **Related work challenges:**
  - Intra-model cache sharing methods: Reuse shallow layersâ€™ KV-Cache for deeper layers to accelerate single LLM inference.
  - Text-to-Text communication: Restricts information exchange among LLMs and incurs noticeable latency.
  - Agent protocols for standardizing text messages: Rigid templates remain insufficient for flexible, open-domain collaboration.
  - Yang et al., 2024b: Limited to single model cache sharing.
  - Wu & Tu, 2024: Does not support sharing across different model families.
  - DroidSeek Liu et al., 2024a: Extends cache reuse but lacks semantic communication.
  - Previous LLM collaboration methods: Limited contextual understanding and knowledge transfer between models.
  - Text-to-Text communication: Inefficiency in collaborative pipelines due to lack of direct cache fusion.
  - Query-level routing: Inability to leverage complementary strengths of different models effectively.
  - N/A: N/A
  - Previous text-to-text communication methods: Limited efficiency and scalability in multi-LLM systems.
  - OpenAI. Introducing gpt-5: N/A
  - Ruoyu Qin et al. Mooncake: A kvcache-centric disaggregated architecture for llm serving: N/A
  - Nikunj Saunshi et al. Reasoning with latent thoughts: On the power of looped transformers: N/A
  - Zejiang Shen et al. Learning to decode collaboratively with multiple language models: N/A
  - Yutao Sun et al. You only cache once: Decoder-decoder architectures for language models: N/A
  - Rao Surapaneni et al. Announcing the agent2agent protocol (a2a): N/A
  - Gemma Team et al. Gemma 3 technical report: N/A
  - Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants: N/A
  - Khanh-Tung Tran et al. Multi-agent collaboration mechanisms: A survey of llms: N/A
  - Junlin Wang et al. Mixture-of-agents enhances large language model capabilities: N/A
  - Haoyi Wu and Kewei Tu. Layer-condensed kv cache for efficient inference of large language models: N/A
  - Qingyun Wu et al. Autogen: Enabling next-gen llm applications via multi-agent conversation framework: N/A
  - You Wu et al. A systematic study of cross-layer kv sharing for efficient llm inference: N/A
  - An Yang et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement: N/A
  - An Yang et al. Qwen3 technical report: N/A
  - Jingbo Yang et al. Kvlink: Accelerating large language models via efficient kv cache reuse: N/A
  - Yifei Yang et al. Kvsharer: Efficient inference via layer-wise dissimilar kv cache sharing: N/A
  - Jiayi Yao et al. Cacheblend: Fast large language model serving with cached knowledge fusion: N/A
  - Lu Ye et al. Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition: N/A
  - Boyi Zeng et al. Pretraining language models to ponder in continuous space: N/A
  - Kaiyan Zhang et al. Fast and slow generating: An empirical study on large and small language models collaborative decoding: N/A
  - Mingjin Zhang et al. Edgeshard: Efficient llm inference via collaborative edge computing: N/A
  - Yusen Zhang et al. Chain of agents: Large language models collaborating on long-context tasks: N/A
  - Wenhao Zheng et al. Citer: Collaborative inference for efficient large language model decoding with token-level routing: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The paper introduces the C2C (Communication-to-Communication) paradigm, which enhances model performance by allowing a stronger model (Sharer) to assist a weaker model (Receiver) through effective communication.

### 4. Method
- **Pipeline**: The method involves a Fuser architecture that integrates the outputs of the Sharer and Receiver models, utilizing a cache enrichment approach to improve performance.
- **Architecture / Loss / Training**: The architecture includes a 3-layer MLP for projecting the Sharer cache into the Receiver's dimensionality, followed by feature fusion and dynamic weighting.
- **Complexity / Resources**: The complexity arises from the additional projection stage and the need for careful alignment of message sections.

### 5. Experiments
- **Datasets & Metrics**: MMLU-Redux, OpenBookQA, ARC-Challenge, C-Eval, LongBench-E
- **Baselines**: C2C, Direct, Few-shot, Identical models, Individual model performance, Individual models, N/A, Oracle, Query-level routing, Qwen3-0.6B, Qwen3-4B, Receiver-only models, T2T, Text communication paradigm, Text-to-Text communication, Text-to-text communication methods, gpt-4-1106-preview, mixtral-8x7b-instruct-v0.1, text-to-text communication
- **Main Results**: C2C-C outperforms the default C2C model, achieving significant improvements in accuracy and PGR across various benchmarks.
- **Ablations**: The effect of single-layer cache enrichment was analyzed, showing performance benefits in specific layers.
- **Limitations / Stress Tests**: The study acknowledges limitations in the systematic investigation of more elaborate architectures, which is left for future work.

### 6. Takeaways
- **Pros**: Utilizes deep, specialized semantics from both models., Avoids explicit intermediate text generation., Delivers significant improvements in accuracy and latency.
- **Cons**: Potential performance degradation in certain layers., Complexity in implementing dynamic weighting and gating mechanisms., Dependence on the quality of the source model's KV-Cache.
- **Future Work**: Explore further enhancements in KV-Cache communication., Investigate applications across diverse domains., Develop more flexible communication protocols.

</details>

### [ProxSTORM -- A Stochastic Trust-Region Algorithm for Nonsmooth Optimization](http://arxiv.org/pdf/2510.03187v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Stochastic optimization

### 2. Motivation & Gaps
- The paper addresses the limitations of deterministic algorithms in solving stochastic optimization problems, highlighting the need for methods that can effectively handle nonsmooth objective functions.

- **Related work challenges:**
  - STORM: Generalizes the stochastic trust-region algorithm for unconstrained optimization of smooth functions.
  - Proximal Trust-Region Algorithm: Imposes exactness conditions on the proximal mapping and allows for deterministic inexactness.
  - Sequential Quadratic Programming: Does not involve nonsmooth functions and is based on constrained optimization techniques.
  - STORM: In STORM, the step acceptance criteria rely on the actual reduction, which is not computable in ProxSTORM.
  - Deterministic frameworks: The deterministic frameworks do not account for the stochastic nature of the problem.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A
  - STORM: Limited convergence guarantees and complexity bounds.
  - Proximal algorithms: Difficulty in handling non-smooth regularization terms.
  - Adam optimization method: Requires a gradient of the â„“1 norm, which complicates its application in certain stochastic settings.
  - Deterministic proximal trust-region method: Does not adapt to the stochastic nature of the problem, leading to less efficient solutions.
  - Deterministic trust-region methods: These methods do not typically require trust-region radii to decrease to zero, which can hinder performance in stochastic settings.
  - Sample average approximation: Sample average approximations may never fully solve the stochastic problem, unlike ProxSTORM.
  - Chen, R., Menickelly, M., Scheinberg, K.: Stochastic optimization using a trust-region method and random models.: N/A
  - Blanchet, J., Cartis, C., Menickelly, M., Scheinberg, K.: Convergence rate analysis of a stochastic trust-region method via supermartingales.: N/A
  - Baraldi, R.J., Kouri, D.P.: A proximal trust-region method for nonsmooth optimization with inexact function and gradient evaluations.: N/A

### 3. Core Idea
- ProxSTORM is a new algorithm designed for objective functions that are bounded from below and include a proper, closed, and convex term that is generally nonsmooth, providing global convergence guarantees and complexity bounds.

### 4. Method
- **Pipeline**: The ProxSTORM algorithm iteratively updates the solution using a stochastic trust-region approach, dynamically adjusting the number of samples based on convergence criteria.
- **Architecture / Loss / Training**: The algorithm incorporates a proximal gradient step that adapts to the stochastic nature of the problem, ensuring robust convergence.
- **Complexity / Resources**: ProxSTORM achieves an epsilon-squared complexity bound, making it efficient for large-scale problems.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilized various stochastic optimization problems to evaluate the performance of ProxSTORM against deterministic algorithms.
- **Baselines**: Adam optimization method, Deterministic frameworks, Deterministic proximal trust-region method, Deterministic trust-region methods, N/A, Proximal Trust-Region Algorithm, SGD, STORM, Sample average approximation
- **Main Results**: ProxSTORM demonstrated superior performance in terms of convergence speed and solution quality compared to its deterministic counterparts.
- **Ablations**: Ablation studies indicated that the dynamic sampling strategy significantly enhances the algorithm's performance.
- **Limitations / Stress Tests**: The paper acknowledges that the stochastic nature of the algorithm may lead to challenges in practical implementations, particularly in ensuring convergence.

### 6. Takeaways
- **Pros**: Accommodates stochastic inexactness in optimization problems., Simplifies assumptions compared to general stochastic constrained optimization., Establishes expected complexity bounds.
- **Cons**: Only addresses convex constraints., May not generalize to non-convex constraints., Stochastic inexactness may still pose challenges in practical applications.
- **Future Work**: Explore extensions to non-convex constraints., Investigate applications in other optimization domains., Enhance the algorithm's robustness against stochastic inexactness.

</details>

## Gaussian Splatting

### [Inferring Stellar Densities with Flexible Models I: The Distribution of RR Lyrae in the Milky Way with $\textit{Gaia}$ DR3](http://arxiv.org/pdf/2510.03221v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Characterizing the inner Galaxy's stellar density distribution

### 2. Motivation & Gaps
- The inner Galaxy is challenging to characterize due to crowding and dust extinction, yet it holds crucial insights into galaxy evolution and dark matter.

- **Related work challenges:**
  - Previous studies on the Galactic disk and halo density distributions of RR Lyrae stars.: Lack of data within 5 kpc of the Galactic center and the complexity of overlapping components in the inner Galaxy.
  - Cappellari 2002; Hogg & Lang 2013; Miller & van Dokkum 2021: Previous works have used Gaussian Mixtures for external galaxies but not for the Milky Way.
  - Deason et al. 2011: Traditional power-law models may not accurately capture the complexity of the Galactic halo.
  - Iorio et al. 2018: Existing models often fail to account for the overlapping structures of the disk, halo, and bulge.
  - Han et al. 2022: Power-law models lack the flexibility needed for precise modeling of the inner Galaxy.
  - Pietrukowicz et al. (2015): Uses RRL from the OGLE survey and converts surface density to 3D density profile.
  - Yang et al. (2022): Estimates halo profile using direct orbit integration of K giant stars and removes disk stars.
  - Han et al. (2022): Uses a Bayesian model to infer halo profile but relies on chemical and dynamical selection.
  - Horta & Schiavon (2025): Utilizes a selection in various abundance ratios to isolate the 'Heracles/proto-galaxy' structure.
  - El-Badry et al. 2018: Understanding the role of dark matter in galaxy formation.
  - Rix et al. 2022: Characterizing the structure of the inner Galaxy.
  - Lucey et al. 2025: Disentangling the structure of the oldest components of the Milky Way.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- Utilizing a hierarchical Bayesian Gaussian Mixture Model (GMM) to map the structure of the Milky Way using RR Lyrae stars.

### 4. Method
- **Pipeline**: A GMM with 10 Gaussian components centered on the Galactic center to model the distribution of RR Lyrae stars.
- **Architecture / Loss / Training**: The model employs a hierarchical Bayesian approach with an adafactor optimizer for efficient training and uncertainty estimation.
- **Complexity / Resources**: The model complexity is increased by using distinct covariances for the inner Galaxy and halo components.

### 5. Experiments
- **Datasets & Metrics**: The study uses RR Lyrae stars to infer the stellar density distribution in the Milky Way.
- **Baselines**: Han et al. (2022), N/A, Pietrukowicz et al. (2015), Power laws typically used to describe halos, Power-law models, Previous Gaussian models, Previous models of stellar density distributions in external galaxies., Previous power law models for stellar density distribution, Yang et al. (2022)
- **Main Results**: The inner Galaxy is dominated by a prolate population, and the halo distribution follows a r^-4 power-law that flattens within 12 kpc of the Galactic center.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The main limitation is the sample size, particularly in high extinction regions affecting the selection function resolution.

### 6. Takeaways
- **Pros**: GMMs provide a flexible framework for modeling complex stellar distributions., The study offers new constraints on the distribution of old stars in the inner Galaxy., RR Lyrae stars serve as effective probes for the Galaxy's earliest formation epochs.
- **Cons**: The method may not fully account for all overlapping components in the inner Galaxy., Limited data within 5 kpc of the Galactic center may affect results., Complexity in the relationship between RR Lyrae and the thin disk remains unresolved.
- **Future Work**: Further studies could refine the modeling of the inner Galaxy's stellar components., Exploration of the origins of thin disk RR Lyrae stars., Incorporation of additional data to improve density distribution models.

</details>

### [HESS J1831$-$098 -- Exploring a pulsar halo scenario with H.E.S.S. data](http://arxiv.org/pdf/2510.03183v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Exploration of pulsar halo scenario using H.E.S.S. data

### 2. Motivation & Gaps
- The study aims to analyze the energy density and characteristics of the pulsar halo associated with HESS J1831âˆ’098.

- **Related work challenges:**
  - HAWC observatory findings on pulsar halos: Understanding the emission mechanisms and physical properties of pulsar halos.
  - Previous models of pulsar halos: Incorporating new data and refining models to better fit observed emissions.
  - HAWC analysis of Geminga and Monogem: Comparison of parameters and understanding the differences in injection indices.
  - Discovery of very-high-energy Î³-ray emission from the vicinity of PSR J1831-952 with H.E.S.S.: Understanding the implications of high-energy emissions in pulsar environments.
  - The parkes multibeam pulsar survey: Identifying and characterizing new pulsars and their properties.
  - Validation of open-source science tools and background model construction in gamma-ray astronomy: Ensuring the accuracy and reliability of models used in gamma-ray astronomy.

### 3. Core Idea
- To compute the average energy density in the pulsar halo and compare it with existing models and data.

### 4. Method
- **Pipeline**: Data collection from H.E.S.S. and analysis of photon energy spectra.
- **Architecture / Loss / Training**: Statistical errors are estimated from likelihood profiles of each parameter.
- **Complexity / Resources**: Utilizes advanced gamma-ray detection techniques and statistical modeling.

### 5. Experiments
- **Datasets & Metrics**: Utilizes H.E.S.S. data and compares it with 1LHAASO and 3HWC catalogs.
- **Baselines**: 1LHAASO components, 3HWC J1831-095, Field of View (FoV) background method, GaussianÃ—PL model, H.E.S.S. significance map, Pulsar halo model, Spectro-morphological likelihood analysis, Templates of gas tracers
- **Main Results**: The average energy density in the pulsar halo is found to be 0.24 eV cmâˆ’3, which is higher than the magnetic fieldâ€™s energy density.
- **Ablations**: Likelihood scans were conducted to assess the confidence intervals of the model parameters.
- **Limitations / Stress Tests**: Assumes constant total injection efficiency for energy density calculations.

### 6. Takeaways
- **Pros**: Provides insights into the nature of pulsar halos., Utilizes advanced modeling techniques to analyze VHE emissions., Contributes to the understanding of pulsar wind nebulae.
- **Cons**: Limited by the quality and quantity of observational data., Challenges in distinguishing between different emission models., Potential biases in background modeling.
- **Future Work**: Further exploration of additional pulsar halo candidates., Refinement of models with new observational data., Investigation into the implications of pulsar halos on cosmic ray propagation.

</details>

### [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](http://arxiv.org/pdf/2510.03155v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Classification of neuron types using generalized leaky integrate-and-fire models

### 2. Motivation & Gaps
- The study aims to classify multiple neuron types using advanced models that approximate neuronal activity.

- **Related work challenges:**
  - Classical leaky integrate-and-fire (LIF) models: Fail to account for variability and strong stimulus-dependence of AP timing.
  - Stimulus-Accelerated Leaky Integrate-and-Fire (SA-LIF) model: Does not incorporate probabilistic nature of neuronal firing.
  - Quantum-inspired models: Need for models that leverage principles such as probabilistic timing and superposition.
  - Gerstner et al.: Employ synthesized stimulus currents to optimize neuron models.
  - Kingma and Welling: Demonstrate learning efficacy using synthetic datasets.
  - Huang et al.: Reveal when quantum ML models surpass classical counterparts.
  - SA-LIF model: Significantly overestimates AP onset at low spike counts and underestimates it as spike count rises, resulting in large relative errors.
  - Quantum-inspired models: Need to demonstrate superiority in capturing the nonlinear, saturating nature of AP timing.
  - Generalized integrate-and-fire models of neuronal activity: High accuracy in approximating spike trains of detailed models.
  - Deep learning with spiking neurons: Identifying opportunities and challenges in applying deep learning techniques to spiking neuron models.
  - Parameter extraction and classification of three cortical neuron types: Revealing distinct adaptation mechanisms among different neuron types.

### 3. Core Idea
- Utilizing generalized leaky integrate-and-fire models to effectively classify various types of neurons based on their spiking behaviors.

### 4. Method
- **Pipeline**: Data collection, model training, and evaluation of neuron classification performance.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for spiking neuron dynamics during training.
- **Complexity / Resources**: The model complexity is balanced with computational resources to ensure efficient training and inference.

### 5. Experiments
- **Datasets & Metrics**: Utilized datasets include various neuron types with metrics focused on classification accuracy and model performance.
- **Baselines**: Classical LIF model, Other machine learning classification algorithms, SA-LIF model, Traditional leaky integrate-and-fire models
- **Main Results**: The proposed model demonstrates superior classification accuracy compared to baseline methods.
- **Ablations**: Ablation studies indicate the importance of specific model components in achieving high performance.
- **Limitations / Stress Tests**: Tests reveal limitations in classifying certain neuron types under specific conditions.

### 6. Takeaways
- **Pros**: QI-LIF model captures biological variability in neuronal firing., Significantly reduces prediction error compared to classical models., Aligns closely with observed biological responses.
- **Cons**: Complexity in model derivation and implementation., Potential challenges in translating quantum-inspired models to practical applications., Limited experimental validation of quantum-inspired approaches.
- **Future Work**: Further exploration of quantum-inspired models in computational neuroscience., Integration of QI-LIF with quantum spiking neural networks., Investigation of applications in temporal pattern recognition.

</details>

## avatar

### [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](http://arxiv.org/pdf/2510.01619v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Modeling thin, codimensional structures like garments

### 2. Motivation & Gaps
- The method achieves state-of-the-art performance in appearance and physical dynamics modeling but has limitations in scalability, relighting, and occlusion-aware generalization.

- **Related work challenges:**
  - PhysAvatar: Fails when animation inputs have a small degree of self-penetration, causing simulation failures.
  - Existing methods using piecewise linear transformations: Limited in accurately capturing complex deformations and tend to overfit to motions observed during training.
  - Xiang et al.: Relies on a time-consuming manual parameter search to approximate reasonable cloth behavior.
  - C-IPC: Fails to resolve collisions for noisy colliders during Continuous Collision Detection (CCD).
  - Learning-based simulation methods: Limited generalizability beyond training dynamics and cannot guarantee physically plausible deformations.
  - DiffAvatar: Omitted appearance modeling and tailored for scan-based asset preparation rather than dynamic avatar reconstruction.
  - Material Point Method (MPM): MPM is primarily used for general object dynamics and lacks effective garment-body collision handling.
  - Anisotropic Constitutive Model: Existing models do not adequately capture the manifold-dependent dynamics of garments.
  - PhysAvatar [78]: Existing approaches assume ideal conditions, limiting their applicability in real-world scenarios.
  - PhysAvatar [78]: Fails when driving body mesh colliders have self-penetrations.
  - C-IPC [31]: Takes a long time to converge for resolving complex collisions.
  - Learning-based simulators [8, 7]: Less effective in modeling unseen dynamics.
  - N/A: N/A
  - PhysAvatar: Achieving accurate garment dynamics and high rendering quality.
  - PhysAvatar: Struggles with accurate garment dynamics and rendering quality.
  - Gaussian Garments: Replaces explicit simulation with a learned dynamics module, leading to inaccuracies.
  - MMLPHuman: Exhibits unnatural surface artifacts or discontinuities under challenging poses.
  - Gaussian Garments [55]: Struggles to capture physical laws under settings where physical parameters must be estimated from only one second of motion, leading to high geometric error.
  - MMLPHuman [74]: Lacks explicit surface modeling and physical understanding, producing unrealistic surface artifacts or broken geometry when encountering unseen poses.
  - Finite-Difference Optimization: Scalability with the number of parameters.
  - Relighting-aware extensions for Gaussian avatars: Current framework does not support relightable rendering.
  - Generative priors for inpainting unobserved regions: Rendering quality may degrade for previously occluded or unseen parts.

### 3. Core Idea
- An anisotropic constitutive model that captures the behavior of cloth under various deformations, enabling realistic garment simulation.

### 4. Method
- **Pipeline**: The pipeline computes the deformation gradient F at each particle using local material directions derived from a Lagrangian mesh.
- **Architecture / Loss / Training**: The model applies QR decomposition to reparameterize the energy as a function of the upper-triangular matrix R, penalizing different types of deformation.
- **Complexity / Resources**: The finite-difference optimization scales linearly with the number of parameters, which may increase computational cost for fine-grained parameterizations.

### 5. Experiments
- **Datasets & Metrics**: ActorsHQ dataset, measuring average cloth-body penetration depth and Key Physical Phenomena Detection score.
- **Baselines**: ARAH [65], C-IPC, Existing MPM methods, GS-Avatar [12], Gaussian Garments, Gaussian Garments [55], MMLPHuman, MMLPHuman [74], N/A, PhysAvatar, PhysAvatar [78], TA V A [32], Traditional animation techniques, XPBD
- **Main Results**: The model demonstrates accurate and physically plausible dynamics in garment behavior.
- **Ablations**: Ablation studies validate the importance of key components such as the constitutive model for anisotropic elastoplasticity and physical parameter learning.
- **Limitations / Stress Tests**: The method acknowledges limitations in scalability, relighting, and occlusion-aware generalization.

### 6. Takeaways
- **Pros**: Supports physically realistic and robust animations for loose garments., Achieves high-fidelity rendering from free viewpoints., Demonstrates zero-shot generalizability to novel scene interactions.
- **Cons**: Requires complex simulation setup., High computational resource demands., Limited to scenarios where multi-view video input is available.
- **Future Work**: Explore further generalizability to diverse interactions., Enhance the efficiency of the simulation process., Investigate applications in virtual and augmented reality.

</details>

### [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](http://arxiv.org/pdf/2510.01182v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Empirical analysis of bug reports in multi-user XR systems

### 2. Motivation & Gaps
- The study reveals unique challenges in multi-user XR systems, particularly regarding synchronization and interaction issues.

- **Related work challenges:**
  - Research on software bugs in various domains: Significant gap in understanding bugs specifically arising from multi-user interactions in XR environments.
  - Existing studies on distributed systems: Do not account for the social presence and interaction complexities in multi-user XR environments.
  - Previous studies on XR systems: Lack of comprehensive analysis on multi-user bug symptoms and their implications.
  - Developer Forums and User Communities: Insufficient quality of user reviews for symptom analysis
  - Previous studies on software bugs: Traditional software bugs are often tolerable, whereas XR bugs can break immersion, raising the stakes for quality assurance.
  - Hubs-Foundation/hubs issue #5586: Orphaned state of objects when both creator and owner leave the session.
  - Hubs-Foundation/hubs issue #1000: Implicit ownership leading to invisibility of objects for new users.
  - Hubs-Foundation/hubs issue #6250: Inadequate access control allowing unauthorized manipulation of objects.
  - Configuration Sensitivity: Many bugs were resolved through configuration adjustments rather than code changes, indicating high sensitivity to deployment configurations.
  - Layered Complexity: Bugs often have multiple contributing root causes, requiring a comprehensive understanding of the entire system stack.
  - Implicit Assumptions: Assumptions valid in single-user contexts often lead to bugs in multi-user scenarios.
  - Empirical study of multi-user XR bugs: Understanding unique challenges in maintaining consistent shared state across heterogeneous devices and network conditions.
  - Analysis of bug patterns in XR development: Need for domain-specific software analysis techniques that capture the unique characteristics of multi-user XR bugs.
  - Development of specialized debugging tools: Current tools are ill-equipped to handle the distributed, real-time nature of multi-user XR applications.
  - Xanthidou et al. [77]: Identified spatial design and collaborative interactions as major challenges in VR collaboration.
  - Ruth et al. [71]: Explored challenges in designing secure and private content sharing for multi-user AR software.
  - Zhang et al. [79]: Studied the dilemma between immersive user experience and higher privacy risks in the Metaverse.
  - Extended Reality: Its Challenges, Usage and Future Ahead: N/A
  - Software Testing for AR/VR: Ensuring Bug-Free Experiences: N/A
  - Evaluation of XR Applications: A Tertiary Review: N/A
  - N/A: N/A

### 3. Core Idea
- Develop comprehensive taxonomies categorizing symptoms, root causes, and consequences of XR defects based on empirical analysis of bug reports.

### 4. Method
- **Pipeline**: Analysis of 2,649 real-world bug reports to identify and categorize defects.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The study involved analyzing 2,649 bug reports from various sources including GitHub, developer forums, and user communities.

### 5. Experiments
- **Datasets & Metrics**: 2,649 real-world bug reports analyzed for symptoms and root causes.
- **Baselines**: Existing debugging tools, Hubs-Foundation/hubs, Multi-user XR systems, N/A, Single-user XR applications, Single-user XR systems, Traditional distributed systems, Traditional distributed systems bug studies, Traditional multi-user applications, Traditional online collaboration tools, Traditional software bug analysis methods, Traditional software quality metrics, Unity SDK, Unreal Engine
- **Main Results**: Over 34% of defects cause severe disruptions like crashes and interaction breakdowns.
- **Ablations**: N/A
- **Limitations / Stress Tests**: Findings may not fully reflect emerging technologies or practices due to rapid evolution of XR landscape.

### 6. Takeaways
- **Pros**: Provides actionable recommendations for developers and platform vendors., Develops a comprehensive taxonomy for multi-user XR bugs., Highlights unique challenges in multi-user XR systems.
- **Cons**: Limited understanding of multi-user XR bugs compared to other domains., Fragmented nature of bug reporting complicates knowledge aggregation., Potential privacy and health implications remain concerning.
- **Future Work**: Further research on automated quality assurance tools for multi-user XR., Exploration of specific privacy and health implications associated with multi-user XR bugs., Development of targeted guidance for bug prevention and mitigation.

</details>

### [Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/pdf/2510.01176v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Animation Generation

### 2. Motivation & Gaps
- Existing methods for facial animation often produce spatiotemporally inconsistent results, leading to unnatural avatars.

- **Related work challenges:**
  - Fan et al. 2022: Lack of sufficient detail for conveying subtle facial cues essential in social interactions.
  - Richard et al. 2021: Computational challenges that limit real-time performance.
  - Xing et al. 2023: Primarily operate in an offline manner requiring entire audio sequences as input.
  - GSTalker: Limited generalization to multiple identities due to training on personalized data.
  - GaussianTalker: Real-time rendering achieved, but audio-based deformations computed offline.
  - EmoTalk3D: Computationally expensive and lacks real-time performance.
  - Universal Relightable Prior Model [Li et al. 2024a]: Existing models may not effectively handle real-time performance and the integration of gaze direction.
  - Diffusion models for facial expression generation: Inherent slowness of diffusion models during inference.
  - Graph-based gaze synthesis: Ensuring smooth and consistent gaze transitions.
  - Audio encoders for real-time applications: Maintaining causality in audio processing without future information.
  - GaussianTalker: Generates high-fidelity 3D face animation but is not designed for real-time applications.
  - TalkingGaussian: Focuses on person-specific 3D deformation and rendering, lacking universality.
  - GaussianTalker [Cho et al. 2024]: Generates high-fidelity 3D face animation but learns person-specific deformations.
  - TalkingGaussian [Li et al. 2024b]: Similar to GaussianTalker, it focuses on person-specific rendering.
  - DiffPoseTalk [Sun et al. 2024]: Employs style conditioning which is not feasible in real-world applications.
  - wav2vec 1.0 [Schneider et al. 2019]: Non-causal encoding limitations in real-time applications.
  - wav2vec 2.0 [Baevski et al. 2020]: Maintaining accuracy while achieving temporal consistency in facial animations.
  - N/A: N/A
  - FaceFormer: High latency unsuitable for real-time applications.
  - CodeTalker: High computational overhead and autoregressive nature.
  - AniPortrait: Produces blurry and distorted artifacts in generated images.
  - Wav2vec 1.0: Preserves causality but limits representation quality.
  - Wav2vec 2.0: Utilizes non-causal CNNs, which may disrupt temporal coherence.
  - HuBERT: Relies on non-causal convolutions, affecting causality.
  - N/A: N/A

### 3. Core Idea
- The proposed method generates high-fidelity avatars with synchronized lip movements by utilizing a gaze graph and a transformer architecture.

### 4. Method
- **Pipeline**: The method involves capturing audio and gaze data, processing it through a transformer architecture, and synthesizing facial animations.
- **Architecture / Loss / Training**: Utilizes a self-attention mechanism with a windowed mask to maintain temporal coherence.
- **Complexity / Resources**: distributed data-parallel (DDP) using two A100 GPUs; Distillation training and emotion-conditioning training are done with a single A100 GPU

### 5. Experiments
- **Datasets & Metrics**: Out of 265 capture subjects, we use 237 for training and 28 for testing. Data are segmented into sequences of frame length 100 (in 30FPS).
- **Baselines**: AniPortrait, Audio2Photoreal [Ng et al. 2024], Audio2Photoreal-Face, CodeTalker, DiffPoseTalk, DiffPoseTalk [Sun et al. 2024], EmoTalk3D, Existing diffusion models, FaceFormer, Fan et al. 2022, GSTalker, GaussianTalker, HuBERT, N/A, Richard et al. 2021, TalkShow [Yi et al. 2023], TalkShow-Face, Traditional regression methods for facial expression generation, Wav2vec 1.0, Wav2vec 2.0, Xing et al. 2023, wav2vec 1.0, wav2vec 2.0
- **Main Results**: Quantitative comparison experiments on freeform speech and sentence reading data.
- **Ablations**: Ablation studies on the impact of gaze graph construction and transformer architecture on performance.
- **Limitations / Stress Tests**: Out of 28 test subjects, two were excluded for excessive frame drops and less than 70 segments can be used for freeform speech; two were excluded for excessive frame drops and less than 30 segments can be used for sentence reading.

### 6. Takeaways
- **Pros**: High fidelity and universal 3D facial avatars., Real-time performance suitable for social interactions., Versatile framework for multimodal applications.
- **Cons**: Challenges in maintaining consistent animation quality in live scenarios., Dependency on audio quality for accurate expression generation., Potential computational limitations in more complex scenarios.
- **Future Work**: Exploration of additional multimodal inputs for enhanced realism., Improvements in latency and computational efficiency., Expansion of applications beyond social telepresence.

</details>

## video understanding

### [Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation](http://arxiv.org/pdf/2510.03216v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Medical Image Segmentation

### 2. Motivation & Gaps
- The proposed Wave-GMS framework aims to provide a lightweight and efficient solution for medical image segmentation, addressing the limitations of existing models that require heavy pretrained components.

- **Related work challenges:**
  - Deep Segmentation Networks (DSN): High computational complexity and large memory requirements.
  - CNN-based architectures: Limited receptive field and poor generalizability.
  - Transformer-based architectures: High model complexity and prone to overfitting on small datasets.
  - Tiny-V AE: Maintaining performance with a highly compressed model.
  - LMM: Learning effective mappings from multi-resolution representations.
  - Deep Supervision: Enhancing feature learning without increasing model complexity.
  - GMS: Relies on a heavyweight pretrained SD-V AE, increasing memory consumption and reducing feasible batch size.
  - SDSeg: Has a large number of parameters (âˆ¼329M), making it less efficient for resource-constrained environments.
  - MedSegDiff-V2: Also has a high parameter count (âˆ¼129.4M), which can lead to overfitting on small datasets.
  - N/A: N/A

### 3. Core Idea
- Wave-GMS utilizes a lightweight, trainable multi-resolution encoder and a pretrained Tiny-V AE to achieve high segmentation performance with significantly fewer parameters.

### 4. Method
- **Pipeline**: Wave-GMS integrates a lightweight multi-resolution encoder with a pretrained Tiny-V AE for generating segmentation masks.
- **Architecture / Loss / Training**: The model employs a Latent Mapping Model (LMM) to map multi-scale image representations to segmentation masks, with latent-space alignment for improved compatibility.
- **Complexity / Resources**: Wave-GMS has approximately 2.6M trainable parameters, allowing efficient training on GPUs with limited memory.

### 5. Experiments
- **Datasets & Metrics**: The model was evaluated on four benchmark datasets, achieving the highest DSC and IoU, and the lowest HD95 scores.
- **Baselines**: ACC-UNet, GMS, GSS, MA-TransformerV2, MLRU++, MedSegDiff-V2, MultiResUNet, N/A, SD-Seg, SegMamba-V2, Slim UNETR, Swin-UMamba, Swin-UNet, SwinUNet, U-Mamba, UCTransNet, UNETR++, UNet, nnUNet
- **Main Results**: Wave-GMS outperformed all competing methods in both transfer-directions, achieving the highest Dice score of 82.1% and lowest HD95 of 15.35 in the BUSI-to-BUS domain-transfer study.
- **Ablations**: Ablation studies showed that Wave-GMS's combination of multi-resolution encoding and latent-space alignment enhances segmentation accuracy.
- **Limitations / Stress Tests**: The current framework is limited to 2D medical image analysis and relies on a distilled version of a pretrained model.

### 6. Takeaways
- **Pros**: Lightweight model with fewer trainable parameters., High performance on medical image segmentation tasks., Ability to train on cost-effective GPUs with large batch sizes.
- **Cons**: Performance on Kvasir-Instrument was on par with GMS., Limited exploration of ablation studies.
- **Future Work**: Explore further optimizations for model efficiency., Investigate applications in other medical imaging tasks., Enhance generalizability on more diverse datasets.

</details>

### [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](http://arxiv.org/pdf/2510.03207v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Reinforcement Learning

### 2. Motivation & Gaps
- The paper discusses the challenges and advancements in reinforcement learning with rich observations, particularly in the context of Block MDPs and the role of privileged information in training policies for POMDPs.

- **Related work challenges:**
  - Mu et al., 2025: Expert distillation converges faster and more stably than RL without latent state information.
  - Arora et al., 2018: Expert distillation fails to encourage purely information-gathering actions.
  - Cai et al., 2024: The prior theoretical model for understanding the benefits of latent state information was too restrictive for image-based locomotion tasks.
  - Efroni et al., 2022: Perfect decodability is an unrealistic assumption in motivating tasks.
  - Cai et al., 2024: Privileged information yields a computational benefit in decodable POMDPs.
  - Kara and Yuksel, 2022: Belief contraction governs the success of standard RL.
  - Efroni et al., 2022: Learning a near-optimal policy in standard RL access model requires a large number of samples.
  - Cai et al., 2024: Demonstrates that with latent state information, the sample and time complexity is significantly reduced.
  - Weihs et al., 2021: Models requiring active information-gathering complicate the assumptions of low decodability error.
  - Zhang et al., 2022: Focus on representation learning and computational challenges in standard Block MDPs.
  - Mhammedi et al., 2023b: Addressing issues of partial observability in reinforcement learning.
  - Rohatgi and Foster, 2025: Exploring the implications of conditioning on past actions in learning policies.
  - Ross and Bagnell, 2010: Error compounding in imitation learning.
  - Rajaraman et al., 2020: Sample complexity and misspecification in behavior cloning.
  - Foster et al., 2024: Horizon dependence in behavior cloning.
  - Warrington et al., 2021: Asymmetric RL methods that iteratively refine the expert.
  - Laskey et al., 2017: Noise injection in imitation learning to robustify behavior cloning.
  - Block et al., 2023b: Mitigating out-of-distribution effects in challenging image-based tasks.
  - N/A: N/A
  - Kinematic state abstraction and provably efficient rich-observation reinforcement learning: N/A
  - Playing atari with deep reinforcement learning: N/A
  - Human-level control through deep reinforcement learning: N/A
  - Efroni et al. (2022): Introduced the L-step decodability assumption, which is crucial for deriving efficient learning algorithms.
  - Jin et al. (2020): Derived a statistically efficient algorithm for POMDPs under weak observability conditions.
  - Kwon et al. (2021): Focused on latent MDPs, highlighting the need for efficient algorithms in the presence of latent state information.
  - Krishnamurthy et al., 2016: Large observation space that is too large to enumerate.
  - Cai et al., 2024: Learning in Î¦-decodable Block MDPs is harder without latent state information.
  - Pan et al., 2017: Expert distillation may fail in certain environments without privileged information.
  - Golowich et al., 2022: Learning Î³-observable POMDPs in quasi-polynomial time requires bounding a generalized version of the belief contraction error.
  - Golowich et al., 2023: Establishing the relationship between Î³-observability and (Îµ; Ï•, L)-belief contraction.
  - N/A: N/A
  - N/A: N/A
  - Golowich et al., 2023: The belief contraction error can be bounded, but the bound does not asymptotically improve as Î´ decreases.
  - N/A: N/A
  - Foster et al., 2024: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The modified Forward algorithm computes an L-step executable policy using a latent Markovian policy and random actions to simplify analysis.

### 4. Method
- **Pipeline**: The modified Forward algorithm with L-step random actions.
- **Architecture / Loss / Training**: The architecture is designed to minimize a specific loss function during training, although details are not provided.
- **Complexity / Resources**: The algorithm can be computed in time poly(n, H, XL, AL).

### 5. Experiments
- **Datasets & Metrics**: Simulated locomotion and manipulation tasks.
- **Baselines**: Behavior Cloning, Behavior cloning, DAgger, Expert distillation, Expert distillation methods, Frame-stacking techniques, N/A, Previous belief contraction results, Reinforcement Learning, Standard RL, Standard RL algorithms, Standard RL without latent state information, Standard reinforcement learning (RL), Standard reinforcement learning algorithms
- **Main Results**: The output of the modified Forward algorithm satisfies certain statistical bounds with high probability.
- **Ablations**: To investigate whether the error at initial timesteps is due to parameter sharing, we also tried to train non-stationary models (i.e., one model for each timestep) or using weighted loss with higher weights for the initial timesteps, but neither approach significantly changed the results.
- **Limitations / Stress Tests**: Limitations include potential overfitting in complex environments and the need for extensive tuning.

### 6. Takeaways
- **Pros**: Expert distillation converges faster and more stably than standard RL., It provides a computational advantage in decodable POMDPs., It effectively disentangles representation learning from decision-making.
- **Cons**: Expert distillation has failure modes due to its inability to encourage information-gathering actions., It may not perform well in scenarios with high stochasticity., The prior theoretical model for understanding its benefits is too restrictive.
- **Future Work**: Explore lightweight improvements to expert distillation., Investigate conditions under which expert distillation is as performant as standard RL., Develop new guidelines for effectively exploiting privileged information.

</details>
