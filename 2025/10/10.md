# Daily Paper Digest Â· 2025-10-10
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [ReSplat: Learning Recurrent Gaussian Splats](http://arxiv.org/pdf/2510.08575v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View Synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of efficient and high-quality view synthesis using a recurrent Gaussian splatting model.

- **Related work challenges:**
  - Charatan et al., 2024: Models predict one or multiple Gaussians for each pixel, leading to scalability issues.
  - Kerbl et al., 2023: Per-scene optimization methods are expensive and not efficient for real-time applications.
  - Zhang et al., 2024a: Single-step feed-forward inference limits achievable quality for complex scenes.
  - SplatFormer (Chen et al., 2025a): Evaluated only on object-centric datasets, making it difficult to apply to complex scenes.
  - DeepView (Flynn et al., 2019): Requires explicit gradient computation, which is not always feasible.
  - G3R (Chen et al., 2024d): Struggles with sparse points and requires well-covered 3D points for initialization.
  - Zhao et al., 2021: NaÃ¯ve prediction of Gaussian parameters from point features leads to performance loss.
  - Vaswani et al., 2017: N/A
  - Xu et al., 2024: N/A
  - 3DGS (Kerbl et al., 2023): Requires thousands of iterations to converge, leading to inefficiency.
  - MVSplat (Chen et al., 2024b): Produces millions of Gaussians, resulting in slower rendering speeds.
  - DepthSplat (Xu et al., 2025): Similar inefficiencies in rendering and convergence.
  - Long-LRM (Chen et al., 2025b): Uses Gaussian pruning based on opacity values, leading to a reduction in the number of Gaussians but slower rendering speed.
  - LVSM (Jin et al., 2025): Encoder-decoder architecture that is outperformed by ReSplat in terms of PSNR and rendering speed.
  - MVSplat (Xu et al., 2025): Previous feed-forward 3DGS models that ReSplat outperforms in terms of structure quality.
  - N/A: N/A
  - Depth anything v2: N/A
  - No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images: N/A
  - gsplat: An open-source library for gaussian splatting: N/A
  - Mip-splatting: Alias-free 3d gaussian splatting: N/A
  - Gs-lrm: Large reconstruction model for 3d gaussian splatting: N/A
  - Gaussian graph network: Learning efficient and generalizable gaussian representations from multi-view images: N/A
  - Point transformer: N/A
  - Stereo magnification: learning view synthesis using multiplane images: N/A

### 3. Core Idea
- ReSplat leverages a recurrent Gaussian splatting approach to achieve efficient view synthesis while maintaining high quality by reducing the number of Gaussians and improving rendering speed.

### 4. Method
- **Pipeline**: The method involves a recurrent process that utilizes rendering error as feedback and operates in a compact 3D space.
- **Architecture / Loss / Training**: The model incorporates kNN attention and global attention mechanisms to enhance performance.
- **Complexity / Resources**: The experiments were conducted using 4 GH200 GPUs.

### 5. Experiments
- **Datasets & Metrics**: The experiments were conducted on the RealEstate10K and DL3DV datasets, evaluating metrics such as PSNR, SSIM, and LPIPS.
- **Baselines**: 3DGS, DeepView, DepthSplat, DepthSplat (Xu et al., 2025), G3R, GS-LRM, Long-LRM, Long-LRM (Chen et al., 2025b), MVSplat, MVSplat (Chen et al., 2024b), N/A, Point Transformer, SplatFormer, gsplat
- **Main Results**: Our ReSplat reconstructs cleaner Gaussians and renders higher-quality images than DepthSplat.
- **Ablations**: Ablation studies indicate the importance of kNN attention and global attention in maintaining performance when compressing Gaussians.
- **Limitations / Stress Tests**: The model's reliance on kNN-based point attention incurs high computational costs, especially with large Gaussian counts.

### 6. Takeaways
- **Pros**: Significantly faster rendering speed compared to optimization-based methods., Robust generalization to unseen datasets and image resolutions., Reduced computational overhead with fewer Gaussians.
- **Cons**: Initial model performance may be limited by the subsampling approach., Recurrent updates may introduce complexity in training.
- **Future Work**: Explore further optimizations for recurrent update mechanisms., Investigate applications in real-time rendering scenarios., Expand to other types of neural rendering tasks.

</details>

### [Reconstructing the local density field with combined convolutional and point cloud architecture](http://arxiv.org/pdf/2510.08573v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Recovering the local density field from peculiar velocities

### 2. Motivation & Gaps
- The study aims to improve predictions of high-density small-scale features in cosmology using machine learning techniques.

- **Related work challenges:**
  - Linear reconstruction techniques: Limited to large scales and cannot optimally use dense information.
  - Convolutional neural networks for density field mapping: Suboptimal use of gridded radial velocities and neglect of small-scale information.
  - Direct inversion methods: Often yield biased low amplitude results.
  - 3D Wiener filter: Uses full 3D velocities, which is not realistic in many observational scenarios.
  - U-Net architectures: May not effectively capture small-scale features without additional information.
  - Wiener Reconstruction of Large-Scale Structure from Peculiar Velocities: Linear Wiener filtering does not capture small-scale features effectively.
  - Three-dimensional Velocity and Density Reconstructions of the Local Universe with Cosmicflows-1: Existing methods struggle with moderate tracer density.
  - Field-based physical inference from peculiar velocity tracers: Incorporating observational errors remains a challenge.

### 3. Core Idea
- A hybrid model combining U-Net and confidence network-controlled DeepSets to enhance the recovery of local density fields.

### 4. Method
- **Pipeline**: The model processes peculiar velocity data to reconstruct the local density field.
- **Architecture / Loss / Training**: Utilizes a combination of convolutional networks and point-cloud networks with a focus on confidence estimation.
- **Complexity / Resources**: Moderate computational resources required, with potential scalability for larger datasets.

### 5. Experiments
- **Datasets & Metrics**: Utilizes Cosmicflows-4 data for training and evaluation.
- **Baselines**: 3D Wiener filter, Confidence U-Net (Âµprediction), Direct inversion (1hâˆ’1Mpc smoothing), Direct inversion (4hâˆ’1Mpc smoothing), Linear Wiener filtering, Normal U-Net, Previous density reconstruction methods, U-Net-only approach
- **Main Results**: Significant improvement in recovering small-scale features compared to linear methods.
- **Ablations**: Further analysis needed to understand the impact of different model components.
- **Limitations / Stress Tests**: Observational errors could affect the robustness of the results.

### 6. Takeaways
- **Pros**: Improved reconstruction quality on nonlinear scales., Efficient use of small-scale information., Combines strengths of convolutional and point-cloud architectures.
- **Cons**: High computational cost due to DeepSets evaluation., Limited to numerical simulations in this work., Potential challenges in applying to observational data.
- **Future Work**: Incorporate stochasticity in the network architecture., Explore joint inference of cosmological parameters and density field., Evaluate on observational data with careful treatment of systematic effects.

</details>

### [Who Said Neural Networks Aren't Linear?](http://arxiv.org/pdf/2510.08570v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Learning invertible coordinate transformations for neural networks

### 2. Motivation & Gaps
- The paper discusses the learning of invertible coordinate transformations that allow neural networks to act as exact linear operators, leading to applications in flow matching, style transfer, and idempotency enforcement.

- **Related work challenges:**
  - Hornik et al. (1989): Neural networks are famously nonlinear, complicating their analysis and manipulation.
  - Strang (2022): Linear systems have well-understood properties that do not apply to nonlinear systems.
  - Strogatz (2024): Iterating nonlinear mappings can lead to intractable dynamics.
  - Flow matching (Lipman et al., 2023; Song et al., 2021b): Traditional velocity integration over time is slow.
  - V AEs (Kingma & Welling, 2014): Lack of a natural encoder for mapping data back into the prior space.
  - Inversion methods for diffusion (Dhariwal & Nichol, 2021; Song et al., 2021a; Huberman-Spiegelglas et al., 2024): Approximate techniques suffer from reconstruction errors and computational overhead.
  - Gatys et al. (2016): Proposed optimizing image pixels for style transfer but lacked practical implementation.
  - Johnson et al. (2016): Introduced perceptual-loss training but did not address idempotency in generative models.
  - Shocher et al. (2024): Developed Idempotent Generative Networks but faced challenges in enforcing true idempotency.
  - Koopman operator theory: Linearizes nonlinear dynamics but does not address learned mappings.
  - Neural Tangent Kernel: Achieves linearity in parameter space but remains nonlinear in input-output mapping.
  - Invertible Neural Networks: Focuses on density modeling rather than enabling linearity in learned mappings.
  - One-step offline distillation of diffusion-based models via koopman modeling: N/A
  - Image style transfer using convolutional neural networks: N/A
  - Flow matching for generative modeling: N/A
  - Denoising diffusion implicit models: N/A
  - Score-based generative modeling through stochastic differential equations: N/A
  - Consistency models: N/A
  - N/A: N/A

### 3. Core Idea
- The core idea is to utilize invertible coordinate transformations to enhance the expressivity and efficiency of neural networks in various applications.

### 4. Method
- **Pipeline**: The framework involves learning invertible mappings and a finite matrix that captures the linearity of the neural network's function.
- **Architecture / Loss / Training**: The architecture requires careful design due to the challenges of training invertible networks.
- **Complexity / Resources**: The method may require significant computational resources for training and scaling.

### 5. Experiments
- **Datasets & Metrics**: The experiments involve various datasets to demonstrate the feasibility of the Linearizer framework.
- **Baselines**: Gatys et al. (2016), Invertible Neural Networks, Johnson et al. (2016), Koopman Distillation Model, Multi-step flow matching, N/A, Normalizing Flows, Shocher et al. (2024), Standard neural network architectures
- **Main Results**: The Linearizer shows improved efficiency and performance in one-step flow matching and other applications.
- **Ablations**: Ablation studies are needed to assess the impact of different components of the framework.
- **Limitations / Stress Tests**: The framework's expressivity and scalability to state-of-the-art benchmarks remain open questions.

### 6. Takeaways
- **Pros**: Enables the application of linear algebra techniques to nonlinear mappings., Facilitates efficient training and inference in diffusion models., Provides a new perspective on the structure and capabilities of neural networks.
- **Cons**: The framework may not be applicable to all types of neural networks., Potential complexity in defining appropriate invertible functions., Limited discussion on practical implementation challenges.
- **Future Work**: Explore the application of Linearizers in various neural network architectures., Investigate the implications of Linearizers on model interpretability., Develop guidelines for selecting appropriate invertible functions.

</details>

## Gaussian Splatting

### [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](http://arxiv.org/pdf/2510.08566v1)
  (summary failed: 'utf-8' codec can't encode characters in position 8413-8414: surrogates not allowed)


### [ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation](http://arxiv.org/pdf/2510.08551v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction and localization

### 2. Motivation & Gaps
- The paper presents ARTDECO, a framework that combines feed-forward priors and structured Gaussian representations for efficient and accurate 3D reconstruction.

- **Related work challenges:**
  - Per-scene optimization methods: Achieve high accuracy but are computationally expensive.
  - Feed-forward models: Enable real-time inference but struggle with accuracy and robustness.
  - MÃ¼ller et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Xu et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Sun et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Lin et al., 2021: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Fu et al., 2024: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Lin et al., 2025b: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Prior 3DGS-based SLAM methods: Relying only on keyframes for reconstruction, which limits the use of captured information.
  - MASt3R predictions: Instability near object boundaries affecting the reliability of reprojection residuals.
  - Loop Closure Detection: Weak correspondences and noisy inputs leading to unreliable loop closures.
  - 3D Gaussian Splatting approaches: Maintaining high-quality rendering details in diverse environments.
  - SLAM systems: Achieving accurate pose estimation in challenging multi-scale datasets.
  - MASt3R-SLAM: Limited robustness under noise, blur, or lighting changes.
  - 3DGS-based methods: Struggles with small-parallax inputs leading to ghosting and blur.
  - Existing SLAM systems: Assumes consistent illumination and sufficient parallax, which can lead to drift or artifacts.
  - N/A: N/A
  - N/A: N/A
  - MASt3R (Leroy et al., 2024b): Predicts two-view correspondences and metric pointmaps with well-calibrated scale but limited temporal context.
  - Ï€3 (Wang et al., 2025b): Not explicitly metric-aware, integrating it into a streaming SLAM-style system introduces challenges in maintaining scale and temporal consistency.
  - N/A: N/A
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- ARTDECO advances on-the-fly 3D reconstruction by effectively combining feed-forward priors and structured Gaussian representations.

### 4. Method
- **Pipeline**: The method involves a feed-forward model for 3D reconstruction and localization from monocular image sequences.
- **Architecture / Loss / Training**: Utilizes a combination of PSNR, SSIM, and LPIPS metrics for performance evaluation.
- **Complexity / Resources**: The system is designed for efficiency, achieving strong results with a focus on runtime performance.

### 5. Experiments
- **Datasets & Metrics**: Fast-LIVO2, TUM, ScanNet, and others with PSNR, SSIM, and LPIPS metrics.
- **Baselines**: DPV-SLAM, DROID-SLAM, Feed-forward models, Go-SLAM, LongSplat, MASt3R, MASt3R-SLAM, MonoGS, N/A, NeRF-based SLAM methods, OnTheFly, OnTheFly-NVS, Ours, Per-scene optimization methods, Prior 3DGS-based SLAM methods, S3PO-GS, SEGS-SLAM, SLAM, Ï€3
- **Main Results**: ARTDECO consistently achieves the highest PSNR and SSIM and the lowest LPIPS across all datasets.
- **Ablations**: Replacing the pairwise correspondence model MASt3R with the multi-frame visual-geometry model Ï€3.
- **Limitations / Stress Tests**: Identified limitations include dependency on 3D foundation models and challenges with low-texture surfaces.

### 6. Takeaways
- **Pros**: High fidelity and efficiency at scale., Robustness across diverse indoor and outdoor scenes., Practical path toward on-the-fly digitization of real-world environments.
- **Cons**: Generally achieve lower accuracy than per-scene optimized methods., Challenges with maintaining global consistency., Handling high-resolution inputs and processing long sequences.
- **Future Work**: Explore further optimizations in rendering efficiency., Investigate additional applications in AR/VR., Enhance robustness in more complex environments.

</details>

## avatar

### [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](http://arxiv.org/pdf/2510.07685v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-time reasoning in interactive e-commerce applications

### 2. Motivation & Gaps
- The paper addresses the need for low-latency, high-performance reasoning models in interactive e-commerce settings.

- **Related work challenges:**
  - Retrieval-Augmented Generation (RAG) systems powered by Large Reasoning Models (LRMs): High inference delays despite strong reasoning capabilities.
  - Reinforcement Learning for LLMs: Aligning LLMs with human preferences while addressing hallucinations and harmful content generation.
  - Efficient Reasoning: Mitigating the 'overthinking' phenomenon in LLMs that leads to verbose reasoning paths.
  - Teacher-based Data Generation: Ensuring the quality of distillation data through effective filtering of generated trajectories.
  - Student MoE Model Fine-tuning: Balancing the load across experts in a Mixture of Experts model to ensure stable training.
  - Reinforcement Learning Optimization: Optimizing for multiple objectives such as reasoning efficiency, correctness, and helpfulness.
  - DeepSeek-R1: High computational cost and inefficiency in generating responses.
  - Standard PPO: Lack of stability and efficiency in training with complex multi-objective rewards.
  - Existing AI models for livestreaming: Lack of efficiency and quality in reasoning
  - Knowledge distillation: Standard distillation inherits verbose reasoning trajectories from the teacher model.
  - MoE architecture: Balancing performance and computational cost in latency-critical applications.
  - Reinforcement learning for reasoning: Minimizing hallucinations while promoting helpfulness in generated responses.
  - Demystifying long chain-of-thought reasoning in llms: N/A
  - Efficient rl training for reasoning models via length-aware optimization: N/A
  - Large language models for information retrieval: A survey: N/A
  - N/A: N/A

### 3. Core Idea
- Introducing LiveThinking, a two-stage training framework that adapts large reasoning models to real-time applications by balancing quality and latency.

### 4. Method
- **Pipeline**: The approach leverages RFT to distill reasoning ability from a 670B teacher model into a lightweight 30B student model, followed by GRPO to compress reasoning paths.
- **Architecture / Loss / Training**: Utilizes a multi-objective reward function to enhance correctness and helpfulness while compressing reasoning paths.
- **Complexity / Resources**: Achieves a 30-fold reduction in computational cost compared to the teacher model.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on proprietary Tblive-E-Commerce QA dataset and the public MuSiQue dataset.
- **Baselines**: 14B dense production model, 670B teacher model, 670B teacher model (DeepSeek-R1), DeepSeek-R1, DeepSeek-R1-Distilled-Llama-8B, Dense models ranging from 4B to 32B parameters, Existing AI models for e-commerce reasoning, N/A, Qwen3-235B-A22B, Qwen3-30B-A3B, Qwen3-32B, Traditional sequence-to-sequence models
- **Main Results**: LiveThinking outperforms its larger teacher model on correctness and helpfulness while being more computationally efficient.
- **Ablations**: Ablation studies confirm the effectiveness of RFT and RL in improving correctness and helpfulness.
- **Limitations / Stress Tests**: The model's performance is sensitive to the target reasoning length, indicating a trade-off between brevity and quality.

### 6. Takeaways
- **Pros**: Significantly reduces computational cost., Improves response correctness and helpfulness., Demonstrates effectiveness in high-traffic production environments.
- **Cons**: Initial model inherits verbose reasoning paths., Latency may still exceed industrial requirements despite optimizations.
- **Future Work**: Explore further optimizations for reasoning paths., Investigate additional applications in real-time systems., Enhance the model's ability to handle more complex queries.

</details>

### [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](http://arxiv.org/pdf/2510.05488v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Gaussian head avatar creation

### 2. Motivation & Gaps
- The paper addresses the need for real-time and continuous adjustment of the level of detail (LOD) in 3D Gaussian head avatars.

- **Related work challenges:**
  - 3D Gaussian Splatting (3DGS): Struggles to maintain real-time performance when multiple avatars are rendered simultaneously.
  - Conventional LOD methods: Provide only a few discrete levels, leading to unsmooth visual effects when switching between levels.
  - UV-based strategies: Do not capture sufficient local information for detailed 3D head appearance.
  - 3D Morphable Models (3DMMs): Less effective at modeling non-rigid facial features like hair.
  - Neural radiance field (NeRF)-based methods: Computationally intensive and less accurate with geometry.
  - LoDAvatar: Only supports discrete LOD control and relies on synthetic multi-view images for training.
  - N/A: N/A
  - GaussianAvatars: Limited detail preservation at high LOD.
  - FlashAvatar: Quality degradation at lower LOD.
  - RGBAvatar: Inability to maintain reasonable quality across varying LOD.
  - Existing methods for 3D head avatars: Limited control over LOD and reliance on accurate tracking for 3D-2D alignment.
  - N/A: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A
  - Headgap: Few-shot 3d head avatar via generalizable gaussian priors: N/A
  - Pointavatar: Deformable point-based head avatars from videos: N/A
  - Instant volumetric head avatars: N/A

### 3. Core Idea
- ArchitectHead introduces a framework that allows for continuous LOD control in 3D Gaussian head avatars by parameterizing Gaussians in UV feature space.

### 4. Method
- **Pipeline**: The method uses a neural decoder to generate Gaussian attributes based on the LOD as an additional condition.
- **Architecture / Loss / Training**: The architecture incorporates a learnable UV latent feature map alongside the UV position map to enhance representation.
- **Complexity / Resources**: The method balances different LODs while ensuring smooth transitions, requiring significant computational resources for training.

### 5. Experiments
- **Datasets & Metrics**: Experiments were conducted on monocular video datasets, with results summarized in various tables and figures.
- **Baselines**: 3D Morphable Models (3DMMs), Conventional LOD methods, Existing 3D head avatar methods, Existing 3DGS-based avatars, FlashAvatar, Gaussian Dejavu, GaussianAvatars, LoDAvatar, N/A, Neural radiance field (NeRF)-based methods, RGBAvatar
- **Main Results**: ArchitectHead achieves state-of-the-art quality in generating 3D head avatars.
- **Ablations**: Ablation studies show the impact of using different resolutions of feature maps on performance.
- **Limitations / Stress Tests**: The method relies on accurate FLAME tracking and may overfit to rare expression modes under large head poses.

### 6. Takeaways
- **Pros**: Supports continuous LOD control for better rendering efficiency., Achieves state-of-the-art quality in head avatar rendering., Allows dynamic adjustment of detail without retraining.
- **Cons**: Initial training requires high computational resources., UV position map may not capture all local details., Quality degradation at lower LODs, though moderate.
- **Future Work**: Explore further optimizations for real-time performance., Investigate additional applications in VR and telepresence., Enhance the UV feature field to capture more detailed head appearances.

</details>

### [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](http://arxiv.org/pdf/2510.04822v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D Virtual Try-On

### 2. Motivation & Gaps
- The paper addresses the challenges of achieving realistic 4D virtual try-on (VTON) using single in-shop garment references, focusing on dynamic pose control and multi-view rendering.

- **Related work challenges:**
  - Image-based VTON models: Lack intrinsic 3D perceptual understanding, producing discontinuous try-on results across changing viewpoints and poses.
  - Animatable avatar-based garment transfer approaches: Depend on large-scale datasets, limiting scalability and practical use.
  - 3D VTON methods: Do not support dynamic manipulation.
  - VITON: Relying primarily on 2D data, struggles with consistent results across varying viewpoints and poses.
  - ViViD: Requires continuous video input, increasing computational and memory demands.
  - GaussianEditor: Lacks precision in controlling detailed textures.
  - Video-based VTON methods: Lack of 3D structural awareness and high computational costs.
  - Image-based VTON methods: Inability to effectively handle temporal coherence across poses and viewpoints.
  - ViViD: Lacks explicit 3D structural reasoning, resulting in texture flickering and deformation errors.
  - IDM-VTON: Limited input sequence length degrades temporal continuity in long sequences.
  - GaussianEditor: Requires per-frame optimization and suffers from severe temporal inconsistency.
  - IDM-VTON combined with AG: Exhibits noticeable temporal flickering and inconsistent texture patterns across frames.
  - ViViD: Lacks genuine 3D spatial reasoning and requires substantial computational resources.
  - N/A: N/A

### 3. Core Idea
- The proposed AvatarVTON framework utilizes a Reciprocal Flow Rectifier for optical flow correction and a Non-Linear Deformer for adaptive deformations, enhancing rendering quality and stability.

### 4. Method
- **Pipeline**: The framework processes single garment references to enable dynamic pose control and multi-view rendering without multi-view datasets.
- **Architecture / Loss / Training**: Incorporates adversarial loss to improve texture clarity and color accuracy.
- **Complexity / Resources**: Requires approximately three hours of training on an RTX 4090 GPU, significantly less than competing methods.

### 5. Experiments
- **Datasets & Metrics**: Utilizes datasets from AvatarReX, ActorsHQ, DressCode, and VITON-HD, evaluating garment texture fidelity, human identity preservation, video temporal coherence, and overall realism.
- **Baselines**: 3D VTON methods, Animatable Gaussians (AG), GaussianEditor, GaussianEditor (3D editing method), GaussianVTON, IDM-VTON, IDM-VTON (2D image-based VTON), IDM-VTON + AG, LHM (4D approach), N/A, SCARF (NeRF-based animatable human reconstruction), VITON, ViViD, ViViD (2D video-based VTON)
- **Main Results**: AvatarVTON consistently achieves higher scores than all competitors across all evaluation dimensions.
- **Ablations**: Demonstrated the impact of removing L_adv on texture clarity and color accuracy.
- **Limitations / Stress Tests**: Identified limitations in handling out-of-distribution scenarios and the need for improved 3D perception capabilities.

### 6. Takeaways
- **Pros**: High-fidelity 4D virtual try-on from a single garment image., Free viewpoint and pose control., Mitigates view-pose coupling inconsistencies.
- **Cons**: Dependence on single 2D garment images may limit realism., Complexity in ensuring coherent avatar geometry., Potential challenges in dynamic garment interactions.
- **Future Work**: Explore integration with more complex garment dynamics., Investigate scalability to larger datasets., Enhance the framework for real-time applications.

</details>

## video understanding

### [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](http://arxiv.org/pdf/2510.08568v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Zero-Shot Manipulation

### 2. Motivation & Gaps
- The paper addresses the need for versatile manipulation techniques that can handle various object types and manipulation scenarios.

- **Related work challenges:**
  - Vision-Language-Action (VLA) models: Require vast quantities of robot-specific vision-language-action data that is difficult and expensive to collect.
  - Modular systems for task understanding and robot control: Translating high-level understanding into physical actions remains an open problem.
  - Prior work on self-collected data for training video models: Reintroduces the data bottleneck and limits generalizability and scalability.
  - Concurrent work on 6D pose extraction for demonstration-free manipulation: Relies on a rigid-body assumption, limiting applicability to a broader class of objects.
  - Flow-based manipulation methods: Require robot data or task-specific training, hindering zero-shot manipulation capabilities.
  - Prior work on object manipulation: Limited ability to handle complex object dynamics and real-time tracking.
  - AVDC: Struggles with precise, long-horizon placements due to lack of 3D awareness.
  - VidBot: Fails when tasks require objectâ€“object relations and precise relative pose placement.
  - Diffusion Policy: Shows poor generalization from a few examples due to random sampling in evaluations.
  - Veo: Closed-source model that produces video clips but may not be physically plausible.
  - Wan2.1: Open-source model that significantly drops performance without a goal image.
  - Open-World Object Manipulation using Pre-Trained Vision-Language Models: Limited adaptability to new environments without retraining.
  - Local Policies Enable Zero-shot Long-horizon Manipulation: Difficulty in generalizing across diverse manipulation tasks.
  - Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations: Dependence on high-quality video generation for effective learning.
  - Wan2.1: Limited support for all required modes in newer versions.
  - Veo: Lack of goal-image conditioning in the model.
  - MegaSaM: Ambiguity in estimated depth maps even after postprocessing.
  - Grounded-SAM2: Need for accurate object grounding in video sequences.
  - Trajectory Optimization: Finding optimal, collision-free, and smooth sequences of joint configurations.
  - Previous manipulation methods: Limited to specific object types and require extensive training data.

### 3. Core Idea
- NovaFlow enables zero-shot manipulation by generating actionable flows from videos, allowing manipulation of diverse objects without prior training.

### 4. Method
- **Pipeline**: The method involves generating actionable flows from videos and applying them to manipulate objects in real-time.
- **Architecture / Loss / Training**: Utilizes a combination of smoothness weight, collision penalty weight, and regularization for training.
- **Complexity / Resources**: The method is designed to be viewpoint-agnostic and can be deployed on various platforms after hand-eye calibration.

### 5. Experiments
- **Datasets & Metrics**: Real-world manipulation experiments were conducted to evaluate the effectiveness of NovaFlow.
- **Baselines**: AVDC, CausalSAM, Data-dependent methods, Demonstration-free methods, Diffusion Policy, Existing video-based manipulation methods, Existing zero-shot techniques, Grounded-SAM2, Inverse Dynamics Model, MegaSaM, Object-centric approaches, Other state-of-the-art video-based manipulation techniques, Recent zero-shot learning approaches, Traditional manipulation methods, Traditional robotic manipulation methods, Veo, VidBot, Wan2.1
- **Main Results**: NovaFlow successfully manipulated rigid, deformable, and articulated objects across different scenarios.
- **Ablations**: Ablation studies demonstrate the effectiveness of each component in the pipeline, particularly the impact of rejection sampling on the quality of generated flows.
- **Limitations / Stress Tests**: The method's limitations include potential inaccuracies in depth estimation and challenges in grounding objects in complex scenes.

### 6. Takeaways
- **Pros**: No need for task-specific tuning., Generalizable across different embodiments., State-of-the-art zero-shot performance on real-world tasks.
- **Cons**: Dependency on the quality of video generation models., Potential limitations in handling highly complex tasks.
- **Future Work**: Explore further enhancements in video generation models., Investigate the application of NovaFlow in more complex environments., Develop methods to improve the robustness of the flow executor.

</details>

### [Where Have All the Kaczmarz Iterates Gone?](http://arxiv.org/pdf/2510.08563v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Numerical analysis of the Randomized Kaczmarz algorithm in noisy linear systems

### 2. Motivation & Gaps
- The paper addresses the challenges of solving doubly-noisy linear systems using the Randomized Kaczmarz algorithm, providing theoretical bounds and empirical validation.

- **Related work challenges:**
  - Strohmer and Vershynin (2009): Demonstrated that RK converges linearly in expectation for consistent systems but did not address inconsistent systems.
  - Needell (2014): Showed that RK iterates approach a ball centering at the least squares solution for consistent systems, but did not consider noise in both the coefficient matrix and the right-hand side.
  - Bergou et al. (2021): Provided convergence analysis for RK in general noisy cases, but the understanding of RK iterates in inconsistent systems remains incomplete.
  - Needell [15]: All works on noisy systems have used the LS solution of the underlying consistent noiseless system as a reference point.
  - Theorems 1.2 and 1.3: Require specific initial points which may not hold in noisy scenarios.
  - N/A: N/A
  - Previous studies on convergence of iterative methods: Lack of precise bounds for limit points in the presence of noise.
  - LIBSVM: A library for support vector machines: Handling noise in real-world data while ensuring convergence of the algorithm.
  - Efficient and robust solution strategies for saddle-point systems: Developing methods that are both efficient and robust in the presence of noise.
  - Numerical approximation of some linear stochastic partial differential equations driven by special additive noises: N/A
  - Online and batch supervised background estimation via l1 regression: N/A
  - New variants of the POCS method using affine subspaces of finite codimension with applications to irregular sampling: N/A

### 3. Core Idea
- The paper presents theoretical bounds for the convergence of the Randomized Kaczmarz algorithm in noisy linear systems and validates these bounds through numerical experiments.

### 4. Method
- **Pipeline**: The method involves generating noisy linear systems and applying the Randomized Kaczmarz algorithm to analyze convergence and approximation errors.
- **Architecture / Loss / Training**: Theoretical bounds are derived and compared against empirical results to validate the algorithm's performance.
- **Complexity / Resources**: The experiments utilize synthetic data and real-world datasets from LIBSVM, with computational resources focused on averaging results over multiple runs.

### 5. Experiments
- **Datasets & Metrics**: Experiments are conducted using synthetic data and real-world datasets from LIBSVM, measuring approximation errors and convergence rates.
- **Baselines**: Cyclic Kaczmarz algorithm, Empirical convergence rates, N/A, Previous works on noisy systems, Randomized Kaczmarz algorithm, Theoretical bounds from Theorem 5 and Corollary 2, Theoretical bounds from previous works
- **Main Results**: The results show that the bounds are valid and that Corollary 2 exhibits a faster convergence rate than Theorem 5.
- **Ablations**: Ablation studies demonstrate the impact of different reference points on convergence behavior.
- **Limitations / Stress Tests**: The limitations include the requirement for noise in the data and the assumption of certain properties of the linear systems.

### 6. Takeaways
- **Pros**: Provides insights into the behavior of Kaczmarz iterates in noisy environments., Establishes bounds on convergence that can guide practical applications., Paves the way for optimized applications in real-world scientific and engineering problems.
- **Cons**: Limited understanding of RK behavior in highly inconsistent systems., Theoretical results may not fully capture practical performance., Numerical experiments may not cover all possible scenarios.
- **Future Work**: Further exploration of RK in more complex noisy environments., Development of modified algorithms for better performance in inconsistent systems., Investigation of other iterative methods in similar contexts.

</details>

### [MultiCOIN: Multi-Modal COntrollable Video INbetweening](http://arxiv.org/pdf/2510.08561v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in high-dynamic video generation.

- **Related work challenges:**
  - Framer: Focused on respecting motion trajectories but lacks versatility and fine-grained user controls.
  - Framer: Achieving impressive results in controllable inbetweening using motion trajectories.
  - Stable Video Diffusion: Generating temporally coherent content.
  - Tune-a-video: Facilitating few-shot video generation.
  - Stable Video Diffusion (SVD): Maintaining temporal consistency across frames.
  - Diffusion Transformer (DiT): Modeling long-range dependencies and global context for fine details.
  - Framer: Relies solely on trajectory control, limiting flexibility in content editing.
  - Framer: Motion is introduced as an external condition, which may not effectively integrate with video features.
  - N/A: N/A
  - Make pixels dance: High-dynamic video generation: High-dynamic video generation techniques are still evolving.
  - The unreasonable effectiveness of deep features as a perceptual metric: Existing metrics may not adequately capture perceptual quality.
  - Audio-driven neural gesture reenactment with video motion graphs: Integrating audio and video for realistic gesture reenactment remains complex.

### 3. Core Idea
- The core idea is to utilize a masked generative transformer architecture to enhance video generation capabilities.

### 4. Method
- **Pipeline**: The method involves a generative pipeline that incorporates masked learning techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for video generation tasks.
- **Complexity / Resources**: The method requires significant computational resources for training due to the complexity of video data.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various video datasets and employ metrics for evaluating video quality.
- **Baselines**: Diffusion Transformer, Existing video generation models, Framer, N/A, Stable Video Diffusion, Traditional generative models, Tune-a-video
- **Main Results**: The results demonstrate improved performance in generating high-quality videos compared to baseline models.
- **Ablations**: Ablation studies indicate the importance of the masked learning approach in enhancing video quality.
- **Limitations / Stress Tests**: Limitations include the need for extensive computational resources and potential overfitting on specific datasets.

### 6. Takeaways
- **Pros**: Allows for versatile and fine-grained user controls., Achieves high-quality and fine-grained video interpolation., Empowers users to create smooth and plausible transitions.
- **Cons**: Requires advanced generative models which may be complex to implement., Mapping controls into a common representation may introduce additional processing steps.
- **Future Work**: Explore further enhancements in user control granularity., Investigate additional modalities for video inbetweening.

</details>
