# Daily Paper Digest Â· 2025-10-11
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## neural rendering

### [ReSplat: Learning Recurrent Gaussian Splats](http://arxiv.org/pdf/2510.08575v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View Synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of efficient and high-quality view synthesis using a recurrent Gaussian splatting model.

- **Related work challenges:**
  - Charatan et al., 2024: Models predict one or multiple Gaussians for each pixel, leading to scalability issues.
  - Kerbl et al., 2023: Per-scene optimization methods are expensive and not efficient for real-time applications.
  - Zhang et al., 2024a: Single-step feed-forward inference limits achievable quality for complex scenes.
  - SplatFormer (Chen et al., 2025a): Evaluated only on object-centric datasets, making it difficult to apply to complex scenes.
  - DeepView (Flynn et al., 2019): Requires explicit gradient computation, which is not always feasible.
  - G3R (Chen et al., 2024d): Struggles with sparse points and requires well-covered 3D points for initialization.
  - Zhao et al., 2021: NaÃ¯ve prediction of Gaussian parameters leads to considerable performance loss.
  - Vaswani et al., 2017: Need for effective encoding of 3D context information.
  - Xu et al., 2024; Chen et al., 2024a: Integrating local and global 3D contexts for improved feature expressiveness.
  - 3DGS (Kerbl et al., 2023): Requires several thousands of iterations to reach convergence.
  - MVSplat (Chen et al., 2024b): Produces millions of Gaussians, leading to slower rendering speeds.
  - DepthSplat (Xu et al., 2025): Similar inefficiencies in rendering and convergence times.
  - Long-LRM (Chen et al., 2025b): Uses Gaussian pruning based on opacity values, leading to a reduction in the number of Gaussians but slower rendering speed.
  - LVSM (Jin et al., 2025): Encoder-decoder architecture that is outperformed by ReSplat in terms of PSNR and rendering speed.
  - 3DGS models: Previous models do not achieve the same level of performance and efficiency as ReSplat.
  - N/A: N/A
  - Depth anything v2: N/A
  - No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images: N/A
  - gsplat: An open-source library for gaussian splatting: N/A
  - Mip-splatting: Alias-free 3d gaussian splatting: N/A
  - Gs-lrm: Large reconstruction model for 3d gaussian splatting: N/A
  - Gaussian graph network: Learning efficient and generalizable gaussian representations from multi-view images: N/A
  - Point transformer: N/A
  - Stereo magnification: learning view synthesis using multiplane images: N/A

### 3. Core Idea
- ReSplat leverages a recurrent Gaussian splatting approach to achieve efficient view synthesis while maintaining high quality by reducing the number of Gaussians and utilizing rendering error as feedback.

### 4. Method
- **Pipeline**: The method involves a recurrent process that operates in a compact subsampled 3D space, allowing for efficient rendering and high-quality output.
- **Architecture / Loss / Training**: The model incorporates kNN attention and global attention mechanisms to enhance performance while compressing the number of Gaussians.
- **Complexity / Resources**: The experiments were conducted using 4 GH200 GPUs, indicating a significant computational resource requirement.

### 5. Experiments
- **Datasets & Metrics**: The experiments were conducted on the DL3DV dataset and RealEstate10K, using metrics such as PSNR, SSIM, and LPIPS.
- **Baselines**: 3DGS, DeepView, DepthSplat, DepthSplat (Xu et al., 2025), G3R, GS-LRM, LVSM, Long-LRM, Long-LRM (Chen et al., 2025b), MVSplat, MVSplat (Chen et al., 2024b), N/A, Point Transformer, SplatFormer, gsplat Mip-Splatting
- **Main Results**: Our ReSplat reconstructs cleaner Gaussians and renders higher-quality images than DepthSplat.
- **Ablations**: Ablation studies indicate the importance of kNN attention and global attention in maintaining performance.
- **Limitations / Stress Tests**: The model's reliance on kNN-based point attention incurs high computational costs, especially with large numbers of Gaussians.

### 6. Takeaways
- **Pros**: Significantly faster rendering speed compared to optimization-based methods., Robust generalization to unseen datasets and resolutions., Reduced computational overhead with fewer Gaussians.
- **Cons**: Initial model performance may be limited by the subsampling approach., Recurrent updates may introduce complexity in training.
- **Future Work**: Explore further optimizations for recurrent update mechanisms., Investigate applications in real-time rendering scenarios., Develop methods to enhance performance on more complex datasets.

</details>

### [Reconstructing the local density field with combined convolutional and point cloud architecture](http://arxiv.org/pdf/2510.08573v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Recovering the local density field from peculiar velocities

### 2. Motivation & Gaps
- The study aims to improve predictions of high-density small-scale features in cosmology using machine learning techniques.

- **Related work challenges:**
  - Traditional linear methods for density reconstruction: Limited to large scales and cannot optimally use dense information from modern peculiar velocity data.
  - Machine learning approaches using convolutional neural networks: Suboptimal use of gridded radial velocities and neglect of small-scale information.
  - Direct inversion methods: Often yield biased low amplitude results.
  - 3D Wiener filter: Uses full 3D velocities, which is not realistic in many observational scenarios.
  - U-Net architectures: May not effectively capture small-scale features without additional information.
  - Wiener Reconstruction of Large-Scale Structure from Peculiar Velocities: Linear Wiener filtering does not capture small-scale features effectively.
  - Three-dimensional Velocity and Density Reconstructions of the Local Universe with Cosmicflows-1: Existing methods struggle with moderate tracer density.
  - Field-based physical inference from peculiar velocity tracers: Challenges in accurately modeling the non-linear matter distribution.

### 3. Core Idea
- A hybrid model combining U-Net and confidence network-controlled DeepSets to enhance the recovery of local density fields.

### 4. Method
- **Pipeline**: The model processes peculiar velocities to reconstruct the local density field.
- **Architecture / Loss / Training**: Utilizes a combination of convolutional networks and point-cloud networks with a focus on confidence estimation.
- **Complexity / Resources**: Moderate computational resources required, with potential scalability for larger datasets.

### 5. Experiments
- **Datasets & Metrics**: Utilizes Cosmicflows-4 data for training and evaluation.
- **Baselines**: 3D Wiener filter, Confidence U-Net (Âµprediction), Direct inversion (1hâˆ’1Mpc smoothing), Direct inversion (4hâˆ’1Mpc smoothing), Linear Wiener filtering, Normal U-Net, Previous density reconstruction methods, Traditional linear methods, U-Net-only approach
- **Main Results**: Significant improvement in recovering small-scale features compared to linear methods.
- **Ablations**: Further analysis needed to understand the impact of different model components.
- **Limitations / Stress Tests**: Observational errors may affect the results, which will be addressed in future work.

### 6. Takeaways
- **Pros**: Improved reconstruction quality on nonlinear scales., Efficient use of both small-scale and large-scale information., Potential for future applications in joint inference of cosmological parameters.
- **Cons**: High computational cost due to DeepSets evaluation., Limited to numerical simulations in this work., Neglect of stochasticity in the current architecture.
- **Future Work**: Incorporate stochasticity in the network architecture., Evaluate on observational data with careful treatment of systematic effects., Explore further improvements in small-scale behavior using advanced models.

</details>

### [Who Said Neural Networks Aren't Linear?](http://arxiv.org/pdf/2510.08570v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Learning invertible coordinate transformations for neural networks

### 2. Motivation & Gaps
- The paper discusses the learning of invertible coordinate transformations that allow neural networks to function as exact linear operators, leading to applications in flow matching, style transfer, and idempotency enforcement.

- **Related work challenges:**
  - Hornik et al. (1989): Neural networks are famously nonlinear, complicating their analysis and manipulation.
  - Strang (2022): Linear systems have well-understood properties that do not apply to nonlinear systems.
  - Strogatz (2024): Iterating nonlinear mappings can lead to intractable dynamics.
  - Flow matching (Lipman et al., 2023; Song et al., 2021b): Traditional velocity integration over time is slow.
  - V AEs (Kingma & Welling, 2014): Lack of a natural encoder for mapping data back into the prior space.
  - Inversion methods for diffusion (Dhariwal & Nichol, 2021; Song et al., 2021a; Huberman-Spiegelglas et al., 2024): Approximate techniques suffer from reconstruction errors and computational overhead.
  - Gatys et al. (2016): Initial style transfer methods were computationally intensive and not practical for real-time applications.
  - Johnson et al. (2016): Perceptual-loss training improved efficiency but still faced issues with idempotency and information preservation.
  - Shocher et al. (2024): Idempotent Generative Networks (IGNs) struggled with enforcing true idempotency and often required complex optimization.
  - Koopman operator theory: Linearizes nonlinear dynamics but does not address learned mappings.
  - Neural Tangent Kernel: Achieves linearity in parameter space but remains nonlinear in input-output mapping.
  - Invertible Neural Networks: Inherently more challenging to train than standard architectures.
  - Scaling one-step flow matching and IGN to larger datasets: Achieving competitive generative models with unprecedented efficiency.
  - Modeling motion dynamics using matrix exponentials: Extending the approach beyond generation to physical and temporal modeling.
  - Characterizing theoretical limits of Linearizer expressivity: Integrating with other operator-learning frameworks.
  - Denoising diffusion implicit models: N/A
  - Score-based generative modeling through stochastic differential equations: N/A
  - Consistency models: N/A
  - N/A: N/A

### 3. Core Idea
- Learning invertible coordinate transformations to enable neural networks to act as exact linear operators.

### 4. Method
- **Pipeline**: Learn invertible coordinate maps and a finite matrix to achieve exact linearity.
- **Architecture / Loss / Training**: Requires careful design due to the complexity of training invertible networks.
- **Complexity / Resources**: Involves significant computational resources for training and implementation.

### 5. Experiments
- **Datasets & Metrics**: Utilizes various datasets to demonstrate the feasibility of the Linearizer framework.
- **Baselines**: Gatys et al. (2016), Johnson et al. (2016), Multi-step flow matching, N/A, Shocher et al. (2024), Standard neural network architectures
- **Main Results**: Demonstrates the ability to achieve exact linearity in learned mappings.
- **Ablations**: Ablation studies confirm the effectiveness of the Linearizer in preserving information and achieving idempotency.
- **Limitations / Stress Tests**: Identifies challenges in training invertible networks and the need for further theoretical exploration.

### 6. Takeaways
- **Pros**: Enables the application of linear algebra techniques to nonlinear mappings., Facilitates efficient training and inference in neural networks., Provides a new perspective on the structure and capabilities of neural networks.
- **Cons**: The method may not be applicable to all types of neural network architectures., Potential complexity in defining appropriate invertible functions., The framework's effectiveness in practical applications remains to be fully validated.
- **Future Work**: Explore the application of Linearizers in various neural network architectures., Investigate the potential for improved training algorithms based on this framework., Develop further theoretical insights into the properties of Linearizers.

</details>

## Gaussian Splatting

### [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](http://arxiv.org/pdf/2510.08566v1)
  (summary failed: 'utf-8' codec can't encode characters in position 8413-8414: surrogates not allowed)


### [ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation](http://arxiv.org/pdf/2510.08551v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D reconstruction and localization

### 2. Motivation & Gaps
- The paper presents ARTDECO, a framework that combines feed-forward priors and structured Gaussian representations for efficient and accurate 3D reconstruction.

- **Related work challenges:**
  - Per-scene optimization methods: Achieve high accuracy but are computationally expensive.
  - Feed-forward models: Enable real-time inference but struggle with accuracy and robustness.
  - MÃ¼ller et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Xu et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Sun et al., 2022: Accelerating training and rendering with hybrid or explicit scene representations.
  - Lin et al., 2021: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Fu et al., 2024: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - Lin et al., 2025b: Joint optimization of camera poses and scene parameters remains computationally intensive.
  - N/A: N/A
  - 3D Gaussian Splatting approaches: Maintaining high-quality rendering details in complex environments.
  - SLAM systems: Achieving accurate pose estimation in diverse conditions.
  - MASt3R-SLAM: Limited robustness under varying viewpoints and environmental conditions.
  - SLAM and SFM-based methods: Struggles with low-texture surfaces and repetitive structures leading to drift.
  - 3D foundation models: Dependence on training distribution limits generalization.
  - N/A: N/A
  - N/A: N/A
  - MASt3R: Predicts two-view correspondences and metric pointmaps with limited temporal context.
  - Ï€3: Not explicitly metric-aware, introducing challenges in maintaining scale and temporal consistency.
  - N/A: N/A
  - MonoGS: N/A
  - SEGS-SLAM: N/A
  - S3PO-GS: N/A
  - OnTheFly-NVS: N/A
  - LongSplat: N/A
  - Ours: N/A
  - N/A: N/A

### 3. Core Idea
- ARTDECO integrates feed-forward 3D foundation models with structured Gaussian representations to enhance the accuracy and efficiency of 3D reconstruction from monocular sequences.

### 4. Method
- **Pipeline**: The method involves a feed-forward model for pose estimation and a structured Gaussian representation for reconstruction.
- **Architecture / Loss / Training**: Utilizes a combination of PSNR, SSIM, and LPIPS metrics for evaluating reconstruction quality.
- **Complexity / Resources**: The method is designed to be efficient, achieving strong results with a runtime that is competitive with existing methods.

### 5. Experiments
- **Datasets & Metrics**: ScanNet, Waymo, VR-NeRF, KITTI, ScanNet++
- **Baselines**: 3D Gaussian Splatting, DPV-SLAM, DROID-SLAM, Feed-forward models, Go-SLAM, LongSplat, MASt3R, MASt3R-SLAM, MonoGS, N/A, NeRF-based SLAM methods, OnTheFly, OnTheFly-NVS, Per-scene optimization methods, S3PO-GS, SEGS-SLAM, SLAM, Ï€3
- **Main Results**: ARTDECO consistently achieves the highest PSNR and SSIM and the lowest LPIPS across all datasets.
- **Ablations**: Replacing the pairwise correspondence model MASt3R with the multi-frame visual-geometry model Ï€3.
- **Limitations / Stress Tests**: Identified limitations include sensitivity to noise, blur, and lighting changes, as well as challenges with low-texture surfaces.

### 6. Takeaways
- **Pros**: Combines efficiency and robustness in 3D reconstruction., Achieves high fidelity while reducing redundancy., Operates robustly across various environments.
- **Cons**: Generally achieve lower accuracy than per-scene optimized methods., Challenges with maintaining global consistency., Handling high-resolution inputs is difficult.
- **Future Work**: Explore further optimizations for large-scale environments., Investigate additional applications in AR/VR., Enhance the robustness of the system in challenging conditions.

</details>

## avatar

### [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](http://arxiv.org/pdf/2510.07685v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-time reasoning in interactive e-commerce applications

### 2. Motivation & Gaps
- The paper addresses the need for low-latency, high-performance reasoning models in interactive e-commerce settings.

- **Related work challenges:**
  - Retrieval-Augmented Generation (RAG) systems powered by Large Reasoning Models (LRMs): High inference delays despite strong reasoning capabilities.
  - Reinforcement Learning for LLMs: Alignment challenges in LLMs including hallucinations and instruction-following failures.
  - Efficient Reasoning: Inducing the 'overthinking' phenomenon in models leading to verbose reasoning paths.
  - RFT (Reinforcement Fine-Tuning): Inherits verbose reasoning from the teacher model, leading to inefficiencies.
  - DeepSeek-R1: High computational cost and lower efficiency in generating responses.
  - Qwen3-30B-A3B: Limited ability to optimize reasoning paths effectively.
  - Existing AI models for livestreaming: Lack of efficiency and quality in reasoning
  - Knowledge distillation: Standard distillation inherits verbose reasoning trajectories from the teacher model.
  - Reinforcement learning for language models: Balancing correctness and helpfulness while minimizing computational costs.
  - MoE architecture: Achieving a performance-to-cost ratio that outperforms traditional dense models.
  - Demystifying long chain-of-thought reasoning in llms: N/A
  - Efficient rl training for reasoning models via length-aware optimization: N/A
  - Large language models for information retrieval: A survey: N/A
  - N/A: N/A

### 3. Core Idea
- Introducing LiveThinking, a two-stage training framework that adapts large reasoning models to real-time applications by balancing quality and latency.

### 4. Method
- **Pipeline**: The approach leverages RFT to distill reasoning ability from a 670B teacher into a lightweight 30B student model, followed by GRPO to compress reasoning paths.
- **Architecture / Loss / Training**: Utilizes a multi-objective reward function to enhance correctness and helpfulness while compressing reasoning paths.
- **Complexity / Resources**: Achieves a 30-fold reduction in computational cost compared to the teacher model.

### 5. Experiments
- **Datasets & Metrics**: Evaluated on proprietary Tblive-E-Commerce QA dataset and the public MuSiQue dataset.
- **Baselines**: 14B dense production model, 670B teacher model, 670B teacher model (DeepSeek-R1), DeepSeek-R1, DeepSeek-R1-Distilled-Llama-8B, Dense models ranging from 4B to 32B parameters, N/A, Previous state-of-the-art models, Qwen3-235B-A22B, Qwen3-30B-A3B, Qwen3-32B, Teacher model (670B parameters)
- **Main Results**: LiveThinking outperforms its larger teacher model on correctness and helpfulness while being more computationally efficient.
- **Ablations**: Ablation studies confirm the effectiveness of RFT and RL in improving model performance.
- **Limitations / Stress Tests**: The model's performance is sensitive to the target reasoning length, indicating a trade-off between brevity and quality.

### 6. Takeaways
- **Pros**: Significantly reduces computational cost., Improves response correctness and helpfulness., Demonstrates effectiveness in high-traffic production environments.
- **Cons**: Initial model inherits verbose reasoning paths., Latency may still exceed industrial requirements without optimization.
- **Future Work**: Explore further optimizations for reasoning paths., Investigate additional applications in real-time systems., Enhance user engagement metrics through improved interaction.

</details>

### [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](http://arxiv.org/pdf/2510.05488v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Gaussian head avatar creation

### 2. Motivation & Gaps
- The paper addresses the need for real-time and continuous adjustment of the level of detail (LOD) in 3D Gaussian head avatars.

- **Related work challenges:**
  - 3D Gaussian Splatting (3DGS): Struggles to maintain real-time performance when rendering multiple avatars simultaneously.
  - Conventional LOD methods: Provide only a few discrete levels, leading to unsmooth visual effects when switching between levels.
  - UV-based strategies: Do not capture sufficient local information for detailed 3D head appearance.
  - 3D Morphable Models (3DMMs): Less effective at modeling non-rigid facial features like hair.
  - Neural radiance field (NeRF)-based methods: Computationally intensive and less accurate with geometry.
  - LoDAvatar: Only supports discrete LOD control and relies on synthetic multi-view images for training.
  - Previous methods using single UV feature maps: Struggled with maintaining local detail when resizing UV feature maps.
  - Multi-level UV feature fields: Implementing continuously controllable Level of Detail (LOD) without losing critical information.
  - GaussianAvatars: Limited detail preservation at varying LODs.
  - FlashAvatar: Inability to maintain quality at lower LODs.
  - RGBAvatar: Challenges in expressive animation.
  - FLAME tracking: Relies on accurate tracking for reliable 3D-2D alignment, essential for maintaining 3D consistency.
  - Existing methods: Some expression modes appear only under large head poses, leading to overfitting and artifacts.
  - N/A: N/A
  - The unreasonable effectiveness of deep features as a perceptual metric: N/A
  - Headgap: Few-shot 3d head avatar via generalizable gaussian priors: N/A
  - Pointavatar: Deformable point-based head avatars from videos: N/A
  - Instant volumetric head avatars: N/A

### 3. Core Idea
- ArchitectHead is the first 3D Gaussian head method to realize continuous LOD control by parameterizing Gaussians in UV feature space.

### 4. Method
- **Pipeline**: A neural decoder generates Gaussian attributes using the LOD as an additional condition.
- **Architecture / Loss / Training**: Introduces a learnable UV latent feature map alongside the UV position map for better representation.
- **Complexity / Resources**: Extends design to a multi-level latent feature field for weighted resampling across resolutions.

### 5. Experiments
- **Datasets & Metrics**: Experiments conducted on monocular video datasets.
- **Baselines**: 3D Morphable Models (3DMMs), Conventional LOD methods, Existing 3D avatar methods, Existing 3DGS-based avatars, FlashAvatar, Gaussian Dejavu, GaussianAvatars, LoDAvatar, N/A, Neural radiance field (NeRF)-based methods, RGBAvatar
- **Main Results**: ArchitectHead achieves state-of-the-art quality in 3D avatar generation.
- **Ablations**: Ablation studies show performance variations with different feature map resolutions.
- **Limitations / Stress Tests**: Method relies on accurate FLAME tracking and may overfit rare expression modes.

### 6. Takeaways
- **Pros**: Supports continuous LOD control for better rendering efficiency., Achieves state-of-the-art quality in 3D head avatar rendering., Allows dynamic adjustment of detail without retraining.
- **Cons**: Requires a significant number of Gaussian points for high-quality rendering., Moderate quality degradation at lower LODs., Challenges in capturing detailed 3D head appearance.
- **Future Work**: Explore further optimizations for real-time performance., Investigate additional applications of continuous LOD in other domains., Enhance the UV feature field to capture more local information.

</details>

### [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](http://arxiv.org/pdf/2510.04822v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D Virtual Try-On

### 2. Motivation & Gaps
- The paper addresses the challenges of achieving realistic 4D virtual try-on (VTON) using single in-shop garment references, focusing on dynamic pose control and multi-view rendering.

- **Related work challenges:**
  - Image-based VTON models: Lack intrinsic 3D perceptual understanding, producing discontinuous try-on results across changing viewpoints and poses.
  - Animatable avatar-based garment transfer approaches: Depend on large-scale datasets, limiting scalability and practical use.
  - 3D VTON methods: Do not support dynamic manipulation.
  - VITON: Relying primarily on 2D data, struggles with consistent results across varying viewpoints and poses.
  - ViViD: Requires continuous video input, increasing computational and memory demands.
  - GaussianEditor: Lacks precision in controlling detailed textures.
  - Video-based VTON methods: Lack of 3D structural awareness and high computational costs.
  - Image-based VTON approaches: Inability to effectively handle temporal coherence across poses and viewpoints.
  - ViViD: Lacks explicit 3D structural reasoning, resulting in texture flickering and deformation errors.
  - IDM-VTON: Limited input sequence length degrades temporal continuity in long sequences.
  - GaussianEditor: Requires per-frame optimization and suffers from severe temporal inconsistency.
  - ViViD: Lacks genuine 3D spatial reasoning and incurs high computational overhead.
  - IDM-VTON + AG: Exhibits noticeable temporal flickering and inconsistent texture patterns.
  - N/A: N/A

### 3. Core Idea
- The proposed AvatarVTON framework utilizes a Reciprocal Flow Rectifier for optical flow correction and a Non-Linear Deformer for adaptive garment deformations, enhancing rendering quality and stability.

### 4. Method
- **Pipeline**: The framework processes single garment images to generate dynamic try-on results across various poses and viewpoints.
- **Architecture / Loss / Training**: Incorporates adversarial loss to improve texture clarity and color accuracy.
- **Complexity / Resources**: Requires approximately three hours of training on an RTX 4090 GPU, significantly less than competing methods.

### 5. Experiments
- **Datasets & Metrics**: Utilizes datasets from AvatarReX, ActorsHQ, DressCode, and VITON-HD, evaluating garment texture fidelity, identity preservation, temporal coherence, and overall realism.
- **Baselines**: Animatable Gaussians (3DGS-based counterpart), Diffusion models, GaussianEditor, GaussianEditor (3D editing method), GaussianVTON, IDM-VTON, IDM-VTON (2D image-based VTON), IDM-VTON + AG, LHM (4D approach), N/A, SCARF (NeRF-based animatable human reconstruction), VITON, ViViD, ViViD (2D video-based VTON)
- **Main Results**: AvatarVTON consistently outperforms competitors in all evaluation dimensions, achieving higher scores in user studies.
- **Ablations**: Ablation studies demonstrate the importance of the Reciprocal Flow Rectifier and adversarial loss in maintaining visual fidelity.
- **Limitations / Stress Tests**: The framework inherits out-of-distribution constraints from prior models, leading to potential artifacts in unseen viewâ€“pose combinations.

### 6. Takeaways
- **Pros**: High-fidelity 4D virtual try-on from a single garment image., Mitigates view-pose coupling inconsistencies., Facilitates adaptive garment deformation transfer.
- **Cons**: Dependence on single 2D garment images may limit realism., Potential challenges in capturing realistic garment dynamics.
- **Future Work**: Explore further enhancements in garment dynamics., Investigate scalability with larger datasets., Develop additional modules for improved qualitative analysis.

</details>

## video understanding

### [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](http://arxiv.org/pdf/2510.08568v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Zero-Shot Manipulation

### 2. Motivation & Gaps
- The paper addresses the need for versatile manipulation techniques that can handle various object types and manipulation scenarios.

- **Related work challenges:**
  - Vision-Language-Action (VLA) models: Require vast quantities of robot-specific vision-language-action data that is difficult and expensive to collect.
  - Modular systems for task understanding and robot control: Translating high-level understanding into physical actions remains an open problem.
  - Prior work on self-collected data for training video models: Reintroduces the data bottleneck and limits generalizability and scalability.
  - Concurrent work on 6D pose extraction for demonstration-free manipulation: Relies on rigid-body assumptions, limiting applicability to a broader class of objects.
  - Flow-based manipulation methods: Require robot data or task-specific training, hindering generalization.
  - Prior work on object manipulation: Limited ability to handle complex object dynamics and real-time execution.
  - AVDC: Struggles with precise, long-horizon placements due to lack of 3D awareness.
  - VidBot: Fails when tasks require object-object relations and precise relative pose placement.
  - Diffusion Policy (DP): Shows poor generalization from a few examples due to random sampling in evaluations.
  - Veo: Closed-source model that produces video clips but lacks robustness in certain tasks.
  - Wan2.1: Open-source model that significantly drops in performance when a goal image is omitted.
  - Open-World Object Manipulation using Pre-Trained Vision-Language Models: Limited adaptability to new environments without retraining.
  - Local Policies Enable Zero-shot Long-horizon Manipulation: Difficulty in generalizing across diverse tasks.
  - Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations: Dependence on high-quality video data for training.
  - Wan2.1: Limited support for all required modes in newer versions.
  - Veo: Lack of goal-image conditioning in the model during experiments.
  - MegaSaM: Ambiguity in estimated depth maps even after postprocessing.
  - Grounded-SAM2: Need for accurate object grounding in video sequences.
  - Trajectory Optimization: Finding optimal, collision-free, and smooth sequences of joint configurations.
  - Previous manipulation methods: Limited to specific object types and require extensive training data.

### 3. Core Idea
- NovaFlow enables zero-shot manipulation by generating actionable flows from videos, allowing manipulation of diverse objects without prior training.

### 4. Method
- **Pipeline**: The method involves generating actionable flows from videos and applying them to manipulate objects in real-time.
- **Architecture / Loss / Training**: Utilizes a combination of smoothness weight, collision penalty weight, and regularization for training.
- **Complexity / Resources**: The method is designed to be viewpoint-agnostic and can be deployed on various platforms after hand-eye calibration.

### 5. Experiments
- **Datasets & Metrics**: Real-world manipulation experiments were conducted to evaluate the effectiveness of NovaFlow.
- **Baselines**: AVDC, Data-dependent methods, Diffusion Policy (DP), Existing video generation models, Existing video-based manipulation methods, Existing video-based manipulation techniques, Existing zero-shot techniques, Gemini 2.5 Pro, Grounded-SAM2, Inverse Dynamics Model (IDM), MegaSaM, Object-centric approaches, Previous demonstration-free methods, Recent zero-shot learning approaches, Traditional manipulation methods, Traditional robotic manipulation methods, Traditional robotic manipulation techniques, Veo, VidBot, Wan2.1
- **Main Results**: NovaFlow successfully manipulated rigid, deformable, and articulated objects across different scenarios.
- **Ablations**: Ablation studies demonstrate the importance of video quality and flow extraction techniques in achieving high performance.
- **Limitations / Stress Tests**: The method's performance may vary based on the complexity of the manipulation task and the quality of the generated video.

### 6. Takeaways
- **Pros**: No need for task-specific tuning., Generalizable across different embodiments., State-of-the-art zero-shot performance on real-world tasks.
- **Cons**: Still relies on the quality of generated videos., May introduce generative artifacts and implausible motions., Calibration of depth estimation can be challenging.
- **Future Work**: Explore further applications in unstructured environments., Investigate improvements in the flow generation process., Enhance the framework's adaptability to more complex tasks.

</details>

### [Where Have All the Kaczmarz Iterates Gone?](http://arxiv.org/pdf/2510.08563v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Numerical analysis and validation of theoretical bounds in noisy linear systems

### 2. Motivation & Gaps
- The paper addresses the challenges of solving doubly-noisy linear systems using the Randomized Kaczmarz algorithm, focusing on theoretical bounds and their empirical validation.

- **Related work challenges:**
  - Strohmer and Vershynin (2009): Demonstrated that RK converges linearly in expectation for consistent systems but did not address inconsistent systems.
  - Needell (2014): Showed that RK iterates approach a ball centering at the least squares solution for consistent systems, but did not consider noise in both the coefficient matrix and the right-hand side.
  - Bergou et al. (2021): Provided convergence analysis for RK in general noisy cases, but the understanding of RK iterates in inconsistent systems remains limited.
  - Needell [15]: All works on noisy systems have used the LS solution of the underlying consistent noiseless system as a reference point.
  - Theorems 1.2 and 1.3: Require specific initial points which may not be satisfied in practice.
  - N/A: N/A
  - Theorems 3 and 5: Establishing bounds for the convergence of RK iterates under different assumptions.
  - Corollary 2: Comparing error estimates between different convergence bounds.
  - LIBSVM: A library for support vector machines: Handling noise in real-world data and ensuring convergence of algorithms.
  - Efficient and robust solution strategies for saddle-point systems: Developing methods that are both efficient and robust in the presence of noise.
  - Numerical approximation of some linear stochastic partial differential equations driven by special additive noises: N/A
  - Online and batch supervised background estimation via l1 regression: N/A
  - New variants of the POCS method using affine subspaces of finite codimension with applications to irregular sampling: N/A

### 3. Core Idea
- The paper presents theoretical bounds for the convergence of the Randomized Kaczmarz algorithm in noisy linear systems and validates these bounds through numerical experiments.

### 4. Method
- **Pipeline**: The method involves generating noisy linear systems, applying the Randomized Kaczmarz algorithm, and comparing theoretical bounds with empirical results.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The experiments utilize synthetic data and real-world datasets from LIBSVM, with computational resources for running multiple iterations.

### 5. Experiments
- **Datasets & Metrics**: The experiments use synthetic data and real-world data from the LIBSVM dataset, measuring approximation errors and convergence rates.
- **Baselines**: Corollary 2, Cyclic Kaczmarz algorithm, N/A, Previous works on Kaczmarz iterates, Randomized Kaczmarz algorithm, Theorem 5, Theoretical bounds from Theorem 5 and Corollary 2
- **Main Results**: The results show that the bounds are valid and that Corollary 2 exhibits a faster convergence rate than Theorem 5.
- **Ablations**: N/A
- **Limitations / Stress Tests**: The experiments do not require the starting point to be in the row space of the noisy matrix, which may limit the generalizability of the results.

### 6. Takeaways
- **Pros**: Provides a deeper understanding of the Kaczmarz algorithm in noisy environments., Offers practical insights into the algorithmâ€™s performance under realistic conditions., Establishes bounds on convergence that can inform future applications.
- **Cons**: Limited understanding of the behavior of Kaczmarz iterates in inconsistent systems., Theoretical results may not fully capture practical performance in all scenarios., Dependence on noise levels may complicate the application of results.
- **Future Work**: Further research on optimizing the Kaczmarz algorithm for inconsistent systems., Exploration of alternative iterative methods for noisy linear systems., Development of more robust convergence guarantees in the presence of noise.

</details>

### [MultiCOIN: Multi-Modal COntrollable Video INbetweening](http://arxiv.org/pdf/2510.08561v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Generation

### 2. Motivation & Gaps
- The paper addresses the challenges in high-dynamic video generation.

- **Related work challenges:**
  - Framer: Focused on respecting motion trajectories but lacks versatility and fine-grained user controls.
  - Framer: Achieving impressive results in controllable inbetweening using motion trajectories.
  - Stable Video Diffusion: Generating temporally coherent content using latent diffusion models.
  - Tune-a-video: Facilitating few-shot video generation through fine-tuning pre-trained image diffusion models.
  - Stable Video Diffusion (SVD): Maintaining temporal consistency across frames.
  - Diffusion Transformer (DiT): Modeling long-range dependencies and global context for fine details.
  - Framer: Relies solely on trajectory control, limiting flexibility in content editing.
  - Framer: Motion is introduced as an external condition, which may not effectively integrate with video features.
  - N/A: N/A
  - Make pixels dance: High-dynamic video generation: High-dynamic video generation techniques are still evolving.
  - The unreasonable effectiveness of deep features as a perceptual metric: Existing metrics may not fully capture perceptual quality.
  - Audio-driven neural gesture reenactment with video motion graphs: Integrating audio and video for realistic gesture reenactment remains complex.

### 3. Core Idea
- The core idea is to utilize a masked generative transformer architecture for generating high-quality videos.

### 4. Method
- **Pipeline**: The method involves a generative pipeline that leverages masked video data.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for video generation tasks.
- **Complexity / Resources**: The method requires significant computational resources for training.

### 5. Experiments
- **Datasets & Metrics**: The experiments utilize various video datasets and standard metrics for evaluation.
- **Baselines**: Diffusion Transformer, Existing video generation models, Framer, N/A, Stable Video Diffusion, Traditional video synthesis techniques, Tune-a-video
- **Main Results**: The results demonstrate superior performance in generating high-dynamic videos compared to baselines.
- **Ablations**: Ablation studies indicate the importance of the masked approach in improving video quality.
- **Limitations / Stress Tests**: Limitations include the need for extensive training data and computational power.

### 6. Takeaways
- **Pros**: Allows for versatile and fine-grained user controls., Achieves high-quality and fine-grained video interpolation., Empowers users to create smooth and plausible transitions.
- **Cons**: Requires advanced computational resources., Complexity in ensuring compatibility between controls and the generative model.
- **Future Work**: Explore further enhancements in user control mechanisms., Investigate additional modalities for video inbetweening.

</details>
