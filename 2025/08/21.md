# Daily Paper Digest Â· 2025-08-21
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 1 papers per query).

## neural rendering

### [Lorentz-Equivariance without Limitations](http://arxiv.org/pdf/2508.14898v1)


<!--break-out-of-list-->
<details>
<summary>ðŸ“„ Paper Summary (click to expand)</summary>

### 1. Task / Problem
- Jet tagging

### 2. Motivation & Gaps
- The study investigates the performance of various neural classifiers in distinguishing between different event generators in particle physics.

- **Related work challenges:**
  - Several Lorentz-equivariant architectures: Performance is often task-specific and not easily transferable.
  - Equivariant approaches in amplitude surrogates and event generation: Unclear performance comparison with networks that are only equivariant under a subgroup of the Lorentz group.
  - LorentzNet: Relies on specialized layers
  - PELICAN: Relies on specialized layers
  - L-GATr: Relies on specialized layers
  - Previous Lorentz-equivariant architectures: Difficulty in achieving optimal performance due to symmetry breaking.
  - MadGraph: Generating sufficient training events while maintaining accuracy.
  - MLP-I: Struggles with permutation symmetry and high multiplicity.
  - L-GATr: Higher computational cost compared to LLoCa.
  - GNN: Limited performance without Lorentz-equivariance.
  - Ref. [26]: The choice of target trajectory is crucial for the performance of CFM phase space generators.
  - Ref. [35]: Previous studies found that permutation-equivariant architectures significantly outperform simple MLPs.
  - N/A: Learning the full phase space density to a precision that renders the network uncertainty negligible.
  - Lorentz-equivariant networks: Struggle to learn mass distributions of virtual particles.
  - E(3)-GATr: Performance drop due to unused translation representations.
  - Standard transformers: Fail to carry performance gains to larger datasets.
  - Ref. [35]: Extends results by including top tagging and additional datasets.
  - ParticleNet: Limited performance due to non-equivariance.
  - ParT: Requires careful hyperparameter tuning due to small training datasets.
  - L-GATr: Computational cost and applicability to specialized architectures.
  - N/A: N/A
  - ParT: Reproducing results with modified architectures.
  - ParticleNet: Achieving better results with ParT training setup.
  - LLoCa-Transformer: Dealing with limited statistics in top tagging.
  - L-GATr: Cannot distinguish samples from L-GATr and LLoCa-Transformer with sufficient training data.
  - SO(2)- and SO(3)-Transformer: Similar performance issues in distinguishing events when sufficient training data is used.
  - N/A: N/A
  - N/A: N/A

### 3. Core Idea
- The paper explores the effectiveness of different transformer architectures in jet tagging tasks, particularly focusing on their ability to distinguish between generated events and ground truth.

### 4. Method
- **Pipeline**: Training multiple transformer architectures with specific hyperparameters and evaluating their performance on jet tagging tasks.
- **Architecture / Loss / Training**: Using Adam optimizer with specific learning rates and dropout rates for training various models.
- **Complexity / Resources**: Evaluating timings, memory consumption, and FLOPs for different architectures.

### 5. Experiments
- **Datasets & Metrics**: JetClass dataset with background rejection rates at fixed signal efficiency.
- **Baselines**: DA-GNN, Established high-performance taggers, GNN, L-GATr, LLoCa-ParT, LLoCa-ParticleNet, LLoCa-Transformer, LorentzNet, MIParT, MIParT-L, MLP-I, Message passing graph network (GNN), N/A, Non-equivariant neural networks, P-CNN, PFN, ParT, ParticleNet, Standard Transformer, Standard transformer, Transformer
- **Main Results**: The classifier AUC shows that the neural classifier can distinguish events generated by the transformer and DA-Transformer from the ground truth.
- **Ablations**: Performance improvements observed with pre-training on the JetClass dataset and fine-tuning.
- **Limitations / Stress Tests**: In small-data regimes, differences between networks are overshadowed by uncertainties from the training process.

### 6. Takeaways
- **Pros**: Achieves exact Lorentz-equivariance with minimal computational overhead., Flexible implementation applicable to various neural network architectures., Allows for direct comparison of performance between different equivariant networks.
- **Cons**: Task-specific performance may still limit generalizability., Symmetry breaking introduces complexity in performance assessment., Dependence on the quality of the local reference frame construction.
- **Future Work**: Further exploration of LLoCa in other ML applications., Investigation of additional symmetry breaking scenarios., Development of more generalized equivariant architectures.

</details>
