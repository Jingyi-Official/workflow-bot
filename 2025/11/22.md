# Daily Paper Digest Â· 2025-11-22
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/pdf/2511.16673v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cross-domain evaluation on XHuman, novel view and pose synthesis

### 2. Motivation & Gaps
- The study addresses the challenges of synthesizing multiview images of avatars using diffusion models, highlighting issues with multiview consistency and the impact of training data inconsistency on perceptual evaluation metrics.

### 3. Core Idea
- Our model does not rely on poses for reconstruction at inference time, unlike pose-dependent methods.

### 4. Method
- **Pipeline**: The method involves using estimated poses from MultiHMR and optimizing both camera and human poses during test time for better evaluation.
- **Architecture / Loss / Training**: The model employs multiple loss functions including LBS, photometric, and projection losses during training.
- **Complexity / Resources**: Training requires multiple NVIDIA GPUs and significant memory resources, with a total training time of approximately 12 days.

</details>

### [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/pdf/2511.16662v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D avatar generation

### 2. Motivation & Gaps
- Existing 4D generation techniques face limitations in temporal consistency, motion accuracy, visual fidelity, and computational efficiency.

### 3. Core Idea
- Combining a specialized diffusion model with triplane representation and skeleton-guided conditioning to achieve precise skeleton-based pose control while reducing generation time.

### 4. Method
- **Pipeline**: Two-stage training approach with initial static avatar generation followed by reposing model training.
- **Architecture / Loss / Training**: Trained on 4 NVIDIA H100 GPUs with a progressive reconstruction strategy.
- **Complexity / Resources**: Utilizes 1,500 unique characters and 3,000 initial configurations for training.

</details>

### [B2F: End-to-End Body-to-Face Motion Generation with Style Reference](https://arxiv.org/pdf/2511.13988v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Motion Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic facial expressions using a model that does not rely on emotion labels.

### 3. Core Idea
- The B2F model generates facial motion by transforming FLAME parameters into ARKit blendshape weights using a parameter-blended Mixture of Experts architecture.

### 4. Method
- **Pipeline**: FLAME-to-ARKit conversion using a gating network and expert networks.
- **Architecture / Loss / Training**: The model employs a loss function combining reconstruction, alignment, KL divergence, consistency, and cross terms, with dynamic scheduling for the KL term.
- **Complexity / Resources**: Training was conducted on a desktop system with an AMD Ryzen 7 7800X3D CPU and an NVIDIA GeForce RTX 4070 GPU.

</details>

## video understanding

### [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/pdf/2511.16669v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Answer Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating video answers that are semantically and visually consistent with human preferences.

### 3. Core Idea
- The proposed V ANS model, enhanced by Joint-GRPO, effectively aligns textual and visual outputs, enabling multi-future predictions and robust reasoning in image-to-video generation.

### 4. Method
- **Pipeline**: The model processes input videos and generates corresponding video answers based on textual prompts.
- **Architecture / Loss / Training**: Utilizes Joint-GRPO for training, focusing on enhancing visual consistency and semantic correctness.
- **Complexity / Resources**: Requires a balanced optimization towards captions that are semantically accurate and visually plausible.

</details>

### [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/pdf/2511.16668v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Rule Following and Block Sliding Evaluation

### 2. Motivation & Gaps
- The paper discusses the evaluation of predictions made by models in tasks involving visual reasoning and physical dynamics.

### 3. Core Idea
- The evaluation of model predictions in visual reasoning tasks requires strict adherence to transformation rules and physical dynamics.

### 4. Method
- **Pipeline**: Evaluation of model predictions against ground-truth images based on specific criteria.
- **Architecture / Loss / Training**: Evaluated six generative video models with varying architectures and training pipelines.
- **Complexity / Resources**: Utilizes a standardized protocol for evaluation, ensuring consistency across models.

</details>

### [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/pdf/2511.16661v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Learning manipulation skills from human demonstrations

### 2. Motivation & Gaps
- The framework aims to improve human-to-robot transfer of manipulation skills using point clouds instead of images.

### 3. Core Idea
- AINA uses object-centric point clouds to enhance robustness and reduce visual disparity in learning from human data.

### 4. Method
- **Pipeline**: Collect in-the-wild human demonstrations and use point clouds for training policies.
- **Architecture / Loss / Training**: Utilizes transformer-based imitation learning architecture with point clouds as input.
- **Complexity / Resources**: Requires Aria Gen 2 glasses for data collection and Realsense cameras for deployment.

</details>

## model collapse

### [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/pdf/2511.16674v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dataset condensation and distillation

### 2. Motivation & Gaps
- The paper addresses the challenges of dataset distillation, aiming to reduce the training time and energy consumption by synthesizing datasets that can replace large real datasets.

### 3. Core Idea
- The core idea is to synthesize datasets that can effectively replace large real datasets, allowing for faster training and reduced resource consumption.

### 4. Method
- **Pipeline**: The method involves distilling datasets through a pyramid representation and optimizing them using a linear classifier.
- **Architecture / Loss / Training**: Utilizes Adam optimizer with a learning rate of 0.002 and distills for 5000 iterations.
- **Complexity / Resources**: The method requires a variety of GPUs, including H200, A100, and others, with specific time requirements for distillation.

</details>

### [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/pdf/2511.16672v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning

### 2. Motivation & Gaps
- The paper addresses the need for advanced multimodal reasoning capabilities in AI models.

### 3. Core Idea
- The core idea is to enhance multimodal reasoning through a chain-of-thought approach.

### 4. Method
- **Pipeline**: The method involves a structured pipeline for multimodal reasoning tasks.
- **Architecture / Loss / Training**: Utilizes a novel architecture with specific loss functions for training.
- **Complexity / Resources**: The model requires significant computational resources for training and inference.

</details>

### [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/pdf/2511.16670v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Question Answering (VQA)

### 2. Motivation & Gaps
- The paper addresses the challenges in visual question answering by utilizing geometric reasoning and logical reasoning.

### 3. Core Idea
- DualMindVLM integrates long thinking processes to enhance reasoning capabilities in visual question answering tasks.

### 4. Method
- **Pipeline**: The method involves a series of logical deductions based on visual inputs and geometric properties.
- **Architecture / Loss / Training**: Utilizes a dual-process architecture to balance between fast and slow reasoning.
- **Complexity / Resources**: Requires significant computational resources for training and inference.

</details>
