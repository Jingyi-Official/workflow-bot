# Daily Paper Digest Â· 2025-11-17
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Dynamic Avatar-Scene Rendering from Human-centric Context](https://arxiv.org/pdf/2511.10539v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Scene Reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of effective information exchange between separately modeled human avatars and background scenes in dynamic scene reconstruction.

### 3. Core Idea
- A shared information mapping mechanism that projects separately designed components into a unified representation space for coherent integration.

### 4. Method
- **Pipeline**: Utilizes lightweight residual MLPs to model Gaussian attributes for scene reconstruction.
- **Architecture / Loss / Training**: Utilizes shared-weight mapping architecture to enhance coherence in integration.
- **Complexity / Resources**: Maintains computational efficiency without exhaustive pairwise interactions.

</details>

### [AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting](https://arxiv.org/pdf/2511.09827v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Real-time human novel view synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of synthesizing novel views of humans in real-time using a generalizable approach.

### 3. Core Idea
- The core idea is to utilize pixel-wise 3D Gaussian splatting to achieve generalizable and efficient human view synthesis.

### 4. Method
- **Pipeline**: The method involves a pipeline that integrates 3D Gaussian representations with real-time rendering techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function tailored for optimizing the quality of synthesized views during training.
- **Complexity / Resources**: The method is designed to be computationally efficient, allowing for real-time applications with moderate resource requirements.

</details>

### [Deep Inverse Shading: Consistent Albedo and Surface Detail Recovery via Generative Refinement](https://arxiv.org/pdf/2511.08079v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- De-shading and rendering of human avatars

### 2. Motivation & Gaps
- Advanced fitting algorithms offer promising enhancements for finger reconstruction.

### 3. Core Idea
- The normal enhancement network, conditioned on RGB inputs, effectively detects depth boundaries and corrects associated errors during optimization.

### 4. Method
- **Pipeline**: Our training pipeline ensures depth continuity.
- **Architecture / Loss / Training**: Utilizes Structural Similarity Index (SSIM) loss to ensure structural similarity during training.
- **Complexity / Resources**: Non-connected triangles occur only within individual rasterization cycles to assist geometry optimization but are excluded from the final reconstructed model.

</details>

## video understanding

### [Building far-from-equilibrium effective field theories using shift symmetries](https://arxiv.org/pdf/2511.11555v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the implications of shift symmetries in kinetic theory and their effects on excitations.

### 2. Motivation & Gaps
- Our work is motivated by the apparent importance of nonhydrodynamic modes in characterizing the thermalization in various physical systems.

### 3. Core Idea
- Diverse nonhydrodynamic excitations can be unified in a local EFT framework by generalizing the concept of shift symmetry.

### 4. Method
- **Pipeline**: The method involves coupling the kinetic action with the holographic action while preserving gauge invariance and shift symmetries.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The model requires understanding of advanced concepts in field theory and kinetic theory.

</details>

### [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/pdf/2511.11552v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Long Visual Document Understanding

### 2. Motivation & Gaps
- The paper addresses the challenges in understanding long visual documents using a multi-agent framework.

### 3. Core Idea
- The core idea is to utilize a multi-agent framework that enhances the understanding of long visual documents through tool augmentation.

### 4. Method
- **Pipeline**: The method involves a multi-agent system where different agents handle specific tasks related to visual document understanding.
- **Architecture / Loss / Training**: Utilizes a combination of loss functions tailored for document understanding tasks.
- **Complexity / Resources**: The Page Navigator benefits from test-time scaling, while the Answer Sampler shows diminishing returns beyond a small sample size.

</details>

### [Accurate models for recoil velocity distribution in black hole mergers with comparable to extreme mass-ratios and their astrophysical implications](https://arxiv.org/pdf/2511.11536v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Analyzing population properties of compact objects using gravitational wave data

### 2. Motivation & Gaps
- The study aims to understand the population of merging compact binaries and their implications for gravitational wave detections.

### 3. Core Idea
- Developing a data-driven model to predict kick velocities of merging black hole binaries.

### 4. Method
- **Pipeline**: The model uses a combination of numerical relativity simulations and analytic expressions to predict kick velocities.
- **Architecture / Loss / Training**: The model is trained on BAM and SXS numerical relativity simulations.
- **Complexity / Resources**: The model complexity is moderate, requiring computational resources for simulation data processing.

</details>

## model collapse

### [Optimizing Mixture of Block Attention](https://arxiv.org/pdf/2511.11571v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Attention Mechanism Optimization

### 2. Motivation & Gaps
- Sparse patterns are challenging to implement efficiently due to irregular memory access.

### 3. Core Idea
- The paper presents a two-level blocking strategy for optimizing the performance of attention mechanisms in neural networks, particularly focusing on FlashAttention-2.

### 4. Method
- **Pipeline**: The method involves dividing matrices into blocks, processing them in parallel, and utilizing a two-level blocking strategy to enhance performance.
- **Architecture / Loss / Training**: The backward pass adapts the memory-efficient design of FlashAttention-2 using a sequence of fused kernels.
- **Complexity / Resources**: The method optimizes memory usage and computational efficiency by caching dense physical blocks in SRAM.

</details>

### [Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems](https://arxiv.org/pdf/2511.11567v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multi-agent trajectory prediction and collision avoidance

### 2. Motivation & Gaps
- The paper addresses the challenges of ensuring safety and efficiency in multi-agent systems through iterative conformal prediction (CP) frameworks.

### 3. Core Idea
- The proposed iterative CP framework recalibrates prediction regions iteratively to achieve desired coverage guarantees while adapting to distribution shifts in multi-agent interactions.

### 4. Method
- **Pipeline**: The framework involves training a trajectory predictor, applying model predictive control (MPC), and iteratively updating predictions until convergence.
- **Architecture / Loss / Training**: Utilizes a long short-term memory (LSTM) model for trajectory predictions, trained on historical agent states.
- **Complexity / Resources**: The method requires a calibration dataset and involves multiple iterations for convergence, with specific parameters for smoothing and stopping criteria.

</details>

### [The Maximal Variance of Unilaterally Truncated Gaussian and Chi Distributions](https://arxiv.org/pdf/2511.11566v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Modeling and analysis of income data using statistical methods

### 2. Motivation & Gaps
- The study aims to improve the accuracy of variance and mean estimates for log-transformed income data.

### 3. Core Idea
- To model the variance and mean of log-transformed income data and original data using statistical methods.

### 4. Method
- **Pipeline**: Utilization of point-slope and two-point moment-intersecting methods for variance and mean estimation.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: The analysis involves mathematical derivations and proofs, requiring a solid understanding of probability theory.

</details>
