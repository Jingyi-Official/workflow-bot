# Daily Paper Digest Â· 2025-11-01
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications](http://arxiv.org/pdf/2510.26251v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigating the impact of avatar characteristics on user ratings in healthcare applications

### 2. Motivation & Gaps
- Virtual avatars are becoming increasingly popular in healthcare applications, influencing user experience and preferences.

### 3. Core Idea
- The perception of virtual avatars varies significantly among users, with competence and information sharing being more influential than warmth and attractiveness.

### 4. Method
- **Pipeline**: Participants interacted with six different avatars in a healthcare questionnaire application, rating their experiences.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

</details>

### [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](http://arxiv.org/pdf/2510.25234v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D facial animation

### 2. Motivation & Gaps
- The paper addresses the need for improved techniques in 3D facial animation driven by speech and emotional expressions.

### 3. Core Idea
- The core idea is to develop a method that disentangles speech and emotional expressions to create more realistic 3D facial animations.

### 4. Method
- **Pipeline**: The method involves a pipeline that processes audio input to generate corresponding facial animations.
- **Architecture / Loss / Training**: Utilizes a novel architecture with specific loss functions to enhance training efficiency.
- **Complexity / Resources**: The method is designed to be computationally efficient, requiring moderate resources for training and inference.

</details>

### [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](http://arxiv.org/pdf/2510.23929v1)
  (summary failed: 'utf-8' codec can't encode characters in position 7033-7034: surrogates not allowed)


## video understanding

### [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](http://arxiv.org/pdf/2510.26802v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video reasoning enhancement

### 2. Motivation & Gaps
- The paper addresses the need for improved reasoning capabilities in multi-modal large language models (MLLMs) specifically for video content.

### 3. Core Idea
- The core idea is to enhance the reasoning capabilities of MLLMs in video analysis through reinforcement learning techniques.

### 4. Method
- **Pipeline**: The method involves a reinforcement learning framework that integrates video reasoning tasks into the training process of MLLMs.
- **Architecture / Loss / Training**: Utilizes a custom loss function designed to optimize reasoning accuracy in video contexts.
- **Complexity / Resources**: The approach requires significant computational resources due to the complexity of video data processing.

</details>

### [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](http://arxiv.org/pdf/2510.26796v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of synthesizing novel views of dynamic scenes using a monocular camera, focusing on achieving globally coherent depth estimation.

### 3. Core Idea
- The proposed method utilizes a monocular camera to synthesize novel views by leveraging globally coherent depth information.

### 4. Method
- **Pipeline**: The method involves capturing video data, estimating depth, and synthesizing new views based on the depth information.
- **Architecture / Loss / Training**: The architecture employs a loss function that encourages depth coherence across frames during training.
- **Complexity / Resources**: The method requires moderate computational resources, primarily for depth estimation and view synthesis.

</details>

### [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](http://arxiv.org/pdf/2510.26794v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Motion Generation from Text Prompts

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic motion sequences from textual descriptions, highlighting the limitations of existing models in handling complex movements.

### 3. Core Idea
- ViMoGen introduces an adaptive branch selection mechanism that intelligently chooses between Motion-to-Motion (M2M) and Text-to-Motion (T2M) branches based on the quality of the initial motion extracted from generated videos.

### 4. Method
- **Pipeline**: The model processes text prompts to generate motion sequences, utilizing an adaptive gate to select the appropriate generation branch.
- **Architecture / Loss / Training**: The architecture employs a dual-branch system with specific loss functions tailored for M2M and T2M outputs.
- **Complexity / Resources**: The model requires significant computational resources for training, including high-performance GPUs and extensive datasets.

</details>

## model collapse

### [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](http://arxiv.org/pdf/2510.26800v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Guided Panorama Perception and Panorama Completion

### 2. Motivation & Gaps
- The paper addresses the challenges in generating accurate and coherent 3D scenes from panoramic images.

### 3. Core Idea
- OmniX generates accurate and locally coherent results for masked areas in panoramic images while ensuring consistency with RGB references.

### 4. Method
- **Pipeline**: Input masked images and corresponding masks to generate RGB outputs and other attributes.
- **Architecture / Loss / Training**: The architecture employs joint modeling of materials and geometry using cross-attention mechanisms to enhance performance.
- **Complexity / Resources**: The method requires training on diverse datasets to ensure generalization across in-domain and out-domain scenarios.

</details>

### [Masked Diffusion Captioning for Visual Feature Learning](http://arxiv.org/pdf/2510.26799v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Captioning

### 2. Motivation & Gaps
- The paper addresses the need for improved visual pretraining methods that incorporate location awareness in caption generation.

### 3. Core Idea
- The introduction of location-aware captioners that enhance the quality of generated captions by considering spatial information.

### 4. Method
- **Pipeline**: The method involves a pretraining phase using a large dataset followed by fine-tuning on specific tasks.
- **Architecture / Loss / Training**: Utilizes a combination of sigmoid loss and standard cross-entropy loss during training.
- **Complexity / Resources**: The model complexity is managed through hyperparameter tuning and resource allocation for training.

</details>
