# Daily Paper Digest Â· 2025-11-18
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/pdf/2511.12935v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D avatar reconstruction

### 2. Motivation & Gaps
- Existing methods struggle with generating high-quality 3D avatars from real-world images, particularly in terms of fidelity and detail preservation.

### 3. Core Idea
- PFAvatar introduces a novel method for reconstructing high-quality 3D avatars by fine-tuning a Pose-Aware Diffusion Model and distilling a 3D NeRF-based avatar, achieving consistent results while mitigating language drift.

### 4. Method
- **Pipeline**: Fine-tuning a Pose-Aware Diffusion Model using few-shot OOTD examples followed by distilling a 3D NeRF-based avatar.
- **Architecture / Loss / Training**: Integrates ControlNet for pose estimation and introduces a Condition Prior Preservation Loss (CPPL).
- **Complexity / Resources**: Optimized through canonical SMPL-X space sampling and Multi-Resolution 3D-SDS.

</details>

### [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/pdf/2511.12662v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Head Avatar Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of creating realistic and deformable head avatars from video input.

### 3. Core Idea
- The proposed method utilizes point-based representations to create deformable head avatars that can adapt to various video inputs.

### 4. Method
- **Pipeline**: The pipeline involves capturing video data, processing it to extract facial features, and generating a 3D avatar using point-based techniques.
- **Architecture / Loss / Training**: The architecture employs a loss function that emphasizes realism and fidelity to the input video.
- **Complexity / Resources**: The method requires moderate computational resources, primarily for video processing and 3D rendering.

</details>

### [Dynamic Avatar-Scene Rendering from Human-centric Context](https://arxiv.org/pdf/2511.10539v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Scene Reconstruction

### 2. Motivation & Gaps
- The paper addresses the challenge of effective information exchange between separately modeled human avatars and background scenes in dynamic scene reconstruction.

### 3. Core Idea
- A shared information mapping mechanism that projects separately designed components into a unified representation space for coherent integration.

### 4. Method
- **Pipeline**: Utilizes lightweight residual MLPs to model Gaussian attributes for scene reconstruction.
- **Architecture / Loss / Training**: Utilizes shared-weight mapping architecture to enhance coherence in integration.
- **Complexity / Resources**: Maintains computational efficiency without exhaustive pairwise interactions.

</details>

## video understanding

### [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/pdf/2511.13719v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Spatial Intelligence Enhancement

### 2. Motivation & Gaps
- The paper discusses the limitations of data scaling in achieving human-level spatial intelligence and proposes open-sourcing the weights of SenseNova-SI to foster algorithmic innovation.

### 3. Core Idea
- Scaling spatial intelligence across multimodal foundation models.

### 4. Method
- **Pipeline**: Controlled experiments to evaluate spatial capabilities across different datasets.
- **Architecture / Loss / Training**: Employs AdamW with a learning rate of 5Ã—10âˆ’6 for all model-training runs.
- **Complexity / Resources**: Training takes approximately three days with a maximum of 16 frames sampled for video data.

</details>

### [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/pdf/2511.13715v1)
  (summary failed: 'utf-8' codec can't encode characters in position 8702-8705: surrogates not allowed)


### [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/pdf/2511.13714v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image Segmentation

### 2. Motivation & Gaps
- The paper addresses the challenges in image segmentation by introducing a self-supervised learning approach that allows for segmentation at any granularity.

### 3. Core Idea
- UNSAMV2 utilizes a divide-and-conquer pipeline to generate mask-granularity pairs and learns instance-part relationships for improved segmentation performance.

### 4. Method
- **Pipeline**: The method involves a two-stage process: a divide stage for generating pseudo masks and a conquer stage for refining these masks using a vision transformer.
- **Architecture / Loss / Training**: The model employs a combination of focal loss and dice loss with a ratio of 20:1 during training.
- **Complexity / Resources**: Experiments are conducted on 2 A-100 or 4 RTX 3090 GPUs with a batch size of 4.

</details>

## model collapse

### [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/pdf/2511.13720v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- High-resolution image generation

### 2. Motivation & Gaps
- The paper explores the use of general-purpose Transformers for high-resolution image generation, leveraging architectural advances from other applications.

### 3. Core Idea
- Decoupling the Transformer design from specific tasks to enhance scalability and performance in image generation.

### 4. Method
- **Pipeline**: Incorporates general-purpose improvements like SwiGLU, RMSNorm, and RoPE into a task-agnostic Transformer framework.
- **Architecture / Loss / Training**: Utilizes a minimalist design without extra losses or pre-training.
- **Complexity / Resources**: Models are compute-friendly and avoid quadratic scaling with resolution.

</details>

### [TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone](https://arxiv.org/pdf/2511.13717v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Security and performance evaluation of on-device large language models

### 2. Motivation & Gaps
- The paper addresses the need for protecting on-device large language models (LLMs) while maintaining performance and memory efficiency.

### 3. Core Idea
- TZ-LLM is designed to protect on-device LLMs using Arm TrustZone, ensuring performance, memory efficiency, and security through elastic secure memory scaling and TEE-REE NPU time-sharing.

### 4. Method
- **Pipeline**: Elastic secure memory scaling with pipelined restoration.
- **Architecture / Loss / Training**: The architecture is designed to minimize additional TCB by isolating the NPU driver in user mode and restricting access to secure memory.
- **Complexity / Resources**: The artifact requires an Orange Pi 5 Plus board and a standalone machine with at least 8 GB of memory and 100 GB of free disk space.

</details>
