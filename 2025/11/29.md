# Daily Paper Digest Â· 2025-11-29
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild](https://arxiv.org/pdf/2511.20366v2)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Geometry Reconstruction

### 2. Motivation & Gaps
- The method aims to improve facial geometry reconstruction under various capture conditions, including motion blur and hand shake.

### 3. Core Idea
- The proposed method leverages multi-view information and VGGT capabilities for accurate facial geometry reconstruction.

### 4. Method
- **Pipeline**: Geometry reconstruction followed by mesh re-rendering based on captured camera parameters.
- **Architecture / Loss / Training**: Minimizes error with respect to predicted point clouds using the Adam optimizer.
- **Complexity / Resources**: Requires significant computational resources for training and inference due to the complexity of the model.

</details>

### [The Making of Digital Ghosts: Designing Ethical AI Afterlives](https://arxiv.org/pdf/2511.20094v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Exploration of AI technologies in the context of digital legacies and grief.

### 2. Motivation & Gaps
- The paper investigates the implications of AI technologies on how we perceive and manage digital legacies after death.

### 3. Core Idea
- The integration of AI in managing digital legacies raises ethical, emotional, and practical concerns that need to be addressed.

### 4. Method
- **Pipeline**: Literature review and analysis of existing research on AI and digital legacies.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: Cost and complexity can vary widely, from simple text chat services to sophisticated custom video avatars.

</details>

### [STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction](https://arxiv.org/pdf/2511.19854v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D head avatar reconstruction

### 2. Motivation & Gaps
- This work addresses the challenges in creating high-fidelity 3D head avatars from monocular videos, focusing on improving the quality and realism of the generated avatars.

### 3. Core Idea
- The proposed method, ST Avatar, improves the reconstruction of 3D head avatars by utilizing Temporal Adaptive Density Control to allocate higher density to transient or frequently occluded regions, enhancing the detail and accuracy of the reconstructed avatars.

### 4. Method
- **Pipeline**: The method involves offset activation, UV sampling, and silhouette coefficient calculations to refine the avatar reconstruction process.
- **Architecture / Loss / Training**: The architecture employs a combination of nonlinear activation functions to stabilize the learning of Gaussian parameters, ensuring valid outputs during training.
- **Complexity / Resources**: The method requires moderate computational resources, leveraging advanced sampling techniques to optimize performance.

</details>

## video understanding

### [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/pdf/2511.21691v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Evaluating the fidelity of generated images based on multiple control types

### 2. Motivation & Gaps
- The paper aims to provide a holistic score for Joint Control Fidelity, reflecting how well generated scenes satisfy identity, pose, spatial layout, and realism.

### 3. Core Idea
- To develop a scoring rubric that evaluates generated images based on identity, pose, spatial layout, and realism.

### 4. Method
- **Pipeline**: The evaluation process involves assessing generated images against control inputs and scoring them based on predefined criteria.
- **Architecture / Loss / Training**: Utilizes ArcFace for identity preservation and DINOv2 for object consistency.
- **Complexity / Resources**: Requires internal datasets and models for pose estimation and instance segmentation.

</details>

### [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/pdf/2511.21690v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D motion supervision for robot manipulation

### 2. Motivation & Gaps
- TraceForge provides large-scale 3D motion supervision for training TraceGen.

### 3. Core Idea
- TraceForge achieves centimeter-level motion accuracy for reliable supervision in training TraceGen.

### 4. Method
- **Pipeline**: TraceForge extraction pipeline for 3D trace prediction.
- **Architecture / Loss / Training**: Multi-encoder architecture with frozen encoders for efficient training.
- **Complexity / Resources**: Reduced trainable parameters, accelerated training, improved generalization.

</details>

### [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/pdf/2511.21688v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D Reconstruction and Spatial Reasoning

### 2. Motivation & Gaps
- The paper addresses the need for a unified model that integrates geometry grounded vision and language understanding for enhanced 3D reconstruction and spatial reasoning.

### 3. Core Idea
- The architecture is a lightweight, 5-layer transformer that applies self-attention exclusively to the features of each individual image.

### 4. Method
- **Pipeline**: The model uses a lightweight transformer-based architecture with separate decoders for different tasks, ensuring efficient processing of geometric and semantic features.
- **Architecture / Loss / Training**: The visual geometry loss function is set with specific weights for each component, and training employs AdamW optimizer with a cosine scheduler.
- **Complexity / Resources**: Low-resolution pretraining runs on 32 A800 GPUs over 7 days and high-resolution runs on 64 A800 GPUs over 3 days.

</details>

## model collapse

### [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/pdf/2511.21692v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cross-difficulty generalization evaluation

### 2. Motivation & Gaps
- The paper addresses the need for a comprehensive evaluation suite to study cross-difficulty generalization in mathematical problem-solving.

### 3. Core Idea
- The study proposes a method to create training samples that better align with the styles of specific models (Qwen and Llama) to improve performance on difficult mathematical problems.

### 4. Method
- **Pipeline**: Collect responses from Qwen and Llama models at various temperatures and replace dataset solutions with sampled solutions.
- **Architecture / Loss / Training**: Finetuning models with datasets that resemble the sampled solutions.
- **Complexity / Resources**: Utilizes publicly available datasets with specific licenses.

</details>
