# Daily Paper Digest Â· 2025-11-21
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/pdf/2511.16673v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cross-domain evaluation on XHuman, novel view and pose synthesis

### 2. Motivation & Gaps
- The study addresses the challenges of synthesizing multiview images of avatars using diffusion models, highlighting issues with multiview consistency and the impact of training data inconsistency on perceptual evaluation metrics.

### 3. Core Idea
- Our method does not rely on pose priors, making it robust against noise in input poses during test-time reconstruction.

### 4. Method
- **Pipeline**: The method involves pose estimation using MultiHMR, followed by test-time optimization of camera and human poses for improved rendering.
- **Architecture / Loss / Training**: The model employs multiple loss functions including LBS, Chamfer, and photometric losses during training to optimize the reconstruction and pose estimation.
- **Complexity / Resources**: The method is computationally efficient compared to complex pose estimation pipelines, which can take over 20 minutes.

</details>

### [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/pdf/2511.16662v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D avatar generation

### 2. Motivation & Gaps
- The paper addresses critical limitations of existing 4D generation techniques, including temporal consistency, motion accuracy, visual fidelity, and computational efficiency.

### 3. Core Idea
- Combining a specialized diffusion model with the triplane representation and skeleton-guided conditioning to achieve precise skeleton-based pose control while reducing generation time.

### 4. Method
- **Pipeline**: Two-stage training approach with initial static avatar generation followed by reposing model training.
- **Architecture / Loss / Training**: Trained on 4 NVIDIA H100 GPUs with a progressive reconstruction strategy to optimize efficiency.
- **Complexity / Resources**: Utilizes 1,500 unique characters and 3,000 initial configurations for training.

</details>

### [B2F: End-to-End Body-to-Face Motion Generation with Style Reference](https://arxiv.org/pdf/2511.13988v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Motion Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic facial expressions using a model that does not rely on emotion labels during training.

### 3. Core Idea
- The B2F model generates facial motion by utilizing a parameter-blended Mixture of Experts architecture to convert FLAME parameters to ARKit blendshape weights.

### 4. Method
- **Pipeline**: The pipeline involves training a model to convert FLAME parameters to ARKit blendshape weights using a gating network and expert networks.
- **Architecture / Loss / Training**: The model employs a loss function that combines reconstruction loss, alignment loss, KL divergence, consistency loss, and cross loss, with dynamic scheduling for the KL term.
- **Complexity / Resources**: The experiments were conducted on a desktop system with an AMD Ryzen 7 7800X3D CPU and an NVIDIA GeForce RTX 4070 GPU, taking approximately 14 hours.

</details>

## video understanding

### [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/pdf/2511.16669v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Answer Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating video answers that are semantically and visually consistent with human preferences.

### 3. Core Idea
- The proposed VANS model with Joint-GRPO enhances video answer generation by effectively aligning textual and visual outputs, allowing for multi-future predictions.

### 4. Method
- **Pipeline**: The model processes input videos and generates multiple plausible video answers based on different hypothetical questions.
- **Architecture / Loss / Training**: Utilizes Joint-GRPO to improve performance metrics such as ROUGE-L and CLIP-T.
- **Complexity / Resources**: Trained on mixed datasets including Koala-36M for image-to-video tasks.

</details>

### [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/pdf/2511.16668v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Rule Following and Block Sliding Evaluation

### 2. Motivation & Gaps
- The paper evaluates the performance of models in predicting visual transformations based on given input and ground-truth images.

### 3. Core Idea
- The evaluation framework assesses the accuracy of visual predictions by comparing them against ground-truth images based on specific transformation rules.

### 4. Method
- **Pipeline**: Input images are processed to generate predictions, which are then evaluated against ground-truth images.
- **Architecture / Loss / Training**: Evaluated six generative video models with varying architectures and training pipelines.
- **Complexity / Resources**: Utilizes a standardized protocol for evaluation, ensuring consistency across models.

</details>

### [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/pdf/2511.16661v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Human-to-Robot Transfer of Manipulation Skills

### 2. Motivation & Gaps
- The framework aims to improve the transfer of manipulation skills from human demonstrations to robots, particularly in varying environments and object types.

### 3. Core Idea
- AINA uses object-centric point clouds to enhance robustness and reduce visual disparity in human-robot interactions.

### 4. Method
- **Pipeline**: Collect in-the-wild human demonstrations and use point clouds for training robot policies.
- **Architecture / Loss / Training**: Utilizes transformer-based imitation learning architecture with point cloud inputs.
- **Complexity / Resources**: Requires Aria Gen 2 glasses for data collection and Realsense cameras for deployment.

</details>

## model collapse

### [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/pdf/2511.16674v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dataset Distillation

### 2. Motivation & Gaps
- The paper addresses the limitations of traditional dataset distillation methods, particularly in terms of memory and data loading constraints.

### 3. Core Idea
- The proposed method synthesizes datasets through a curriculum approach, optimizing pyramid representations to improve efficiency and reduce training time.

### 4. Method
- **Pipeline**: The method involves distilling synthetic images from pyramid representations and optimizing them using a linear classifier.
- **Architecture / Loss / Training**: Utilizes Adam optimizer with a learning rate of 0.002, distilling for 5000 iterations.
- **Complexity / Resources**: Employs a variety of GPUs including H200, A100, and others, with specific time requirements for distillation.

</details>

### [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/pdf/2511.16672v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning

### 2. Motivation & Gaps
- The paper addresses the need for advanced multimodal reasoning capabilities in AI models.

### 3. Core Idea
- The core idea is to enhance multimodal reasoning through a chain-of-thought approach, allowing models to reason more effectively across different modalities.

### 4. Method
- **Pipeline**: The method involves a structured pipeline that integrates various multimodal inputs and processes them through a reasoning framework.
- **Architecture / Loss / Training**: The architecture employs a loss function specifically designed for multimodal reasoning tasks, optimizing for accuracy and efficiency.
- **Complexity / Resources**: The model requires significant computational resources for training, including high-performance GPUs and large datasets.

</details>

### [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/pdf/2511.16670v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Question Answering (VQA)

### 2. Motivation & Gaps
- The paper addresses the challenges in geometric reasoning and logic reasoning in visual question answering tasks.

### 3. Core Idea
- DualMindVLM utilizes a dual process approach to enhance reasoning capabilities in visual question answering tasks.

### 4. Method
- **Pipeline**: The method involves a series of logical deductions based on visual inputs and geometric properties.
- **Architecture / Loss / Training**: The architecture is trained using a combination of visual and textual data to improve reasoning accuracy.
- **Complexity / Resources**: The model requires significant computational resources for training and inference.

</details>
