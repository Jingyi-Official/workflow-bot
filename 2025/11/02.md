# Daily Paper Digest Â· 2025-11-02
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications](http://arxiv.org/pdf/2510.26251v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Investigate the impact of avatar characteristics on user ratings in healthcare applications

### 2. Motivation & Gaps
- The integration of virtual avatars in mental healthcare has potential benefits, but the effectiveness and design features remain underexplored.

### 3. Core Idea
- The study highlights the influence of gender stereotypes on the perception of virtual avatars in healthcare applications, emphasizing the importance of competence over warmth and attractiveness.

### 4. Method
- **Pipeline**: Participants interacted with six different avatars in a randomized order while completing health-related questionnaires.
- **Architecture / Loss / Training**: N/A
- **Complexity / Resources**: N/A

</details>

### [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](http://arxiv.org/pdf/2510.25234v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 3D emotional talking-face animation

### 2. Motivation & Gaps
- Current methods for facial animation often fail to accurately synchronize lip movements with speech and emotional expressions.

### 3. Core Idea
- A novel approach that learns disentangled speech-driven and expression-driven deformations to improve emotional expressiveness without sacrificing speech articulation accuracy.

### 4. Method
- **Pipeline**: Modeling facial animation as a linear combination of two blendshape sets learned from high-quality 3D scans.
- **Architecture / Loss / Training**: Trained with the sparsity constraint loss to separate driving factors while accommodating subtle cross-domain effects.
- **Complexity / Resources**: Lightweight model achieving over 165 FPS on a commercial GPU (NVIDIA GeForce RTX 3090).

</details>

### [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](http://arxiv.org/pdf/2510.23929v1)
  (summary failed: 'utf-8' codec can't encode characters in position 7033-7034: surrogates not allowed)


## video understanding

### [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](http://arxiv.org/pdf/2510.26802v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video reasoning enhancement

### 2. Motivation & Gaps
- The paper addresses the need for improved reasoning capabilities in multi-modal large language models (MLLMs) specifically for video content.

### 3. Core Idea
- The core idea is to enhance video reasoning in MLLMs through reinforcement learning techniques, enabling better understanding and generation of video content.

### 4. Method
- **Pipeline**: The method involves a reinforcement learning framework that integrates video reasoning tasks into the training process of MLLMs.
- **Architecture / Loss / Training**: Utilizes a custom loss function designed to optimize reasoning accuracy in video contexts.
- **Complexity / Resources**: The approach requires significant computational resources due to the complexity of video data processing.

</details>

### [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](http://arxiv.org/pdf/2510.26796v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- View synthesis

### 2. Motivation & Gaps
- The paper addresses the challenge of synthesizing novel views of dynamic scenes using a monocular camera, focusing on achieving globally coherent depth estimation.

### 3. Core Idea
- The core idea is to utilize a monocular camera to synthesize novel views of dynamic scenes by leveraging globally coherent depth information.

### 4. Method
- **Pipeline**: The method involves capturing dynamic scenes with a monocular camera and processing the data to generate novel views.
- **Architecture / Loss / Training**: The architecture employs a loss function that ensures depth coherence across synthesized views.
- **Complexity / Resources**: The method requires moderate computational resources for training and inference.

</details>

### [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](http://arxiv.org/pdf/2510.26794v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Motion Generation from Text Prompts

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic motion sequences from textual descriptions, focusing on the limitations of existing models in handling complex movements.

### 3. Core Idea
- ViMoGen introduces an adaptive branch selection mechanism that intelligently chooses between Motion-to-Motion (M2M) and Text-to-Motion (T2M) branches based on the quality of the initial motion extracted from generated videos.

### 4. Method
- **Pipeline**: The model processes text prompts to generate motion sequences, utilizing an adaptive gate to select the appropriate generation branch.
- **Architecture / Loss / Training**: The architecture employs a loss function that balances realism and adherence to the text prompt during training.
- **Complexity / Resources**: The model requires moderate computational resources for training and inference, leveraging existing motion capture datasets.

</details>

## model collapse

### [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](http://arxiv.org/pdf/2510.26800v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Guided Panorama Perception and Panorama Completion

### 2. Motivation & Gaps
- The paper addresses the challenges in generating accurate and coherent 3D scenes from panoramic images.

### 3. Core Idea
- OmniX generates accurate and locally coherent results for masked areas in panoramic images while ensuring consistency with RGB references.

### 4. Method
- **Pipeline**: Input masked images and corresponding masks to generate RGB outputs and other attributes like distance, normal, albedo, roughness, and metallic.
- **Architecture / Loss / Training**: Built on top of pre-trained 2D flow matching models.
- **Complexity / Resources**: Inherits shortcomings of pre-trained models, affecting efficiency.

</details>

### [Masked Diffusion Captioning for Visual Feature Learning](http://arxiv.org/pdf/2510.26799v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Image captioning and visual pretraining

### 2. Motivation & Gaps
- The paper addresses the need for improved visual pretraining methods that incorporate location awareness in caption generation.

### 3. Core Idea
- The core idea is to enhance visual pretraining by integrating location-aware captioning techniques to improve the contextual relevance of generated captions.

### 4. Method
- **Pipeline**: The method involves a pretraining pipeline that utilizes location-aware captioners to generate captions based on visual inputs.
- **Architecture / Loss / Training**: The architecture employs a sigmoid loss function for training, focusing on language-image alignment.
- **Complexity / Resources**: The model complexity is managed through hyperparameter tuning and resource allocation for training on large datasets.

</details>
