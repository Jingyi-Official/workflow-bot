# Daily Paper Digest Â· 2025-11-23
> Auto-generated: Recent submissions from arXiv are fetched by topic and keyword (up to 3 papers per query).

## avatar

### [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/pdf/2511.16673v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Cross-domain evaluation on XHuman, novel view and pose synthesis

### 2. Motivation & Gaps
- The study addresses the challenges of synthesizing multiview images of avatars using diffusion models, highlighting issues with multiview consistency and the impact of training data inconsistency on perceptual evaluation metrics.

### 3. Core Idea
- Our method does not rely on pose priors, making it robust to noise in input poses during test-time reconstruction.

### 4. Method
- **Pipeline**: The method involves pose estimation using MultiHMR, followed by test-time optimization of camera and human poses for improved rendering.
- **Architecture / Loss / Training**: The model employs multiple loss functions including LBS, photometric, and projection losses during training to optimize the shape and pose.
- **Complexity / Resources**: The method is computationally efficient compared to complex pose estimation pipelines, which can take over 20 minutes.

</details>

### [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/pdf/2511.16662v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- 4D avatar generation

### 2. Motivation & Gaps
- Existing 4D generation techniques face limitations in temporal consistency, motion accuracy, visual fidelity, and computational efficiency.

### 3. Core Idea
- Combining a specialized diffusion model with triplane representation and skeleton-guided conditioning to achieve precise skeleton-based pose control while reducing generation time.

### 4. Method
- **Pipeline**: Two-stage training approach with initial static avatar generation followed by reposing model training.
- **Architecture / Loss / Training**: Trained on 4 NVIDIA H100 GPUs with a progressive reconstruction strategy.
- **Complexity / Resources**: Utilizes 1,500 unique characters and 3,000 initial configurations for training.

</details>

### [B2F: End-to-End Body-to-Face Motion Generation with Style Reference](https://arxiv.org/pdf/2511.13988v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Facial Motion Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating realistic facial expressions using a model that does not rely on emotion labels.

### 3. Core Idea
- The B2F model generates facial motion by transforming FLAME parameters into ARKit blendshape weights using a parameter-blended Mixture of Experts architecture.

### 4. Method
- **Pipeline**: FLAME parameters are processed through an encoder and a gating network to blend expert networks, producing ARKit blendshape weights.
- **Architecture / Loss / Training**: The model uses a loss function combining reconstruction, alignment, KL divergence, consistency, and cross terms, with dynamic scheduling for the KL term.
- **Complexity / Resources**: Training was conducted on a desktop with an AMD Ryzen 7 7800X3D CPU and NVIDIA GeForce RTX 4070 GPU, taking approximately 14 hours.

</details>

## video understanding

### [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/pdf/2511.16669v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Video Answer Generation

### 2. Motivation & Gaps
- The paper addresses the challenge of generating video answers that are semantically and visually consistent with human preferences.

### 3. Core Idea
- The proposed V ANS model, enhanced by Joint-GRPO, effectively aligns textual and visual outputs, enabling multi-future predictions and robust reasoning in image-to-video generation.

### 4. Method
- **Pipeline**: The model processes input videos and generates corresponding video answers based on textual prompts.
- **Architecture / Loss / Training**: Utilizes Joint-GRPO to improve performance metrics such as ROUGE-L and CLIP-T.
- **Complexity / Resources**: Trained on mixed datasets including Koala-36M for I2V tasks, requiring significant computational resources.

</details>

### [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/pdf/2511.16668v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Rule Following and Block Sliding Evaluation

### 2. Motivation & Gaps
- The paper discusses the evaluation of predictions made by models in tasks involving visual reasoning and physical dynamics.

### 3. Core Idea
- The evaluation of model predictions in visual reasoning tasks requires adherence to specific transformation rules and visual fidelity to ground-truth images.

### 4. Method
- **Pipeline**: Evaluation of models across 13 reasoning tasks using pass@5 metrics.
- **Architecture / Loss / Training**: Models trained with reconstruction objectives that reward fine texture and temporal smoothness.
- **Complexity / Resources**: Utilizes a standardized protocol for evaluation, ensuring consistency across models.

</details>

### [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/pdf/2511.16661v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Learning manipulation skills from human demonstrations

### 2. Motivation & Gaps
- The framework aims to improve human-to-robot transfer of manipulation skills using point clouds instead of images.

### 3. Core Idea
- AINA uses object-centric point clouds to enhance robustness and generalization in robotic manipulation tasks.

### 4. Method
- **Pipeline**: Collect in-the-wild human demonstrations and use point clouds for training policies.
- **Architecture / Loss / Training**: Utilizes transformer-based imitation learning architecture with point cloud inputs.
- **Complexity / Resources**: Requires Aria Gen 2 glasses for data collection and Realsense cameras for deployment.

</details>

## model collapse

### [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/pdf/2511.16674v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Dataset Distillation

### 2. Motivation & Gaps
- The paper addresses the limitations of traditional dataset distillation methods, particularly in terms of memory and data loading constraints.

### 3. Core Idea
- The core idea is to synthesize datasets through a curriculum approach that optimizes pyramid representations to improve the efficiency of training models.

### 4. Method
- **Pipeline**: The method involves distilling synthetic images from pyramid representations and optimizing them using a linear classifier.
- **Architecture / Loss / Training**: The synthetic loss is computed by passing the synthetic batch through a feature extractor and a linear classifier, with gradients vectorized for optimization.
- **Complexity / Resources**: Utilizes a variety of GPUs including H200, A100, L40s, Ada6000, and 4090, with specific training times for ImageNet datasets.

</details>

### [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/pdf/2511.16672v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Multimodal reasoning and learning without human supervision

### 2. Motivation & Gaps
- The paper addresses the need for improved visual reasoning in large multimodal models without relying on human-annotated supervision or external reward models.

### 3. Core Idea
- The proposed EvoLMM framework couples a Proposer and Solver within a shared backbone, optimizing them using a continuous self-consistency reward to enhance visual reasoning.

### 4. Method
- **Pipeline**: Proposer-Solver training setup with continuous self-consistency reward.
- **Architecture / Loss / Training**: The model is trained without question-answer supervision, using only the proposed continuous self-consistency reward.
- **Complexity / Resources**: The method is architecture-agnostic and scales reliably across different model sizes.

</details>

### [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/pdf/2511.16670v1)


<!--break-out-of-list-->
<details markdown="1">
<summary>ðŸ“„ Paper Summary </summary>

### 1. Task / Problem
- Visual Question Answering (VQA)

### 2. Motivation & Gaps
- The paper addresses the challenges in geometric reasoning and logic reasoning in visual question answering.

### 3. Core Idea
- DualMindVLM utilizes a dual approach to enhance reasoning capabilities in visual question answering tasks.

### 4. Method
- **Pipeline**: The method involves a series of logical deductions based on visual inputs and question prompts.
- **Architecture / Loss / Training**: The model is trained with a focus on optimizing the selection of thinking modes during inference.
- **Complexity / Resources**: The model achieves competitive performance while using significantly fewer tokens on average.

</details>
